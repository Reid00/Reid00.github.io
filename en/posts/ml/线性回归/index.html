<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>线性回归 | Reid's Blog</title>
<meta name=keywords content="简单线性回归"><meta name=description content="线性回归"><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/en/posts/ml/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://reid00.github.io/en/posts/ml/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><head><meta name=referrer content="no-referrer"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script><script>var doNotTrack=!1,dnt;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK")}</script><meta property="og:title" content="线性回归"><meta property="og:description" content="线性回归"><meta property="og:type" content="article"><meta property="og:url" content="https://reid00.github.io/en/posts/ml/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-16T19:35:25+08:00"><meta property="article:modified_time" content="2023-03-16T19:35:25+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="线性回归"><meta name=twitter:description content="线性回归"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://reid00.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"机器学习，深度学习，知识图谱相关","item":"https://reid00.github.io/en/posts/ml/"},{"@type":"ListItem","position":3,"name":"线性回归","item":"https://reid00.github.io/en/posts/ml/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"线性回归","name":"线性回归","description":"线性回归","keywords":["简单线性回归"],"articleBody":"介绍 称函数为效用函数 线性回归模型看起来非常简单，简单到让人怀疑其是否有研究价值以及使用价值。但实际上，线性回归模型可以说是最重要的数学模型之一，很多模型都是建立在它的基础之上，可以被称为是“模型之母”。\n1.1 什么是简单线性回归 所谓简单，是指只有一个样本特征，即只有一个自变量；所谓线性，是指方程是线性的；所谓回归，是指用方程来模拟变量之间是如何关联的。\n简单线性回归，其思想简单，实现容易（与其背后强大的数学性质相关。同时也是许多强大的非线性模型（多项式回归、逻辑回归、SVM）的基础。并且其结果具有很好的可解释性。\n1.2 一种基本推导思 我们所谓的建模过程，其实就是找到一个模型，最大程度的拟合我们的数据。 在简单线回归问题中，模型就是我们的直线方程：y = ax + b 。\n要想最大的拟合数据，本质上就是找到没有拟合的部分，也就是损失的部分尽量小，就是损失函数（loss function）（也有算法是衡量拟合的程度，称函数为效用函数（utility function））：\n因此，推导思路为：\n通过分析问题，确定问题的损失函数或者效用函数； 然后通过最优化损失函数或者效用函数，获得机器学习的模型 近乎所有参数学习算法都是这样的套路，区别是模型不同，建立的目标函数不同，优化的方式也不同。\n回到简单线性回归问题，目标：\n已知训练数据样本、 ，找到和的值，使 尽可能小\n这是一个典型的最小二乘法问题（最小化误差的平方）\n通过最小二乘法可以求出a、b的表达式：\n最小二乘法 2.1 由损失函数引出一堆“风险” 2.1.1 损失函数 在机器学习中，所有的算法模型其实都依赖于最小化或最大化某一个函数，我们称之为“目标函数”。\n最小化的这组函数被称为“损失函数”。什么是损失函数呢？\n损失函数描述了单个样本预测值和真实值之间误差的程度。用来度量模型一次预测的好坏。\n损失函数是衡量预测模型预测期望结果表现的指标。损失函数越小，模型的鲁棒性越好。。\n常用损失函数有：\n0-1损失函数：用来表述分类问题，当预测分类错误时，损失函数值为1，正确为 平方损失函数：用来描述回归问题，用来表示连续性变量，为预测值与真实值差值的平方。（误差值越大、惩罚力度越强，也就是对差值敏感）\n绝对损失函数：用在回归模型，用距离的绝对值来衡量 对数损失函数：是预测值Y和条件概率之间的衡量。事实上，该损失函数用到了极大似然估计的思想。P(Y|X)通俗的解释就是：在当前模型的基础上，对于样本X，其预测值为Y，也就是预测正确的概率。由于概率之间的同时满足需要使用乘法，为了将其转化为加法，我们将其取对数。最后由于是损失函数，所以预测正确的概率越高，其损失值应该是越小，因此再加个负号取个反。 以上损失函数是针对于单个样本的，但是一个训练数据集中存在N个样本，N个样本给出N个损失，如何进行选择呢？\n这就引出了风险函数。\n2.1.2 期望风险 期望风险是损失函数的期望，用来表达理论上模型f(X)关于联合分布P(X,Y)的平均意义下的损失。又叫期望损失/风险函数。\n2.1.3 经验风险 模型f(X)关于训练数据集的平均损失，称为经验风险或经验损失。\n其公式含义为：模型关于训练集的平均损失（每个样本的损失加起来，然后平均一下）\n经验风险最小的模型为最优模型。在训练集上最小经验风险最小，也就意味着预测值和真实值尽可能接近，模型的效果越好。公式含义为取训练样本集中对数损失函数平均值的最小。\n2.1.4 经验风险最小化和结构风险最小化 期望风险是模型关于联合分布的期望损失，经验风险是模型关于训练样本数据集的平均损失。根据大数定律，当样本容量N趋于无穷时，经验风险趋于期望风险。\n因此很自然地想到用经验风险去估计期望风险。但是由于训练样本个数有限，可能会出现过度拟合的问题，即决策函数对于训练集几乎全部拟合，但是对于测试集拟合效果过差。因此需要对其进行矫正：\n结构风险最小化：当样本容量不大的时候，经验风险最小化容易产生“过拟合”的问题，为了“减缓”过拟合问题，提出了结构风险最小理论。结构风险最小化为经验风险与复杂度同时较小。 通过公式可以看出，结构风险：在经验风险上加上一个正则化项(regularizer)，或者叫做罚项(penalty) 。正则化项是J(f)是函数的复杂度再乘一个权重系数（用以权衡经验风险和复杂度）\n2.1.5 小结 1、损失函数：单个样本预测值和真实值之间误差的程度。\n2、期望风险：是损失函数的期望，理论上模型f(X)关于联合分布P(X,Y)的平均意义下的损失。\n3、经验风险：模型关于训练集的平均损失（每个样本的损失加起来，然后平均一下）。\n4、结构风险：在经验风险上加上一个正则化项，防止过拟合的策略。\n2.2 最小二乘法 2.2.1 什么是最小二乘法 言归正传，进入最小二乘法的部分。\n大名鼎鼎的最小二乘法，虽然听上去挺高大上，但是思想还是挺朴素的，符合大家的直觉。\n最小二乘法源于法国数学家阿德里安的猜想：\n对于测量值来说，让总的误差的平方最小的就是真实值。这是基于，如果误差是随机的，应该围绕真值上下波动。\n即：\n那么为了求出这个二次函数的最小值，对其进行求导，导数为0的时候取得最小值：\n进而：\n正好是算数平均数（算数平均数是最小二乘法的特例）。\n这就是最小二乘法，所谓“二乘”就是平方的意思。\n（高斯证明过：如果误差的分布是正态分布，那么最小二乘法得到的就是最有可能的值。）\n2.2.2 线性回归中的应用 我们在第一章中提到：\n目标是，找到a和b，使得损失函数：尽可能的小。\n这里，将简单线性问题转为最优化问题。下面对函数的各个位置分量求导，导数为0的地方就是极值：\n对 进行求导：\n然后mb提到等号前面，两边同时除以m，等号右面的每一项相当于均值。\n现在 对 进行求导：\n此时将对 进行求导得到的结果 代入上式中，得到：\n将上式进行整理，得到\n将上式继续进行整理：\n这样在实现的时候简单很多。\n最终我们通过最小二乘法得到a、b的表达式：\n总结 本章中，我们从数学的角度了解了简单线性回归，从中总结出一类机器学习算法的基本思路：\n通过分析问题，确定问题的损失函数或者效用函数； 然后通过最优化损失函数或者效用函数，获得机器学习的模型。 理解了损失函数的概念，并列举出了常见损失函数，并引出了一堆“风险”。最后为了求出最小的损失函数，学习了最小二乘法，并进行了完整的数学推导。\n下一篇，我们将会实现简单线性回归，并添加到我们自己的工程文件里。\n","wordCount":"104","inLanguage":"en","datePublished":"2023-03-16T19:35:25+08:00","dateModified":"2023-03-16T19:35:25+08:00","author":[{"@type":"Person","name":"Reid"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://reid00.github.io/en/posts/ml/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},"publisher":{"@type":"Organization","name":"Reid's Blog","logo":{"@type":"ImageObject","url":"https://reid00.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://reid00.github.io/en/ accesskey=h title="Reid's Blog (Alt + H)">Reid's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://reid00.github.io/en/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://reid00.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://reid00.github.io/en/categories/ title=Categorys><span>Categorys</span></a></li><li><a href=https://reid00.github.io/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://reid00.github.io/en/>Home</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/ml/>机器学习，深度学习，知识图谱相关</a></div><h1 class=post-title>线性回归</h1><div class=post-description>线性回归</div><div class=post-meta></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e4%bb%8b%e7%bb%8d aria-label=介绍>介绍</a><ul><li><a href=#11-%e4%bb%80%e4%b9%88%e6%98%af%e7%ae%80%e5%8d%95%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92 aria-label="1.1 什么是简单线性回归">1.1 什么是简单线性回归</a></li><li><a href=#12--%e4%b8%80%e7%a7%8d%e5%9f%ba%e6%9c%ac%e6%8e%a8%e5%af%bc%e6%80%9d aria-label="1.2  一种基本推导思">1.2 一种基本推导思</a></li><li><a href=#%e6%9c%80%e5%b0%8f%e4%ba%8c%e4%b9%98%e6%b3%95 aria-label=最小二乘法>最小二乘法</a><ul><li><a href=#21-%e7%94%b1%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e5%bc%95%e5%87%ba%e4%b8%80%e5%a0%86%e9%a3%8e%e9%99%a9 aria-label="2.1 由损失函数引出一堆“风险”">2.1 由损失函数引出一堆“风险”</a><ul><li><a href=#211-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0 aria-label="2.1.1 损失函数">2.1.1 损失函数</a></li><li><a href=#212-%e6%9c%9f%e6%9c%9b%e9%a3%8e%e9%99%a9 aria-label="2.1.2 期望风险">2.1.2 期望风险</a></li><li><a href=#213-%e7%bb%8f%e9%aa%8c%e9%a3%8e%e9%99%a9 aria-label="2.1.3 经验风险">2.1.3 经验风险</a></li><li><a href=#214-%e7%bb%8f%e9%aa%8c%e9%a3%8e%e9%99%a9%e6%9c%80%e5%b0%8f%e5%8c%96%e5%92%8c%e7%bb%93%e6%9e%84%e9%a3%8e%e9%99%a9%e6%9c%80%e5%b0%8f%e5%8c%96 aria-label="2.1.4 经验风险最小化和结构风险最小化">2.1.4 经验风险最小化和结构风险最小化</a></li><li><a href=#215-%e5%b0%8f%e7%bb%93 aria-label="2.1.5 小结">2.1.5 小结</a></li></ul></li><li><a href=#22-%e6%9c%80%e5%b0%8f%e4%ba%8c%e4%b9%98%e6%b3%95 aria-label="2.2 最小二乘法">2.2 最小二乘法</a><ul><li><a href=#221-%e4%bb%80%e4%b9%88%e6%98%af%e6%9c%80%e5%b0%8f%e4%ba%8c%e4%b9%98%e6%b3%95 aria-label="2.2.1 什么是最小二乘法">2.2.1 什么是最小二乘法</a></li><li><a href=#222-%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8 aria-label="2.2.2 线性回归中的应用">2.2.2 线性回归中的应用</a></li></ul></li></ul></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=介绍>介绍<a hidden class=anchor aria-hidden=true href=#介绍>#</a></h1><p>称函数为<strong>效用函数</strong> 线性回归模型看起来非常简单，简单到让人怀疑其是否有研究价值以及使用价值。但实际上，线性回归模型可以说是最重要的数学模型之一，很多模型都是建立在它的基础之上，可以被称为是“模型之母”。</p><h2 id=11-什么是简单线性回归>1.1 什么是简单线性回归<a hidden class=anchor aria-hidden=true href=#11-什么是简单线性回归>#</a></h2><p>所谓简单，是指只有一个样本特征，即只有一个自变量；所谓线性，是指方程是线性的；所谓回归，是指用方程来模拟变量之间是如何关联的。</p><p>简单线性回归，其思想简单，实现容易（与其背后强大的数学性质相关。同时也是许多强大的非线性模型（多项式回归、逻辑回归、SVM）的基础。并且其结果具有很好的可解释性。</p><h2 id=12--一种基本推导思>1.2 一种基本推导思<a hidden class=anchor aria-hidden=true href=#12--一种基本推导思>#</a></h2><p><strong>我们所谓的建模过程，其实就是找到一个模型，最大程度的拟合我们的数据。</strong> 在简单线回归问题中，模型就是我们的直线方程：y = ax + b 。</p><p>要想最大的拟合数据，本质上就是找到没有拟合的部分，也就是损失的部分尽量小，就是<strong>损失函数</strong>（loss function）（也有算法是衡量拟合的程度，称函数为<strong>效用函数</strong>（utility function））：</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.hv31rx1fxaw.webp alt=image-20191201222429886></p><p>因此，推导思路为：</p><ol><li>通过分析问题，确定问题的损失函数或者效用函数；</li><li>然后通过最优化损失函数或者效用函数，获得机器学习的模型</li></ol><p>近乎所有参数学习算法都是这样的套路，区别是模型不同，建立的目标函数不同，优化的方式也不同。</p><p>回到简单线性回归问题，目标：</p><blockquote><p>已知训练数据样本、 ，找到和的值，使 尽可能小</p></blockquote><p>这是一个典型的最小二乘法问题（最小化误差的平方）</p><p>通过最小二乘法可以求出a、b的表达式：</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.1j8ecbere97k.webp alt=image-20191201222456542></p><h2 id=最小二乘法>最小二乘法<a hidden class=anchor aria-hidden=true href=#最小二乘法>#</a></h2><h3 id=21-由损失函数引出一堆风险>2.1 由损失函数引出一堆“风险”<a hidden class=anchor aria-hidden=true href=#21-由损失函数引出一堆风险>#</a></h3><h4 id=211-损失函数>2.1.1 损失函数<a hidden class=anchor aria-hidden=true href=#211-损失函数>#</a></h4><p>在机器学习中，所有的算法模型其实都依赖于<strong>最小化或最大化某一个函数</strong>，我们称之为“<strong>目标函数</strong>”。</p><p>最小化的这组函数被称为“损失函数”。什么是损失函数呢？</p><blockquote><p>损失函数描述了单个样本预测值和真实值之间误差的程度。用来度量模型一次预测的好坏。</p></blockquote><p>损失函数是衡量预测模型预测期望结果表现的指标。损失函数越小，模型的鲁棒性越好。。</p><p>常用损失函数有：</p><ul><li><p>0-1损失函数：用来表述分类问题，当预测分类错误时，损失函数值为1，正确为<img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.1rfowr7oke68.webp alt=image-20191201222555831></p></li><li><p>平方损失函数：用来描述回归问题，用来表示连续性变量，为预测值与真实值差值的平方。（误差值越大、惩罚力度越强，也就是对差值敏感）</p></li></ul><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.5jdjsn40gn40.webp alt=image-20191201222620710></p><ul><li>绝对损失函数：用在回归模型，用距离的绝对值来衡量</li></ul><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.22fwss0ekwu8.webp alt=image-20191201222638485></p><ul><li>对数损失函数：是预测值Y和条件概率之间的衡量。事实上，该损失函数用到了极大似然估计的思想。P(Y|X)通俗的解释就是：在当前模型的基础上，对于样本X，其预测值为Y，也就是预测正确的概率。由于概率之间的同时满足需要使用乘法，为了将其转化为加法，我们将其取对数。最后由于是损失函数，所以预测正确的概率越高，其损失值应该是越小，因此再加个负号取个反。</li></ul><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.391e0sb2yfs0.webp alt=image-20191201222700230></p><p>以上损失函数是针对于单个样本的，但是一个训练数据集中存在N个样本，N个样本给出N个损失，如何进行选择呢？</p><p>这就引出了风险函数。</p><h4 id=212-期望风险>2.1.2 期望风险<a hidden class=anchor aria-hidden=true href=#212-期望风险>#</a></h4><p><strong>期望风险</strong>是<strong>损失函数的期望</strong>，用来表达<strong>理论上模型f(X)关于联合分布P(X,Y)的平均意义下的损失</strong>。又叫<strong>期望损失/风险函数</strong>。</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_jpg/1fsH49VZrGEyleqiaSavoviamvhHGQoDrXOJgib78PN0uZ9iaStQtXNorYAVAkaa7o0R6kgHEqctiaJD1lhRziay1Igw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><h4 id=213-经验风险>2.1.3 经验风险<a hidden class=anchor aria-hidden=true href=#213-经验风险>#</a></h4><p><strong>模型f(X)关于训练数据集的平均损失，称为经验风险或经验损失</strong>。</p><p>其公式含义为：模型关于训练集的平均损失（每个样本的损失加起来，然后平均一下）</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_jpg/1fsH49VZrGEyleqiaSavoviamvhHGQoDrXf4a5qURt6g4oibVq5jObibUfbw6eiajyjiaePqwIUiaSRp3Q5fqoicjPaTAw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p><strong>经验风险最小的模型为最优模型</strong>。在训练集上最小经验风险最小，也就意味着预测值和真实值尽可能接近，模型的效果越好。公式含义为取训练样本集中对数损失函数平均值的最小。</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_jpg/1fsH49VZrGEyleqiaSavoviamvhHGQoDrXSKiaGFyJkS3alZrnQMicpwy6UbiajJa2Ff7MLdxmaUov0hwk8KMrXyzLQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><h4 id=214-经验风险最小化和结构风险最小化>2.1.4 经验风险最小化和结构风险最小化<a hidden class=anchor aria-hidden=true href=#214-经验风险最小化和结构风险最小化>#</a></h4><p>期望风险是模型关于联合分布的期望损失，经验风险是模型关于训练样本数据集的平均损失。根据大数定律，<strong>当样本容量N趋于无穷时，经验风险趋于期望风险。</strong></p><p>因此很自然地想到<strong>用经验风险去估计期望风险</strong>。但是由于训练样本个数有限，可能会出现过度拟合的问题，即决策函数对于训练集几乎全部拟合，但是对于测试集拟合效果过差。因此需要对其进行矫正：</p><ul><li><strong>结构风险最小化</strong>：当样本容量不大的时候，经验风险最小化容易产生“过拟合”的问题，为了“减缓”过拟合问题，提出了<strong>结构风险最小</strong>理论。结构风险最小化为经验风险与复杂度同时较小。</li></ul><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_jpg/1fsH49VZrGEyleqiaSavoviamvhHGQoDrXcZTgYiaFzLRBl6ibchic17icZVFfXvXsKT1Ooq4LHuVrAbXvBUPSrYUnJw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>通过公式可以看出，结构风险：<strong>在经验风险上加上一个正则化项</strong>(regularizer)，或者叫做罚项(penalty) 。正则化项是J(f)是函数的复杂度再乘一个权重系数（用以权衡经验风险和复杂度）</p><h4 id=215-小结>2.1.5 小结<a hidden class=anchor aria-hidden=true href=#215-小结>#</a></h4><p>1、损失函数：单个样本预测值和真实值之间误差的程度。</p><p>2、期望风险：是损失函数的期望，理论上模型f(X)关于联合分布P(X,Y)的平均意义下的损失。</p><p>3、经验风险：模型关于训练集的平均损失（每个样本的损失加起来，然后平均一下）。</p><p>4、结构风险：在经验风险上加上一个正则化项，防止过拟合的策略。</p><h3 id=22-最小二乘法>2.2 最小二乘法<a hidden class=anchor aria-hidden=true href=#22-最小二乘法>#</a></h3><h4 id=221-什么是最小二乘法>2.2.1 什么是最小二乘法<a hidden class=anchor aria-hidden=true href=#221-什么是最小二乘法>#</a></h4><p>言归正传，进入最小二乘法的部分。</p><p>大名鼎鼎的最小二乘法，虽然听上去挺高大上，但是思想还是挺朴素的，符合大家的直觉。</p><p>最小二乘法源于法国数学家阿德里安的猜想：</p><blockquote><p>对于测量值来说，让总的误差的平方最小的就是真实值。这是基于，如果误差是随机的，应该围绕真值上下波动。</p></blockquote><p>即：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_jpg/1fsH49VZrGEyleqiaSavoviamvhHGQoDrX1bXZfKEgic6ibyNKnw7oMxg7icUicYwXtBEQUYa3qYcuRia5PLib91QHibVdg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>那么为了求出这个二次函数的最小值，对其进行求导，导数为0的时候取得最小值：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_jpg/1fsH49VZrGEyleqiaSavoviamvhHGQoDrXH9P6xNN2ZY3et4OiaZeYJ1Am04bM9rP1t7xzlNeCuSod0E300S4tBlQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>进而：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_jpg/1fsH49VZrGEyleqiaSavoviamvhHGQoDrXBXn2IN5Z3gAQMFegK2coPyVGu9ktjRyeViaukOCg7Bj9xcmLBA9lSHQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>正好是算数平均数（算数平均数是最小二乘法的特例）。</p><p>这就是最小二乘法，所谓“二乘”就是平方的意思。</p><p>（高斯证明过：如果误差的分布是正态分布，那么最小二乘法得到的就是最有可能的值。）</p><h4 id=222-线性回归中的应用>2.2.2 线性回归中的应用<a hidden class=anchor aria-hidden=true href=#222-线性回归中的应用>#</a></h4><p>我们在第一章中提到：</p><blockquote><p>目标是，找到a和b，使得损失函数：尽可能的小。</p></blockquote><p>这里，将简单线性问题转为最优化问题。下面对函数的各个位置分量求导，导数为0的地方就是极值：</p><p>对 进行求导：</p><p>然后mb提到等号前面，两边同时除以m，等号右面的每一项相当于均值。</p><p>现在 对 进行求导：</p><p>此时将对 进行求导得到的结果 代入上式中，得到：</p><p>将上式进行整理，得到</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_jpg/1fsH49VZrGEyleqiaSavoviamvhHGQoDrXW2rQPAy5ibrfGmBz7rpd36WY8IZSl27LZsMEjPwew7gTiacEzTC7AiaPw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>将上式继续进行整理：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_jpg/1fsH49VZrGEyleqiaSavoviamvhHGQoDrXHiav4h8wZdciagf6f584U20MqHdN2JibwEqXnXOtTxF9Yoibiaricibs3k5Bg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>这样在实现的时候简单很多。</p><p>最终我们通过最小二乘法得到a、b的表达式：</p><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>本章中，我们从数学的角度了解了简单线性回归，从中总结出一类机器学习算法的基本思路：</p><ol><li>通过分析问题，确定问题的损失函数或者效用函数；</li><li>然后通过最优化损失函数或者效用函数，获得机器学习的模型。</li></ol><p>理解了损失函数的概念，并列举出了常见损失函数，并引出了一堆“风险”。最后为了求出最小的损失函数，学习了最小二乘法，并进行了完整的数学推导。</p><p>下一篇，我们将会实现简单线性回归，并添加到我们自己的工程文件里。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://reid00.github.io/en/tags/%E7%AE%80%E5%8D%95%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/>简单线性回归</a></li></ul><nav class=paginav><a class=prev href=https://reid00.github.io/en/posts/ml/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8Bvs%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/><span class=title>« Prev</span><br><span>生成模型vs判别模型</span>
</a><a class=next href=https://reid00.github.io/en/posts/ml/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/><span class=title>Next »</span><br><span>逻辑回归</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://reid00.github.io/en/>Reid's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>