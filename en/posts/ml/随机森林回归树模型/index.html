<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>随机森林（回归树）模型 | Reid's Blog</title>
<meta name=keywords content="SKLearn,随机森林,RF,Random Forest"><meta name=description content="随机森林（回归树）模型"><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/en/posts/ml/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%9B%9E%E5%BD%92%E6%A0%91%E6%A8%A1%E5%9E%8B/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://reid00.github.io/en/posts/ml/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%9B%9E%E5%BD%92%E6%A0%91%E6%A8%A1%E5%9E%8B/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><head><meta name=referrer content="no-referrer"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script><script>var doNotTrack=!1,dnt;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK")}</script><meta property="og:title" content="随机森林（回归树）模型"><meta property="og:description" content="随机森林（回归树）模型"><meta property="og:type" content="article"><meta property="og:url" content="https://reid00.github.io/en/posts/ml/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%9B%9E%E5%BD%92%E6%A0%91%E6%A8%A1%E5%9E%8B/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-16T19:35:26+08:00"><meta property="article:modified_time" content="2023-03-16T19:35:26+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="随机森林（回归树）模型"><meta name=twitter:description content="随机森林（回归树）模型"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://reid00.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"机器学习，深度学习，知识图谱相关","item":"https://reid00.github.io/en/posts/ml/"},{"@type":"ListItem","position":3,"name":"随机森林（回归树）模型","item":"https://reid00.github.io/en/posts/ml/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%9B%9E%E5%BD%92%E6%A0%91%E6%A8%A1%E5%9E%8B/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"随机森林（回归树）模型","name":"随机森林（回归树）模型","description":"随机森林（回归树）模型","keywords":["SKLearn","随机森林","RF","Random Forest"],"articleBody":"调参 ★ 在 scikit-learn 中，Random Forest（以下简称RF）的分类类是 RandomForestClassifier，回归类是 RandomForestRegressor。\nRF 需要调参的参数也包括两部分，第一部分是 Bagging 框架的参数，第二部分是 CART 决策树的参数。下面我们就对这些参数做一个介绍。\nRF 框架参数 首先我们关注于 RF 的 Bagging 框架的参数。这里可以和 GBDT 对比来学习。GBDT 的框架参数比较多，重要的有最大迭代器个数，步长和子采样比例，调参起来比较费力。但是 RF 则比较简单，这是因为 bagging 框架里的各个弱学习器之间是没有依赖关系的，这减小的调参的难度。换句话说，达到同样的调参效果，RF 调参时间要比 GBDT 少一些。\n下面我来看看 RF 重要的 Bagging 框架的参数，由于 RandomForestClassifier 和 RandomForestRegressor 参数绝大部分相同，这里会将它们一起讲，不同点会指出。\nn_estimators：也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说 n_estimators 太小，容易欠拟合，n_estimators 太大，计算量会太大，并且 n_estimators 到一定的数量后，再增大 n_estimators 获得的模型提升会很小，所以一般选择一个适中的数值。默认是 100 。\noob_score：即是否采用袋外样本来评估模型的好坏。默认识 False 。个人推荐设置为 True ，因为袋外分数反应了一个模型拟合后的泛化能力。\ncriterion: 即 CART 树做划分时对特征的评价标准。分类模型和回归模型的损失函数是不一样的。分类 RF 对应的 CART 分类树默认是基尼系数 gini ，另一个可选择的标准是信息增益。回归 RF 对应的 CART 回归树默认是均方差 mse ，另一个可以选择的标准是绝对值差 mae 。一般来说选择默认的标准就已经很好的。\n从上面可以看出， RF 重要的框架参数比较少，主要需要关注的是 n_estimators，即 RF 最大的决策树个数。\nRF 决策树参数 RF 划分时考虑的最大特征数 max_features：\n可以使用很多种类型的值，默认是 auto ，意味着划分时最多考虑 $\\sqrt {N}$ 个特征；如果是 log2 意味着划分时最多考虑 $log_2N$ 个特征；如果是 sqrt 或者 auto 意味着划分时最多考虑$\\sqrt {N}$ 个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比 x $N$）取整后的特征数。其中 $N$ 为样本总特征数。一般我们用默认的 auto 就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。\n决策树最大深度 max_depth：\n默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值 10-100 之间。\n内部节点再划分所需最小样本数 min_samples_split：\n这个值限制了子树继续划分的条件，如果某节点的样本数少于 min_samples_split，则不会继续再尝试选择最优特征来进行划分。默认是 2，如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。\n叶子节点最少样本数 min_samples_leaf：\n这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是 1，可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。\n叶子节点最小的样本权重 min_weight_fraction_leaf：\n这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。默认是 0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。\n最大叶子节点数 max_leaf_nodes：\n通过限制最大叶子节点数，可以防止过拟合，默认是 None ，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。\n节点划分最小不纯度 min_impurity_split：\n这个值限制了决策树的增长，如果某节点的不纯度（基于基尼系数，均方差）小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动，默认值 1e-7。\n上面决策树参数中最重要的包括最大特征数 max_features， 最大深度 max_depth， 内部节点再划分所需最小样本数 min_samples_split 和叶子节点最少样本数 min_samples_leaf\n调参实例 我们使用社交网络数据集为例，利用 sklearn.grid_search 中的 GridSearchCV 类进行网格搜索最佳参数，现有两种调参思路。\n串行调参思路：调整一个或几个参数，固定其他参数，得到所调整参数的最优。重复以上步骤，使得每个参数都得打最优 并行调参思路：参数同时进行调整，不过计算量比较大，但是一次性能够找到最好的参数 载入需要的库 1 2 from sklearn.model_selection import GridSearchCV from sklearn.ensemble import RandomForestClassifier 调整参数 n_estimators 1 2 3 4 5 6 7 8 param_test1 = {'n_estimators': list(range(10,71,10))} classifier = RandomForestClassifier(n_estimators=10, min_samples_split=20,min_samples_leaf=2,max_depth=9,criterion='entropy', oob_score=True,random_state=0) gsearch1= GridSearchCV(estimator=classifier,param_grid=param_test1,scoring='roc_auc',cv=5) gsearch1.fit(X_train,y_train) print(gsearch1.cv_results_) print(gsearch1.best_params_) print(gsearch1.best_score_) 调整参数 max_depth 得到了最佳的弱学习器迭代次数为 30 ，我们将相应参数进行修改，接着我们对决策树最大深度 max_depth 和内部节点再划分所需最小样本数 min_samples_split 进行网格搜索。\n1 2 3 4 5 6 7 8 9 10 param_test2 = {'max_depth': list(range(3, 12, 1)), 'min_samples_split': list(range(5, 101, 5))} classifier = RandomForestClassifier(n_estimators=30, min_samples_leaf=2, criterion='entropy', oob_score=True, random_state=0) gsearch2 = GridSearchCV(estimator=classifier, param_grid=param_test2, scoring='roc_auc', iid=False, cv=5) gsearch2.fit(X_train, y_train) print(gsearch2.best_params_) print(gsearch2.best_score_) Result\n1 2 {'max_depth': 7, 'min_samples_split': 20} 0.9426982890941702 调整参数 min_samples_split 和 min_samples_split 对于内部节点再划分所需最小样本数 min_samples_split ，我们暂时不能一起定下来，因为这个还和决策树其他的参数存在关联。下面我们再对内部节点再划分所需最小样本数 min_samples_split 和叶子节点最少样本数 min_samples_leaf 一起调参。\n1 2 3 4 5 6 7 8 9 10 param_test3 = {'min_samples_split': list(range(5, 51, 5)), 'min_samples_leaf': list(range(5, 51, 5))} classifier = RandomForestClassifier(n_estimators=30, max_depth=7, criterion='entropy', oob_score=True, random_state=0) gsearch3 = GridSearchCV(estimator=classifier, param_grid=param_test3, scoring='roc_auc', iid=False, cv=5) gsearch3.fit(X_train, y_train) print(gsearch3.best_params_) print(gsearch3.best_score_) Result:\n1 2 {'min_samples_leaf': 5, 'min_samples_split': 30} 0.9419350440517489 至此相关参数已调整完毕，不过这种串行调优方式并不能一次性找到最好的解。我们可以将参数可选的值一次性的导入网格，使得所有参数调优同步进行，但是这样会降低程序运行的效率，可能需要大量的计算资源。如果算力强大，还是值得一试的。\n","wordCount":"300","inLanguage":"en","datePublished":"2023-03-16T19:35:26+08:00","dateModified":"2023-03-16T19:35:26+08:00","author":[{"@type":"Person","name":"Reid"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://reid00.github.io/en/posts/ml/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%9B%9E%E5%BD%92%E6%A0%91%E6%A8%A1%E5%9E%8B/"},"publisher":{"@type":"Organization","name":"Reid's Blog","logo":{"@type":"ImageObject","url":"https://reid00.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://reid00.github.io/en/ accesskey=h title="Reid's Blog (Alt + H)">Reid's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://reid00.github.io/en/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://reid00.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://reid00.github.io/en/categories/ title=Categorys><span>Categorys</span></a></li><li><a href=https://reid00.github.io/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://reid00.github.io/en/>Home</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/ml/>机器学习，深度学习，知识图谱相关</a></div><h1 class=post-title>随机森林（回归树）模型</h1><div class=post-description>随机森林（回归树）模型</div><div class=post-meta><span title='2023-03-16 19:35:26 +0800 +0800'>2023-03-16</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;300 words&nbsp;·&nbsp;Reid</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e8%b0%83%e5%8f%82- aria-label="调参 ★">调参 ★</a></li><li><a href=#rf-%e6%a1%86%e6%9e%b6%e5%8f%82%e6%95%b0 aria-label="RF 框架参数">RF 框架参数</a></li><li><a href=#rf-%e5%86%b3%e7%ad%96%e6%a0%91%e5%8f%82%e6%95%b0 aria-label="RF 决策树参数">RF 决策树参数</a></li><li><a href=#%e8%b0%83%e5%8f%82%e5%ae%9e%e4%be%8b aria-label=调参实例>调参实例</a><ul><li><a href=#%e8%bd%bd%e5%85%a5%e9%9c%80%e8%a6%81%e7%9a%84%e5%ba%93 aria-label=载入需要的库>载入需要的库</a></li><li><a href=#%e8%b0%83%e6%95%b4%e5%8f%82%e6%95%b0-n_estimators aria-label="调整参数 n_estimators">调整参数 n_estimators</a></li><li><a href=#%e8%b0%83%e6%95%b4%e5%8f%82%e6%95%b0-max_depth aria-label="调整参数 max_depth">调整参数 max_depth</a></li><li><a href=#%e8%b0%83%e6%95%b4%e5%8f%82%e6%95%b0-min_samples_split-%e5%92%8c-min_samples_split aria-label="调整参数 min_samples_split 和 min_samples_split">调整参数 min_samples_split 和 min_samples_split</a></li></ul></li></ul></div></details></div><div class=post-content><h3 id=调参->调参 ★<a hidden class=anchor aria-hidden=true href=#调参->#</a></h3><p>在 <strong>scikit-learn</strong> 中，<strong>Random Forest</strong>（以下简称<strong>RF</strong>）的分类类是 <strong>RandomForestClassifier</strong>，回归类是 <strong>RandomForestRegressor</strong>。</p><p><strong>RF</strong> 需要调参的参数也包括两部分，第一部分是 <strong>Bagging</strong> 框架的参数，第二部分是 <strong>CART</strong> 决策树的参数。下面我们就对这些参数做一个介绍。</p><h3 id=rf-框架参数>RF 框架参数<a hidden class=anchor aria-hidden=true href=#rf-框架参数>#</a></h3><p>首先我们关注于 <strong>RF</strong> 的 <strong>Bagging</strong> 框架的参数。这里可以和 <strong>GBDT</strong> 对比来学习。<strong>GBDT</strong> 的框架参数比较多，重要的有最大迭代器个数，步长和子采样比例，调参起来比较费力。但是 <strong>RF</strong> 则比较简单，这是因为 <strong>bagging</strong> 框架里的各个弱学习器之间是没有依赖关系的，这减小的调参的难度。换句话说，达到同样的调参效果，<strong>RF</strong> 调参时间要比 <strong>GBDT</strong> 少一些。</p><p>下面我来看看 <strong>RF</strong> 重要的 <strong>Bagging</strong> 框架的参数，由于 <strong>RandomForestClassifier</strong> 和 <strong>RandomForestRegressor</strong> 参数绝大部分相同，这里会将它们一起讲，不同点会指出。</p><p><strong>n_estimators</strong>：也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说 <strong>n_estimators</strong> 太小，容易欠拟合，<strong>n_estimators</strong> 太大，计算量会太大，并且 <strong>n_estimators</strong> 到一定的数量后，再增大 <strong>n_estimators</strong> 获得的模型提升会很小，所以一般选择一个适中的数值。默认是 <code>100</code> 。</p><p><strong>oob_score</strong>：即是否采用袋外样本来评估模型的好坏。默认识 <strong><code>False</code></strong> 。个人推荐设置为 <strong><code>True</code></strong> ，因为袋外分数反应了一个模型拟合后的泛化能力。</p><p><strong>criterion</strong>: 即 <strong>CART</strong> 树做划分时对特征的评价标准。分类模型和回归模型的损失函数是不一样的。分类 <strong>RF</strong> 对应的 <strong>CART</strong> 分类树默认是基尼系数 <strong><code>gini</code></strong> ，另一个可选择的标准是<strong>信息增益</strong>。回归 <strong>RF</strong> 对应的 <strong>CART</strong> 回归树默认是均方差 <strong><code>mse</code></strong> ，另一个可以选择的标准是绝对值差 <strong><code>mae</code></strong> 。一般来说选择默认的标准就已经很好的。</p><p>从上面可以看出， <strong>RF</strong> 重要的框架参数比较少，主要需要关注的是 <strong>n_estimators</strong>，即 <strong>RF</strong> 最大的决策树个数。</p><h3 id=rf-决策树参数>RF 决策树参数<a hidden class=anchor aria-hidden=true href=#rf-决策树参数>#</a></h3><ul><li><p><strong>RF</strong> 划分时考虑的最大特征数 <strong>max_features</strong>：</p><p>可以使用很多种类型的值，默认是 <strong><code>auto</code></strong> ，意味着划分时最多考虑 $\sqrt {N}$ 个特征；如果是 <strong><code>log2</code></strong> 意味着划分时最多考虑 $log_2N$ 个特征；如果是 <strong><code>sqrt</code></strong> 或者 <strong><code>auto</code></strong> 意味着划分时最多考虑$\sqrt {N}$ 个特征。如果是整数，代表考虑的特征<strong>绝对数</strong>。如果是浮点数，代表考虑特征<strong>百分比</strong>，即考虑（百分比 x $N$）取整后的特征数。其中 $N$ 为样本总特征数。一般我们用默认的 <strong><code>auto</code></strong> 就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。</p></li><li><p>决策树最大深度 <strong>max_depth</strong>：</p><p>默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值 <code>10-100</code> 之间。</p></li><li><p>内部节点再划分所需最小样本数 <strong>min_samples_split</strong>：</p><p>这个值限制了子树继续划分的条件，如果某节点的样本数少于 <strong>min_samples_split</strong>，则不会继续再尝试选择最优特征来进行划分。默认是 <code>2</code>，如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</p></li><li><p>叶子节点最少样本数 <strong>min_samples_leaf</strong>：</p><p>这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是 <code>1</code>，可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</p></li><li><p>叶子节点最小的样本权重 <strong>min_weight_fraction_leaf</strong>：</p><p>这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。默认是 <code>0</code>，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。</p></li><li><p>最大叶子节点数 <strong>max_leaf_nodes</strong>：</p><p>通过限制最大叶子节点数，可以防止过拟合，默认是 <strong><code>None</code></strong> ，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。</p></li><li><p>节点划分最小不纯度 <strong>min_impurity_split</strong>：</p><p>这个值限制了决策树的增长，如果某节点的不纯度（基于基尼系数，均方差）小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动，默认值 <code>1e-7</code>。</p></li></ul><p>上面决策树参数中最重要的包括最大特征数 <strong>max_features</strong>， 最大深度 <strong>max_depth</strong>， 内部节点再划分所需最小样本数 <strong>min_samples_split</strong> 和叶子节点最少样本数 <strong>min_samples_leaf</strong></p><h3 id=调参实例>调参实例<a hidden class=anchor aria-hidden=true href=#调参实例>#</a></h3><p>我们使用社交网络数据集为例，利用 <strong><code>sklearn.grid_search</code></strong> 中的 <strong><code>GridSearchCV</code></strong> 类进行网格搜索最佳参数，现有两种调参思路。</p><ul><li>串行调参思路：调整一个或几个参数，固定其他参数，得到所调整参数的最优。重复以上步骤，使得每个参数都得打最优</li><li>并行调参思路：参数同时进行调整，不过计算量比较大，但是一次性能够找到最好的参数</li></ul><h4 id=载入需要的库>载入需要的库<a hidden class=anchor aria-hidden=true href=#载入需要的库>#</a></h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>GridSearchCV</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.ensemble</span> <span class=kn>import</span> <span class=n>RandomForestClassifier</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=调整参数-n_estimators>调整参数 n_estimators<a hidden class=anchor aria-hidden=true href=#调整参数-n_estimators>#</a></h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>param_test1</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;n_estimators&#39;</span><span class=p>:</span> <span class=nb>list</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span><span class=mi>71</span><span class=p>,</span><span class=mi>10</span><span class=p>))}</span>
</span></span><span class=line><span class=cl><span class=n>classifier</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>min_samples_split</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span><span class=n>min_samples_leaf</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span><span class=n>max_depth</span><span class=o>=</span><span class=mi>9</span><span class=p>,</span><span class=n>criterion</span><span class=o>=</span><span class=s1>&#39;entropy&#39;</span><span class=p>,</span> <span class=n>oob_score</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span><span class=n>random_state</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>gsearch1</span><span class=o>=</span> <span class=n>GridSearchCV</span><span class=p>(</span><span class=n>estimator</span><span class=o>=</span><span class=n>classifier</span><span class=p>,</span><span class=n>param_grid</span><span class=o>=</span><span class=n>param_test1</span><span class=p>,</span><span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;roc_auc&#39;</span><span class=p>,</span><span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>gsearch1</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span><span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>gsearch1</span><span class=o>.</span><span class=n>cv_results_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>gsearch1</span><span class=o>.</span><span class=n>best_params_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>gsearch1</span><span class=o>.</span><span class=n>best_score_</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=调整参数-max_depth>调整参数 max_depth<a hidden class=anchor aria-hidden=true href=#调整参数-max_depth>#</a></h4><p>得到了最佳的弱学习器迭代次数为 <code>30</code> ，我们将相应参数进行修改，接着我们对决策树最大深度 <strong>max_depth</strong> 和内部节点再划分所需最小样本数 <strong>min_samples_split</strong> 进行网格搜索。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>param_test2</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;max_depth&#39;</span><span class=p>:</span> <span class=nb>list</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>12</span><span class=p>,</span> <span class=mi>1</span><span class=p>)),</span> <span class=s1>&#39;min_samples_split&#39;</span><span class=p>:</span> <span class=nb>list</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>101</span><span class=p>,</span> <span class=mi>5</span><span class=p>))}</span>
</span></span><span class=line><span class=cl><span class=n>classifier</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                    <span class=n>min_samples_leaf</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                    <span class=n>criterion</span><span class=o>=</span><span class=s1>&#39;entropy&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                    <span class=n>oob_score</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                    <span class=n>random_state</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>gsearch2</span> <span class=o>=</span> <span class=n>GridSearchCV</span><span class=p>(</span><span class=n>estimator</span><span class=o>=</span><span class=n>classifier</span><span class=p>,</span> <span class=n>param_grid</span><span class=o>=</span><span class=n>param_test2</span><span class=p>,</span> <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;roc_auc&#39;</span><span class=p>,</span> <span class=n>iid</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>gsearch2</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>gsearch2</span><span class=o>.</span><span class=n>best_params_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>gsearch2</span><span class=o>.</span><span class=n>best_score_</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Result</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>{&#39;max_depth&#39;: 7, &#39;min_samples_split&#39;: 20}
</span></span><span class=line><span class=cl>0.9426982890941702
</span></span></code></pre></td></tr></table></div></div><h4 id=调整参数-min_samples_split-和-min_samples_split>调整参数 min_samples_split 和 min_samples_split<a hidden class=anchor aria-hidden=true href=#调整参数-min_samples_split-和-min_samples_split>#</a></h4><p>对于内部节点再划分所需最小样本数 <strong>min_samples_split</strong> ，我们暂时不能一起定下来，因为这个还和决策树其他的参数存在关联。下面我们再对内部节点再划分所需最小样本数 <strong>min_samples_split</strong> 和叶子节点最少样本数 <strong>min_samples_leaf</strong> 一起调参。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>param_test3</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;min_samples_split&#39;</span><span class=p>:</span> <span class=nb>list</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>51</span><span class=p>,</span> <span class=mi>5</span><span class=p>)),</span> <span class=s1>&#39;min_samples_leaf&#39;</span><span class=p>:</span> <span class=nb>list</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>51</span><span class=p>,</span> <span class=mi>5</span><span class=p>))}</span>
</span></span><span class=line><span class=cl><span class=n>classifier</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>n_estimators</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                    <span class=n>max_depth</span><span class=o>=</span><span class=mi>7</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                    <span class=n>criterion</span><span class=o>=</span><span class=s1>&#39;entropy&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                    <span class=n>oob_score</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                    <span class=n>random_state</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>gsearch3</span> <span class=o>=</span> <span class=n>GridSearchCV</span><span class=p>(</span><span class=n>estimator</span><span class=o>=</span><span class=n>classifier</span><span class=p>,</span> <span class=n>param_grid</span><span class=o>=</span><span class=n>param_test3</span><span class=p>,</span> <span class=n>scoring</span><span class=o>=</span><span class=s1>&#39;roc_auc&#39;</span><span class=p>,</span> <span class=n>iid</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>gsearch3</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>gsearch3</span><span class=o>.</span><span class=n>best_params_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>gsearch3</span><span class=o>.</span><span class=n>best_score_</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Result:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>{&#39;min_samples_leaf&#39;: 5, &#39;min_samples_split&#39;: 30}
</span></span><span class=line><span class=cl>0.9419350440517489
</span></span></code></pre></td></tr></table></div></div><p>至此相关参数已调整完毕，不过这种串行调优方式并不能一次性找到最好的解。我们可以将参数可选的值一次性的导入网格，使得所有参数调优同步进行，但是这样会降低程序运行的效率，可能需要大量的计算资源。如果算力强大，还是值得一试的。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://reid00.github.io/en/tags/sklearn/>SKLearn</a></li><li><a href=https://reid00.github.io/en/tags/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/>随机森林</a></li><li><a href=https://reid00.github.io/en/tags/rf/>RF</a></li><li><a href=https://reid00.github.io/en/tags/random-forest/>Random Forest</a></li></ul><nav class=paginav><a class=prev href=https://reid00.github.io/en/posts/ml/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/><span class=title>« Prev</span><br><span>逻辑回归的常见面试题总结</span>
</a><a class=next href=https://reid00.github.io/en/posts/ml/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E5%9C%A8%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/><span class=title>Next »</span><br><span>随机森林算法及其在特征选择中的应用</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://reid00.github.io/en/>Reid's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>