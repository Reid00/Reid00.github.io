<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>SVM | Reid's Blog</title>
<meta name=keywords content="SVM,支持向量机,Support Vector Machine"><meta name=description content="SVM"><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/en/posts/ml/svm/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://reid00.github.io/en/posts/ml/svm/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><head><meta name=referrer content="no-referrer"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK")}</script><meta property="og:title" content="SVM"><meta property="og:description" content="SVM"><meta property="og:type" content="article"><meta property="og:url" content="https://reid00.github.io/en/posts/ml/svm/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-16T19:35:19+08:00"><meta property="article:modified_time" content="2023-03-16T19:35:19+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="SVM"><meta name=twitter:description content="SVM"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://reid00.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"机器学习，深度学习，知识图谱相关","item":"https://reid00.github.io/en/posts/ml/"},{"@type":"ListItem","position":3,"name":"SVM","item":"https://reid00.github.io/en/posts/ml/svm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"SVM","name":"SVM","description":"SVM","keywords":["SVM","支持向量机","Support Vector Machine"],"articleBody":"1. SVM SVM的应用 SVM在很多诸如文本分类，图像分类，生物序列分析和生物数据挖掘，手写字符识别等领域有很多的应用，但或许你并没强烈的意识到，SVM可以成功应用的领域远远超出现在已经在开发应用了的领域。\n通常人们会从一些常用的核函数中选择（根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数），例如：多项式核、高斯核、线性核。\nSVM是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。（间隔最大是它有别于感知机）\n（1）当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；\n（2）当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；\n（3）当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。\n注：以上各SVM的数学推导应该熟悉：硬间隔最大化（几何间隔）—学习的对偶问题—软间隔最大化（引入松弛变量）—非线性支持向量机（核技巧）。\n读者可能还是没明白核函数到底是个什么东西？我再简要概括下，即以下三点：\n实际中，我们会经常遇到线性不可分的样例，此时，我们的常用做法是把样例特征映射到高维空间中去(映射到高维空间后，相关特征便被分开了，也就达到了分类的目的)； 但进一步，如果凡是遇到线性不可分的样例，一律映射到高维空间，那么这个维度大小是会高到可怕的。那咋办呢？ 此时，核函数就隆重登场了，核函数的价值在于它虽然也是将特征进行从低维到高维的转换，但核函数绝就绝在它事先在低维上进行计算，而将实质上的分类效果表现在了高维上，避免了直接在高维空间中的复杂计算 2. SVM的一些问题 SVM为什么采用间隔最大化？ 当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。\n感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。\n线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。\n然后应该借此阐述，几何间隔，函数间隔，及从函数间隔—\u003e求解最小化1/2 ||w||^2 时的w和b。即线性可分支持向量机学习算法—最大间隔法的由来。\nSVM如何处理多分类问题？** 一般有两种做法：一种是直接法，直接在目标函数上修改，将多个分类面的参数求解合并到一个最优化问题里面。看似简单但是计算量却非常的大。\n另外一种做法是间接法：对训练器进行组合。其中比较典型的有一对一，和一对多。\n一对多，就是对每个类都训练出一个分类器，由svm是二分类，所以将此而分类器的两类设定为目标类为一类，其余类为另外一类。这样针对k个类可以训练出k个分类器，当有一个新的样本来的时候，用这k个分类器来测试，那个分类器的概率高，那么这个样本就属于哪一类。这种方法效果不太好，bias比较高。\nsvm一对一法（one-vs-one），针对任意两个类训练出一个分类器，如果有k类，一共训练出C(2,k) 个分类器，这样当有一个新的样本要来的时候，用这C(2,k) 个分类器来测试，每当被判定属于某一类的时候，该类就加一，最后票数最多的类别被认定为该样本的类。\n是否存在一组参数使SVM训练误差为0？ Y\n训练误差为0的SVM分类器一定存在吗？ 一定存在\n加入松弛变量的SVM的训练误差可以为0吗？ 如果数据中出现了离群点outliers，那么就可以使用松弛变量来解决。\n使用SMO算法训练的线性分类器并不一定能得到训练误差为0的模型。这是由 于我们的优化目标改变了，并不再是使训练误差最小。\n带核的SVM为什么能分类非线性问题? 核函数的本质是两个函数的內积，通过核函数将其隐射到高维空间，在高维空间非线性问题转化为线性问题, SVM得到超平面是高维空间的线性分类平面。其分类结果也视为低维空间的非线性分类结果, 因而带核的SVM就能分类非线性问题。\n如何选择核函数？ 如果特征的数量大到和样本数量差不多，则选用LR或者线性核的SVM； 如果特征的数量小，样本的数量正常，则选用SVM+高斯核函数； 如果特征的数量小，而样本的数量很大，则需要手工添加一些特征从而变成第一种情况。 3. LR和SVM的联系与区别 相同点 都是线性分类器。本质上都是求一个最佳分类超平面。\n都是监督学习算法\n都是判别模型。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。\n不同点 LR是参数模型，svm是非参数模型，linear和rbf则是针对数据线性可分和不可分的区别\n从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。\nSVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。\n逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。\nlogic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。\n4. 线性分类器与非线性分类器的区别以及优劣 线性和非线性是针对模型参数和输入特征来讲的；比如输入x，模型y=ax+ax^2 那么就是非线性模型，如果输入是x和X^2则模型是线性的。\n线性分类器可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些。\nLR,贝叶斯分类，单层感知机、线性回归\n非线性分类器效果拟合能力较强，不足之处是数据量不足容易过拟合、计算复杂度高、可解释性不好。\n决策树、RF、GBDT、多层感知机\nSVM两种都有（看线性核还是高斯核 即RBF ）\n线性核：主要用于线性可分的情形，参数少，速度快，对于一般数据，分类效果已经很理想了。\nRBF 核：主要用于线性不可分的情形，参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交叉验证来寻找合适的参数，不过这个过程比较耗时。 如果 Feature 的数量很大，跟样本数量差不多，这时候选用线性核的 SVM。 如果 Feature 的数量比较小，样本数量一般，不算大也不算小，选用高斯核的 SVM。\n*为什么要转为对偶问题？（阿里面试）*\n(a) 目前处理的模型严重依赖于数据集的维度d，如果维度d太高就会严重提升运算时间；\n(b) 对偶问题事实上把SVM从依赖d个维度转变到依赖N个数据点，考虑到在最后计算时只有支持向量才有意义，所以这个计算量实际上比N小很多。\n一、是对偶问题往往更易求解（当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。）\n二、自然引入核函数，进而推广到非线性分类问题\n参考: https://cloud.tencent.com/developer/article/1541701\n","wordCount":"2944","inLanguage":"en","image":"https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png","datePublished":"2023-03-16T19:35:19+08:00","dateModified":"2023-03-16T19:35:19+08:00","author":[{"@type":"Person","name":"Reid"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://reid00.github.io/en/posts/ml/svm/"},"publisher":{"@type":"Organization","name":"Reid's Blog","logo":{"@type":"ImageObject","url":"https://reid00.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://reid00.github.io/en/ accesskey=h title="Reid's Blog (Alt + H)">Reid's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://reid00.github.io/en/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://reid00.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://reid00.github.io/en/categories/ title=Categorys><span>Categorys</span></a></li><li><a href=https://reid00.github.io/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://reid00.github.io/en/>Home</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/ml/>机器学习，深度学习，知识图谱相关</a></div><h1 class="post-title entry-hint-parent">SVM</h1><div class=post-description>SVM</div><div class=post-meta><span title='2023-03-16 19:35:19 +0800 +0800'>2023-03-16 19:35</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;2944 words&nbsp;·&nbsp;Reid</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-svm aria-label="1. SVM">1. SVM</a><ul><ul><li><a href=#svm%e7%9a%84%e5%ba%94%e7%94%a8 aria-label=SVM的应用>SVM的应用</a></li></ul></ul></li><li><a href=#2-svm%e7%9a%84%e4%b8%80%e4%ba%9b%e9%97%ae%e9%a2%98 aria-label="2. SVM的一些问题">2. SVM的一些问题</a><ul><ul><li><a href=#svm%e4%b8%ba%e4%bb%80%e4%b9%88%e9%87%87%e7%94%a8%e9%97%b4%e9%9a%94%e6%9c%80%e5%a4%a7%e5%8c%96 aria-label=SVM为什么采用间隔最大化？>SVM为什么采用间隔最大化？</a></li><li><a href=#svm%e5%a6%82%e4%bd%95%e5%a4%84%e7%90%86%e5%a4%9a%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%98 aria-label=SVM如何处理多分类问题？**>SVM如何处理多分类问题？**</a></li><li><a href=#%e6%98%af%e5%90%a6%e5%ad%98%e5%9c%a8%e4%b8%80%e7%bb%84%e5%8f%82%e6%95%b0%e4%bd%bfsvm%e8%ae%ad%e7%bb%83%e8%af%af%e5%b7%ae%e4%b8%ba0 aria-label=是否存在一组参数使SVM训练误差为0？>是否存在一组参数使SVM训练误差为0？</a></li><li><a href=#%e8%ae%ad%e7%bb%83%e8%af%af%e5%b7%ae%e4%b8%ba0%e7%9a%84svm%e5%88%86%e7%b1%bb%e5%99%a8%e4%b8%80%e5%ae%9a%e5%ad%98%e5%9c%a8%e5%90%97 aria-label=训练误差为0的SVM分类器一定存在吗？>训练误差为0的SVM分类器一定存在吗？</a></li><li><a href=#%e5%8a%a0%e5%85%a5%e6%9d%be%e5%bc%9b%e5%8f%98%e9%87%8f%e7%9a%84svm%e7%9a%84%e8%ae%ad%e7%bb%83%e8%af%af%e5%b7%ae%e5%8f%af%e4%bb%a5%e4%b8%ba0%e5%90%97 aria-label=加入松弛变量的SVM的训练误差可以为0吗？>加入松弛变量的SVM的训练误差可以为0吗？</a></li><li><a href=#%e5%b8%a6%e6%a0%b8%e7%9a%84svm%e4%b8%ba%e4%bb%80%e4%b9%88%e8%83%bd%e5%88%86%e7%b1%bb%e9%9d%9e%e7%ba%bf%e6%80%a7%e9%97%ae%e9%a2%98 aria-label=带核的SVM为什么能分类非线性问题?>带核的SVM为什么能分类非线性问题?</a></li><li><a href=#%e5%a6%82%e4%bd%95%e9%80%89%e6%8b%a9%e6%a0%b8%e5%87%bd%e6%95%b0 aria-label=如何选择核函数？>如何选择核函数？</a></li></ul></ul></li><li><a href=#3-lr%e5%92%8csvm%e7%9a%84%e8%81%94%e7%b3%bb%e4%b8%8e%e5%8c%ba%e5%88%ab aria-label="3. LR和SVM的联系与区别">3. LR和SVM的联系与区别</a><ul><ul><li><a href=#%e7%9b%b8%e5%90%8c%e7%82%b9 aria-label=相同点>相同点</a></li><li><a href=#%e4%b8%8d%e5%90%8c%e7%82%b9 aria-label=不同点>不同点</a></li></ul></ul></li><li><a href=#4-%e7%ba%bf%e6%80%a7%e5%88%86%e7%b1%bb%e5%99%a8%e4%b8%8e%e9%9d%9e%e7%ba%bf%e6%80%a7%e5%88%86%e7%b1%bb%e5%99%a8%e7%9a%84%e5%8c%ba%e5%88%ab%e4%bb%a5%e5%8f%8a%e4%bc%98%e5%8a%a3 aria-label="4. 线性分类器与非线性分类器的区别以及优劣">4. 线性分类器与非线性分类器的区别以及优劣</a></li></ul></div></details></div><div class=post-content><h1 id=1-svm>1. SVM<a hidden class=anchor aria-hidden=true href=#1-svm>#</a></h1><h3 id=svm的应用>SVM的应用<a hidden class=anchor aria-hidden=true href=#svm的应用>#</a></h3><p>SVM在很多诸如文本分类，图像分类，生物序列分析和生物数据挖掘，手写字符识别等领域有很多的应用，但或许你并没强烈的意识到，SVM可以成功应用的领域远远超出现在已经在开发应用了的领域。</p><p>通常人们会从一些常用的核函数中选择（根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数），例如：多项式核、高斯核、线性核。</p><p>SVM是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。（间隔最大是它有别于感知机）</p><p>（1）当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；</p><p>（2）当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；</p><p>（3）当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。</p><p>注：以上各SVM的数学推导应该熟悉：<strong>硬间隔最大化（几何间隔）&mdash;学习的对偶问题&mdash;软间隔最大化（引入松弛变量）&mdash;非线性支持向量机（核技巧）。</strong></p><p>读者可能还是没明白核函数到底是个什么东西？我再简要概括下，即以下三点：</p><ol><li>实际中，我们会经常遇到线性不可分的样例，此时，我们的常用做法是把样例特征映射到高维空间中去(映射到高维空间后，相关特征便被分开了，也就达到了分类的目的)；</li><li>但进一步，如果凡是遇到线性不可分的样例，一律映射到高维空间，那么这个维度大小是会高到可怕的。那咋办呢？</li><li>此时，核函数就隆重登场了，核函数的价值在于它虽然也是将特征进行从低维到高维的转换，但核函数绝就绝在它事先在低维上进行计算，而将实质上的分类效果表现在了高维上，避免了直接在高维空间中的复杂计算</li></ol><h1 id=2-svm的一些问题>2. SVM的一些问题<a hidden class=anchor aria-hidden=true href=#2-svm的一些问题>#</a></h1><h3 id=svm为什么采用间隔最大化><strong>SVM为什么采用间隔最大化？</strong><a hidden class=anchor aria-hidden=true href=#svm为什么采用间隔最大化>#</a></h3><p>当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。</p><p>感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。</p><p>线性可分支持向量机<strong>利用间隔最大化求得最优分离超平面</strong>，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是<strong>最鲁棒</strong>的，对未知实例的<strong>泛化能力最强</strong>。</p><p>然后应该借此阐述，几何间隔，函数间隔，及从函数间隔—>求解最小化1/2 ||w||^2 时的w和b。即线性可分支持向量机学习算法—最大间隔法的由来。</p><h3 id=svm如何处理多分类问题>SVM如何处理多分类问题？**<a hidden class=anchor aria-hidden=true href=#svm如何处理多分类问题>#</a></h3><p>一般有两种做法：一种是直接法，直接在目标函数上修改，将多个分类面的参数求解合并到一个最优化问题里面。看似简单但是计算量却非常的大。</p><p>另外一种做法是间接法：对训练器进行组合。其中比较典型的有<strong>一对一</strong>，和<strong>一对多</strong>。</p><p>一对多，就是对每个类都训练出一个分类器，由svm是二分类，所以将此而分类器的两类设定为目标类为一类，其余类为另外一类。这样针对k个类可以训练出k个分类器，当有一个新的样本来的时候，用这k个分类器来测试，那个分类器的概率高，那么这个样本就属于哪一类。这种方法效果不太好，bias比较高。</p><p>svm一对一法（one-vs-one），针对任意两个类训练出一个分类器，如果有k类，一共训练出C(2,k) 个分类器，这样当有一个新的样本要来的时候，用这C(2,k) 个分类器来测试，每当被判定属于某一类的时候，该类就加一，最后票数最多的类别被认定为该样本的类。</p><h3 id=是否存在一组参数使svm训练误差为0>是否存在一组参数使SVM训练误差为0？<a hidden class=anchor aria-hidden=true href=#是否存在一组参数使svm训练误差为0>#</a></h3><p>Y</p><h3 id=训练误差为0的svm分类器一定存在吗>训练误差为0的SVM分类器一定存在吗？<a hidden class=anchor aria-hidden=true href=#训练误差为0的svm分类器一定存在吗>#</a></h3><p>一定存在</p><h3 id=加入松弛变量的svm的训练误差可以为0吗>加入松弛变量的SVM的训练误差可以为0吗？<a hidden class=anchor aria-hidden=true href=#加入松弛变量的svm的训练误差可以为0吗>#</a></h3><p>如果数据中出现了离群点outliers，那么就可以使用松弛变量来解决。</p><p>使用SMO算法训练的线性分类器并不一定能得到训练误差为0的模型。这是由 于我们的优化目标改变了，并不再是使训练误差最小。</p><h3 id=带核的svm为什么能分类非线性问题>带核的SVM为什么能分类非线性问题?<a hidden class=anchor aria-hidden=true href=#带核的svm为什么能分类非线性问题>#</a></h3><p>核函数的本质是两个函数的內积，通过核函数将其隐射到高维空间，在高维空间非线性问题转化为线性问题, SVM得到超平面是高维空间的线性分类平面。其分类结果也视为低维空间的非线性分类结果, 因而带核的SVM就能分类非线性问题。</p><h3 id=如何选择核函数>如何选择核函数？<a hidden class=anchor aria-hidden=true href=#如何选择核函数>#</a></h3><ul><li>如果特征的数量大到和样本数量差不多，则选用LR或者线性核的SVM；</li><li>如果特征的数量小，样本的数量正常，则选用SVM+高斯核函数；</li><li>如果特征的数量小，而样本的数量很大，则需要手工添加一些特征从而变成第一种情况。</li></ul><h1 id=3-lr和svm的联系与区别>3. LR和SVM的联系与区别<a hidden class=anchor aria-hidden=true href=#3-lr和svm的联系与区别>#</a></h1><h3 id=相同点>相同点<a hidden class=anchor aria-hidden=true href=#相同点>#</a></h3><p>都是线性分类器。本质上都是求一个最佳分类超平面。</p><p>都是监督学习算法</p><p>都是判别模型。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。</p><h3 id=不同点>不同点<a hidden class=anchor aria-hidden=true href=#不同点>#</a></h3><p>LR是参数模型，svm是非参数模型，linear和rbf则是针对数据线性可分和不可分的区别</p><p>从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。</p><p>SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。</p><p>逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。</p><p>logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。</p><h1 id=4-线性分类器与非线性分类器的区别以及优劣>4. 线性分类器与非线性分类器的区别以及优劣<a hidden class=anchor aria-hidden=true href=#4-线性分类器与非线性分类器的区别以及优劣>#</a></h1><p>线性和非线性是针对模型参数和输入特征来讲的；比如输入x，模型y=ax+ax^2 那么就是非线性模型，如果输入是x和X^2则模型是线性的。</p><p>线性分类器可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些。</p><p>LR,贝叶斯分类，单层感知机、线性回归</p><p>非线性分类器效果拟合能力较强，不足之处是数据量不足容易过拟合、计算复杂度高、可解释性不好。</p><p>决策树、RF、GBDT、多层感知机</p><p><strong>SVM两种都有（看线性核还是高斯核 即RBF ）</strong></p><p>线性核：主要用于线性可分的情形，参数少，速度快，对于一般数据，分类效果已经很理想了。</p><p>RBF 核：主要用于线性不可分的情形，参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交叉验证来寻找合适的参数，不过这个过程比较耗时。 如果 Feature 的数量很大，跟样本数量差不多，这时候选用线性核的 SVM。 如果 Feature 的数量比较小，样本数量一般，不算大也不算小，选用高斯核的 SVM。</p><p><em><strong>*为什么要转为对偶问题？（阿里面试）*</strong></em></p><p>(a) 目前处理的模型严重依赖于数据集的维度d，如果维度d太高就会严重提升运算时间；</p><p>(b) 对偶问题事实上把SVM<strong>从依赖d个维度转变到依赖N个数据点</strong>，考虑到在最后计算时只有支持向量才有意义，所以这个计算量实际上比N小很多。</p><p>一、是对偶问题往往更易求解（当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。）</p><p>二、自然引入核函数，进而推广到非线性分类问题</p><p>参考: <a href=https://cloud.tencent.com/developer/article/1541701>https://cloud.tencent.com/developer/article/1541701</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://reid00.github.io/en/tags/svm/>SVM</a></li><li><a href=https://reid00.github.io/en/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/>支持向量机</a></li><li><a href=https://reid00.github.io/en/tags/support-vector-machine/>Support Vector Machine</a></li></ul><nav class=paginav><a class=prev href=https://reid00.github.io/en/posts/ml/%E5%B8%B8%E7%94%A8normalization%E6%96%B9%E6%B3%95%E7%9A%84%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83/><span class=title>« Prev</span><br><span>常用Normalization方法的总结与思考</span>
</a><a class=next href=https://reid00.github.io/en/posts/ml/word2vec/><span class=title>Next »</span><br><span>Word2vec</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://reid00.github.io/en/>Reid's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>