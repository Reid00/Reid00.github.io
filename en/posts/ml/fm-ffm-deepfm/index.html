<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>FM FFM DeepFM | Reid's Blog</title><meta name=keywords content="FM,FFM"><meta name=description content="FM FFM DeepFM"><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/en/posts/ml/fm-ffm-deepfm/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><head><meta name=referrer content="no-referrer"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK",{anonymize_ip:!1})}</script><meta property="og:title" content="FM FFM DeepFM"><meta property="og:description" content="FM FFM DeepFM"><meta property="og:type" content="article"><meta property="og:url" content="https://reid00.github.io/en/posts/ml/fm-ffm-deepfm/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-16T19:35:16+08:00"><meta property="article:modified_time" content="2023-03-16T19:35:16+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="FM FFM DeepFM"><meta name=twitter:description content="FM FFM DeepFM"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://reid00.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"机器学习，深度学习，知识图谱相关","item":"https://reid00.github.io/en/posts/ml/"},{"@type":"ListItem","position":3,"name":"FM FFM DeepFM","item":"https://reid00.github.io/en/posts/ml/fm-ffm-deepfm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"FM FFM DeepFM","name":"FM FFM DeepFM","description":"FM FFM DeepFM","keywords":["FM","FFM"],"articleBody":"介绍 FM和FMM模型在数据量比较大并且特征稀疏的情况下，仍然有优秀的性能表现，在CTR/CVR任务上尤其突出。\n本文包括：\n- FM 模型 - FFM 模型 - Deep FM 模型 - Deep FFM模型 FM模型的引入-广告特征的稀疏性 FM（Factorization machines）模型由Steffen Rendle于2010年提出，目的是解决稀疏数据下的特征组合问题。\n在介绍FM模型之前，来看看稀疏数据的训练问题。\n以广告CTR（click-through rate）点击率预测任务为例，假设有如下数据\nClicked? Country Day Ad_type 1 USA 26/11/15 Movie 0 China 19/2/15 Game 1 China 26/11/15 Game 第一列Clicked是类别标记，标记用户是否点击了该广告，而其余列则是特征（这里的三个特征都是类别类型），一般的，我们会对数据进行One-hot编码将类别特征转化为数值特征，转化后数据如下:\nClicked? Country=USA Country=China Day=26/11/15 Day=19/2/15 Ad_type=Movie Ad_type=Game 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 经过One-hot编码后，特征空间是十分稀疏的。特别的，某类别特征有m种不同的取值，则one-hot编码后就会被变为m维！当类别特征越多、类别特征的取值越多，其特征空间就更加稀疏。\n此外，往往我们会将特征进行两两的组合，这是因为：\n通过观察大量的样本数据可以发现，某些特征经过关联之后，与label之间的相关性就会提高。例如，“USA”与“Thanksgiving”、“China”与“Chinese New Year”这样的关联特征，对用户的点击有着正向的影响。换句话说，来自“China”的用户很可能会在“Chinese New Year”有大量的浏览、购买行为，而在“Thanksgiving”却不会有特别的消费行为。这种关联特征与label的正向相关性在实际问题中是普遍存在的，如“化妆品”类商品与“女”性，“球类运动配件”的商品与“男”性，“电影票”的商品与“电影”品类偏好等。\n再比如，用户更常在饭点的时间下载外卖app，因此，引入两个特征的组合是非常有意义的。\n如何表示两个特征的组合呢？一种直接的方法就是采用多项式模型来表示两个特征的组合，xixi为第ii个特征的取值（注意和以往表示第ii个样本的特征向量的区别），xixjxixj表示特征xixi和xjxj的特征组合，其系数wijwij即为我们学习的参数，也是xixjxixj组合的重要程度：\n式1-1也可以称为Poly2(degree-2 poly-nomial mappings)模型。注意到式子1-1中参数的个数是非常多的！一次项有d+1个，二次项（即组合特征的参数）共有d(d−1)2d(d−1)2个，而参数与参数之间彼此独立，在稀疏场景下，二次项的训练是很困难的。因为要训练wijwij，需要有大量的xixi和xjxj都非零的样本（只有非零组合才有意义）。而样本本身是稀疏的，满足xixj≠0xixj≠0的样本会非常少，样本少则难以估计参数wijwij，训练出来容易导致模型的过拟合。\n为此，Rendle于2010年提出FM模型，它能很好的求解式1-1，其特点如下：\nFM模型可以在非常稀疏的情况下进行参数估计 FM模型是线性时间复杂度的，可以直接使用原问题进行求解，而且不用像SVM一样依赖支持向量。 FM模型是一个通用的模型，其训练数据的特征取值可以是任意实数。而其它最先进的分解模型对输入数据有严格的限制。FMs可以模拟MF、SVD++、PITF或FPMC模型。 FM模型 前面提到过，式1-1的参数难以训练时因为训练数据的稀疏性。对于不同的特征对xi,xjxi,xj和xi,xkxi,xk，式1-1认为是完全独立的，对参数wijwij和wikwik分别进行训练。而实际上并非如此，不同的特征之间进行组合并非完全独立，如下图所示:\n回想矩阵分解，一个rating可以分解为user矩阵和item矩阵，如下图所示：\n分解后得到user和item矩阵的维度分别为nknk和kmkm，（k一般由用户指定），相比原来的rating矩阵，空间占用得到降低，并且分解后的user矩阵暗含着user偏好，Item矩阵暗含着item的属性，而user矩阵乘上item矩阵就是rating矩阵中用户对item的评分。\n因此，参考矩阵分解的过程，FM模型也将式1-1的二次项参数wijwij进行分解:\n其中vivi是第ii维特征的隐向量，其长度为k(k≪d)k(k≪d)。 (vi⋅vj)(vi⋅vj)为内积，其乘积为原来的wijwij，即 ^wij=(vi⋅vj)=∑kf=1vi,f⋅vj,fw^ij=(vi⋅vj)=∑f=1kvi,f⋅vj,f\n为了方便说明，考虑下面的数据集（实际中应该进行one-hot编码，但并不影响此处的说明）：\n数据集 Clicked? Publisher Advertiser Poly2参数 FM参数 训练集 1 NBC Nike wNBC,NikewNBC,Nike VNBC⋅VNikeVNBC⋅VNike 训练集 0 EPSN Adidas wEPSN,AdidaswEPSN,Adidas VEPSN⋅VAdidasVEPSN⋅VAdidas 测试集 ? NBC Adidas wNBC,AdidaswNBC,Adidas VNBC⋅VAdidas 对于上面的训练集，没有（NBC，Adidas）组合，因此，Poly2模型就无法学习到参数wNBC,AdidaswNBC,Adidas。而FM模型可以通过特征组合(NBC，Nike)、(EPSN，Adidas) 分别学习到隐向量VNBCVNBC和VAdidasVAdidas，这样使得在测试集中得以进行预测。\n更一般的，经过分解，式2-1的参数个数减少为kdkd个，对比式1-1，参数个数大大减少。使用小的k，使得模型能够提高在稀疏情况下的泛化性能。此外，将wijwij进行分解，使得不同的特征对不再是完全独立的，而它们的关联性可以用隐式因子表示，这将使得有更多的数据可以用于模型参数的学习。比如xi,xjxi,xj与xi,xkxi,xk的参数分别为：⟨vi,vj⟩⟨vi,vj⟩和⟨vi,vk⟩⟨vi,vk⟩，它们都可以用来学习vivi，更一般的，包含xixj≠0\u0026i≠jxixj≠0\u0026i≠j的所有样本都能用来学习vivi，很大程度上避免了数据稀疏性的影响。\n此外，式2-1的复杂度可以从O(kd2)O(kd2)优化到O(kd)O(kd)：\n可以看出，FM模型可以在线性的时间做出预测。\nFM模型学习 在2-4式中，∑dj=1vj,fxj∑j=1dvj,fxj只与ff有关而与ii无关，在每次迭代过程中，可以预先对所有ff的∑dj=1vj,fxj∑j=1dvj,fxj进行计算，复杂度O(kd)O(kd)，就能在常数时间O(1)O(1)内得到vi,fvi,f的梯度。而对于其它参数w0w0和wiwi，显然也是在常数时间内计算梯度。此外，更新参数只需要O(1)O(1), 一共有1+d+kd1+d+kd个参数，因此FM参数训练的复杂度也是O(kd)O(kd)。\n所以说，FM模型是一种高效的模型，是线性时间复杂度的，可以在线性的时间做出训练和预测。\nFFM模型 考虑下面的数据集：\nClicked? Publisher(P) Advertiser(A) Gender(G) 1 EPSN Nike Male 0 NBC Adidas Female 对于第一条数据来说，FM模型的二次项为：wEPSN⋅wNike+wEPSN⋅wMale+wNike⋅wMalewEPSN⋅wNike+wEPSN⋅wMale+wNike⋅wMale。（这里只是把上面的v符合改成了w）每个特征只用一个隐向量来学习和其它特征的潜在影响。对于上面的例子中，Nike是广告主，Male是用户的性别，描述（EPSN，Nike）和（EPSN，Male）特征组合，FM模型都用同一个wESPNwESPN，而实际上，ESPN作为广告商，其对广告主和用户性别的潜在影响可能是不同的。\n因此，Yu-Chin Juan借鉴Michael Jahrer的论文（Ensemble of collaborative filtering and feature engineered models for click through rate prediction），将field概念引入FM模型。\nfield是什么呢？即相同性质的特征放在一个field。比如EPSN、NBC都是属于广告商field的，Nike、Adidas都是属于广告主field，Male、Female都是属于性别field的。简单的说，同一个类别特征进行one-hot编码后生成的数值特征都可以放在同一个field中，比如最开始的例子中Day=26/11/15 Day=19/2/15可以放于同一个field中。如果是数值特征而非类别，可以直接作为一个field。\n引入了field后，对于刚才的例子来说，二次项变为：\n对于特征组合（EPSN，Nike）来说，其隐向量采用的是wEPSN,AwEPSN,A和wNike,PwNike,P，对于wEPSN,AwEPSN,A这是因为Nike属于广告主（Advertiser）的field，而第二项wNike,PwNike,P则是EPSN是广告商（Publisher）的field。 再举个例子，对于特征组合（EPSN，Male）来说，wEPSN,GwEPSN,G 是因为Male是用户性别(Gender)的field，而第二项wMale,PwMale,P是因为EPSN是广告商（Publisher）的field。 下面的图来自criteo，很好的表示了三个模型的区别\nFFM 数学公式 因此，FFM的数学公式表示为：\n其中fifi和fjfj分别代表第i个特征和第j个特征所属的field。若field有ff个，隐向量的长度为k，则二次项系数共有dfkdfk个，远多于FM模型的dkdk个。此外，隐向量和field相关，并不能像FM模型一样将二次项化简，计算的复杂度是d2kd2k。\n通常情况下，每个隐向量只需要学习特定field的表示，所以有kFFM≪kFMkFFM≪kFM。\nFFM 模型学习 为了方便推导，这里省略FFM的一次项和常数项，公式为：\n注意到∂Lerr∂ϕ∂Lerr∂ϕ和参数无关，每次更新模型时，只需要计算一次，之后直接调用结果即可。对于总共有dfkdfk个模型参数的计算来说，使用这种方式能极大提升运算效率。\n第二个trick是FFM的学习率是随迭代次数变化的，具体的是采用AdaGrad算法，这里进行简单的介绍。\nAdagrad算法能够在训练中自动的调整学习率，对于稀疏的参数增加学习率，而稠密的参数则降低学习率。因此，Adagrad非常适合处理稀疏数据。\n设gt,jgt,j为第t轮第j个参数的梯度，则SGD和采用Adagrad的参数更新公式分别如下：\n可以看出，Adagrad在学习率ηη上还除以一项√Gt,jj+ϵGt,jj+ϵ，这是什么意思呢？ϵϵ为平滑项，防止分母为0，Gt,jj=∑tι=1g2ι,jjGt,jj=∑ι=1tgι,jj2即Gt,jjGt,jj为对角矩阵，每个对角线位置j,jj,j的值为参数wjwj每一轮的平方和，可以看出，随着迭代的进行，每个参数的历史梯度累加到一起，使得每个参数的学习率逐渐减小。\n实现的trick 除了上面提到的梯度分步计算和自适应学习率两个trick外，还有：\nOpenMP多核并行计算。OpenMP是用于共享内存并行系统的多处理器程序设计的编译方案，便于移植和多核扩展[12]。FFM的源码采用了OpenMP的API，对参数训练过程SGD进行了多线程扩展，支持多线程编译。因此，OpenMP技术极大地提高了FFM的训练效率和多核CPU的利用率。在训练模型时，输入的训练参数ns_threads指定了线程数量，一般设定为CPU的核心数，便于完全利用CPU资源。 SSE3指令并行编程。SSE3全称为数据流单指令多数据扩展指令集3，是CPU对数据层并行的关键指令，主要用于多媒体和游戏的应用程序中[13]。SSE3指令采用128位的寄存器，同时操作4个单精度浮点数或整数。SSE3指令的功能非常类似于向量运算。例如，a和b采用SSE3指令相加（a和b分别包含4个数据），其功能是a种的4个元素与b中4个元素对应相加，得到4个相加后的值。采用SSE3指令后，向量运算的速度更加快捷，这对包含大量向量运算的FFM模型是非常有利的。 除了上面的技巧之外，FFM的实现中还有很多调优技巧需要探索。例如，代码是按field和特征的编号申请参数空间的，如果选取了非连续或过大的编号，就会造成大量的内存浪费；在每个样本中加入值为1的新特征，相当于引入了因子化的一次项，避免了缺少一次项带来的模型偏差等。\n适用范围和使用技巧 在FFM原论文中，作者指出，FFM模型对于one-hot后类别特征十分有效，但是如果数据不够稀疏，可能相比其它模型提升没有稀疏的时候那么大，此外，对于数值型的数据效果不是特别的好。\n在Github上有FFM的开源实现，要使用FFM模型，特征需要转化为“field_id:feature_id:value”格式，相比LibSVM的格式多了field_id，即特征所属的field的编号，feature_id是特征编号，value为特征的值。\n此外，美团点评的文章中，提到了训练FFM时的一些注意事项：\n第一，样本归一化。FFM默认是进行样本数据的归一化的 。若不进行归一化，很容易造成数据inf溢出，进而引起梯度计算的nan错误。因此，样本层面的数据是推荐进行归一化的。\n第二，特征归一化。CTR/CVR模型采用了多种类型的源特征，包括数值型和categorical类型等。但是，categorical类编码后的特征取值只有0或1，较大的数值型特征会造成样本归一化后categorical类生成特征的值非常小，没有区分性。例如，一条用户-商品记录，用户为“男”性，商品的销量是5000个（假设其它特征的值为零），那么归一化后特征“sex=male”（性别为男）的值略小于0.0002，而“volume”（销量）的值近似为1。特征“sex=male”在这个样本中的作用几乎可以忽略不计，这是相当不合理的。因此，将源数值型特征的值归一化到[0,1]是非常必要的。\n第三，省略零值特征。从FFM模型的表达式(3-1)可以看出，零值特征对模型完全没有贡献。包含零值特征的一次项和组合项均为零，对于训练模型参数或者目标值预估是没有作用的。因此，可以省去零值特征，提高FFM模型训练和预测的速度，这也是稀疏样本采用FFM的显著优势。\n参考: https://www.hrwhisper.me/machine-learning-fm-ffm-deepfm-deepffm/\n","wordCount":"191","inLanguage":"en","datePublished":"2023-03-16T19:35:16+08:00","dateModified":"2023-03-16T19:35:16+08:00","author":[{"@type":"Person","name":"Reid"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://reid00.github.io/en/posts/ml/fm-ffm-deepfm/"},"publisher":{"@type":"Organization","name":"Reid's Blog","logo":{"@type":"ImageObject","url":"https://reid00.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://reid00.github.io/en/ accesskey=h title="Reid's Blog (Alt + H)">Reid's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://reid00.github.io/en/archives title=Archive><span>Archive</span></a></li><li><a href=https://reid00.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://reid00.github.io/en/categories/ title=Categorys><span>Categorys</span></a></li><li><a href=https://reid00.github.io/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://reid00.github.io/en/>Home</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/ml/>机器学习，深度学习，知识图谱相关</a></div><h1 class=post-title>FM FFM DeepFM</h1><div class=post-description>FM FFM DeepFM</div><div class=post-meta><span title='2023-03-16 19:35:16 +0800 +0800'>2023-03-16</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Reid</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e4%bb%8b%e7%bb%8d aria-label=介绍>介绍</a><ul><li><a href=#fm%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%bc%95%e5%85%a5-%e5%b9%bf%e5%91%8a%e7%89%b9%e5%be%81%e7%9a%84%e7%a8%80%e7%96%8f%e6%80%a7 aria-label=FM模型的引入-广告特征的稀疏性>FM模型的引入-广告特征的稀疏性</a></li><li><a href=#fm%e6%a8%a1%e5%9e%8b aria-label=FM模型>FM模型</a></li><li><a href=#fm%e6%a8%a1%e5%9e%8b%e5%ad%a6%e4%b9%a0 aria-label=FM模型学习>FM模型学习</a></li><li><a href=#ffm%e6%a8%a1%e5%9e%8b aria-label=FFM模型>FFM模型</a></li><li><a href=#ffm-%e6%95%b0%e5%ad%a6%e5%85%ac%e5%bc%8f aria-label="FFM 数学公式">FFM 数学公式</a></li><li><a href=#ffm-%e6%a8%a1%e5%9e%8b%e5%ad%a6%e4%b9%a0 aria-label="FFM 模型学习">FFM 模型学习</a></li><li><a href=#%e5%ae%9e%e7%8e%b0%e7%9a%84trick aria-label=实现的trick>实现的trick</a></li><li><a href=#%e9%80%82%e7%94%a8%e8%8c%83%e5%9b%b4%e5%92%8c%e4%bd%bf%e7%94%a8%e6%8a%80%e5%b7%a7 aria-label=适用范围和使用技巧>适用范围和使用技巧</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=介绍>介绍<a hidden class=anchor aria-hidden=true href=#介绍>#</a></h2><p>FM和FMM模型在数据量比较大并且特征稀疏的情况下，仍然有优秀的性能表现，在CTR/CVR任务上尤其突出。</p><p>本文包括：</p><pre><code>- FM 模型
- FFM 模型
- Deep FM 模型
- Deep FFM模型
</code></pre><h3 id=fm模型的引入-广告特征的稀疏性>FM模型的引入-广告特征的稀疏性<a hidden class=anchor aria-hidden=true href=#fm模型的引入-广告特征的稀疏性>#</a></h3><p>FM（Factorization machines）模型由Steffen Rendle于2010年提出，目的是解决稀疏数据下的特征组合问题。</p><p>在介绍FM模型之前，来看看稀疏数据的训练问题。</p><p>以广告CTR（click-through rate）点击率预测任务为例，假设有如下数据</p><table><thead><tr><th><strong>Clicked?</strong></th><th>Country</th><th>Day</th><th>Ad_type</th></tr></thead><tbody><tr><td>1</td><td>USA</td><td>26/11/15</td><td>Movie</td></tr><tr><td>0</td><td>China</td><td>19/2/15</td><td>Game</td></tr><tr><td>1</td><td>China</td><td>26/11/15</td><td>Game</td></tr></tbody></table><p>第一列Clicked是类别标记，标记用户是否点击了该广告，而其余列则是特征（这里的三个特征都是类别类型），一般的，我们会对数据进行One-hot编码将类别特征转化为数值特征，转化后数据如下:</p><table><thead><tr><th><strong>Clicked?</strong></th><th>Country=USA</th><th>Country=China</th><th>Day=26/11/15</th><th>Day=19/2/15</th><th>Ad_type=Movie</th><th>Ad_type=Game</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td></tr><tr><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td></tr><tr><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td></tr></tbody></table><p>经过One-hot编码后，特征空间是十分稀疏的。特别的，某类别特征有m种不同的取值，则one-hot编码后就会被变为m维！当类别特征越多、类别特征的取值越多，其特征空间就更加稀疏。</p><p>此外，往往我们会将特征进行两两的组合，这是因为：</p><blockquote><p>通过观察大量的样本数据可以发现，某些特征经过关联之后，与label之间的相关性就会提高。例如，“USA”与“Thanksgiving”、“China”与“Chinese New Year”这样的关联特征，对用户的点击有着正向的影响。换句话说，来自“China”的用户很可能会在“Chinese New Year”有大量的浏览、购买行为，而在“Thanksgiving”却不会有特别的消费行为。这种关联特征与label的正向相关性在实际问题中是普遍存在的，如“化妆品”类商品与“女”性，“球类运动配件”的商品与“男”性，“电影票”的商品与“电影”品类偏好等。</p></blockquote><p>再比如，用户更常在饭点的时间下载外卖app，因此，引入两个特征的组合是非常有意义的。</p><p>如何表示两个特征的组合呢？一种直接的方法就是采用多项式模型来表示两个特征的组合，xixi为<strong>第ii个特征的取值</strong>（注意和以往表示第ii个样本的特征向量的区别），xixjxixj表示特征xixi和xjxj的特征组合，其系数wijwij即为我们学习的参数，也是xixjxixj组合的重要程度：</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.w6chi2jyvcw.webp alt=image></p><p>式1-1也可以称为<strong>Poly2</strong>(degree-2 poly-nomial mappings)模型。注意到式子1-1中参数的个数是非常多的！一次项有d+1个，二次项（即组合特征的参数）共有d(d−1)2d(d−1)2个，而参数与参数之间彼此独立，在稀疏场景下，二次项的训练是很困难的。因为要训练wijwij，需要有大量的xixi和xjxj都非零的样本（只有非零组合才有意义）。而样本本身是稀疏的，满足xixj≠0xixj≠0的样本会非常少，样本少则难以估计参数wijwij，训练出来容易导致模型的过拟合。</p><p>为此，Rendle于2010年提出FM模型，它能很好的求解式1-1，其特点如下：</p><ul><li>FM模型<strong>可以在非常稀疏的情况下</strong>进行参数估计</li><li>FM模型是<strong>线性时间复杂度</strong>的，可以直接使用原问题进行求解，而且不用像SVM一样依赖支持向量。</li><li>FM模型是一个<strong>通用</strong>的模型，其训练数据的特征取值可以是任意实数。而其它最先进的分解模型对输入数据有严格的限制。FMs可以模拟MF、SVD++、PITF或FPMC模型。</li></ul><h3 id=fm模型>FM模型<a hidden class=anchor aria-hidden=true href=#fm模型>#</a></h3><p>前面提到过，式1-1的参数难以训练时因为训练数据的稀疏性。对于不同的特征对xi,xjxi,xj和xi,xkxi,xk，式1-1认为是完全独立的，对参数wijwij和wikwik分别进行训练。而实际上并非如此，不同的特征之间进行组合并非完全独立，如下图所示:</p><p><img loading=lazy src=https://www.hrwhisper.me/images/machine-learning-fm-ffm-deepfm-deepffm/fm-feaure-pair.png alt=fm-feaure-pair></p><p>回想矩阵分解，一个rating可以分解为user矩阵和item矩阵，如下图所示：</p><p><img loading=lazy src=https://www.hrwhisper.me/images/machine-learning-fm-ffm-deepfm-deepffm/fm-matrix-factorization.png alt=fm-matrix-factorization></p><p>分解后得到user和item矩阵的维度分别为nknk和kmkm，（k一般由用户指定），相比原来的rating矩阵，空间占用得到降低，并且分解后的user矩阵暗含着user偏好，Item矩阵暗含着item的属性，而user矩阵乘上item矩阵就是rating矩阵中用户对item的评分。</p><p>因此，参考矩阵分解的过程，FM模型也将式1-1的二次项参数wijwij进行分解:</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.4lwjpf9xhek0.webp alt=image></p><p>其中vivi是第ii维特征的隐向量，其长度为k(k≪d)k(k≪d)。 (vi⋅vj)(vi⋅vj)为内积，其乘积为原来的wijwij，即 ^wij=(vi⋅vj)=∑kf=1vi,f⋅vj,fw^ij=(vi⋅vj)=∑f=1kvi,f⋅vj,f</p><p>为了方便说明，考虑下面的数据集（实际中应该进行one-hot编码，但并不影响此处的说明）：</p><table><thead><tr><th>数据集</th><th>Clicked?</th><th>Publisher</th><th>Advertiser</th><th>Poly2参数</th><th>FM参数</th></tr></thead><tbody><tr><td>训练集</td><td>1</td><td>NBC</td><td>Nike</td><td>wNBC,NikewNBC,Nike</td><td>VNBC⋅VNikeVNBC⋅VNike</td></tr><tr><td>训练集</td><td>0</td><td>EPSN</td><td>Adidas</td><td>wEPSN,AdidaswEPSN,Adidas</td><td>VEPSN⋅VAdidasVEPSN⋅VAdidas</td></tr><tr><td>测试集</td><td>?</td><td>NBC</td><td>Adidas</td><td>wNBC,AdidaswNBC,Adidas</td><td>VNBC⋅VAdidas</td></tr></tbody></table><p>对于上面的训练集，没有（NBC，Adidas）组合，因此，Poly2模型就无法学习到参数wNBC,AdidaswNBC,Adidas。而FM模型可以通过特征组合(NBC，Nike)、(EPSN，Adidas) 分别学习到隐向量VNBCVNBC和VAdidasVAdidas，这样使得在测试集中得以进行预测。</p><p>更一般的，经过分解，式2-1的参数个数减少为kdkd个，对比式1-1，参数个数大大减少。使用小的k，<strong>使得模型能够提高在稀疏情况下的泛化性能</strong>。此外，将wijwij进行分解，使得不同的特征对不再是完全独立的，而它们的关联性可以用隐式因子表示，这将使得有更多的数据可以用于模型参数的学习。比如xi,xjxi,xj与xi,xkxi,xk的参数分别为：⟨vi,vj⟩⟨vi,vj⟩和⟨vi,vk⟩⟨vi,vk⟩，它们都可以用来学习vivi，更一般的，<strong>包含xixj≠0&amp;i≠jxixj≠0&amp;i≠j的所有样本都能用来学习vivi</strong>，很大程度上避免了数据稀疏性的影响。</p><p>此外，式2-1的复杂度可以从O(kd2)O(kd2)优化到O(kd)O(kd)：</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.7iob55wpe8c0.webp alt=image></p><p>可以看出，FM模型可以在线性的时间做出预测。</p><h3 id=fm模型学习>FM模型学习<a hidden class=anchor aria-hidden=true href=#fm模型学习>#</a></h3><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.tr0c0boniv4.webp alt=image></p><p>在2-4式中，∑dj=1vj,fxj∑j=1dvj,fxj只与ff有关而与ii无关，在每次迭代过程中，可以预先对所有ff的∑dj=1vj,fxj∑j=1dvj,fxj进行计算，复杂度O(kd)O(kd)，就能在常数时间O(1)O(1)内得到vi,fvi,f的梯度。而对于其它参数w0w0和wiwi，显然也是在常数时间内计算梯度。此外，更新参数只需要O(1)O(1), 一共有1+d+kd1+d+kd个参数，因此FM参数训练的复杂度也是O(kd)O(kd)。</p><p>所以说，FM模型是一种高效的模型，<strong>是线性时间复杂度的</strong>，可以在线性的时间做出训练和预测。</p><h3 id=ffm模型>FFM模型<a hidden class=anchor aria-hidden=true href=#ffm模型>#</a></h3><p>考虑下面的数据集：</p><table><thead><tr><th>Clicked?</th><th>Publisher(P)</th><th>Advertiser(A)</th><th>Gender(G)</th></tr></thead><tbody><tr><td>1</td><td>EPSN</td><td>Nike</td><td>Male</td></tr><tr><td>0</td><td>NBC</td><td>Adidas</td><td>Female</td></tr></tbody></table><p>对于第一条数据来说，FM模型的二次项为：wEPSN⋅wNike+wEPSN⋅wMale+wNike⋅wMalewEPSN⋅wNike+wEPSN⋅wMale+wNike⋅wMale。（这里只是把上面的v符合改成了w）每个特征只用一个隐向量来学习和其它特征的潜在影响。对于上面的例子中，Nike是广告主，Male是用户的性别，描述（EPSN，Nike）和（EPSN，Male）特征组合，FM模型都用同一个wESPNwESPN，而实际上，ESPN作为广告商，其对广告主和用户性别的潜在影响可能是不同的。</p><p>因此，Yu-Chin Juan借鉴Michael Jahrer的论文（Ensemble of collaborative filtering and feature engineered models for click through rate prediction），将field概念引入FM模型。</p><p>field是什么呢？即相同性质的特征放在一个field。比如EPSN、NBC都是属于广告商field的，Nike、Adidas都是属于广告主field，Male、Female都是属于性别field的。简单的说，同一个类别特征进行one-hot编码后生成的数值特征都可以放在同一个field中，比如最开始的例子中Day=26/11/15 Day=19/2/15可以放于同一个field中。如果是数值特征而非类别，可以直接作为一个field。</p><p>引入了field后，对于刚才的例子来说，二次项变为：</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.x5hy1fhe3pc.webp alt=image></p><ul><li>对于特征组合（EPSN，Nike）来说，其隐向量采用的是wEPSN,AwEPSN,A和wNike,PwNike,P，对于wEPSN,AwEPSN,A这是因为Nike属于广告主（Advertiser）的field，而第二项wNike,PwNike,P则是EPSN是广告商（Publisher）的field。</li><li>再举个例子，对于特征组合（EPSN，Male）来说，wEPSN,GwEPSN,G 是因为Male是用户性别(Gender)的field，而第二项wMale,PwMale,P是因为EPSN是广告商（Publisher）的field。</li></ul><p>下面的图来自criteo，很好的表示了三个模型的区别</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.icbdeukz4ds.webp alt=image></p><h3 id=ffm-数学公式>FFM 数学公式<a hidden class=anchor aria-hidden=true href=#ffm-数学公式>#</a></h3><p>因此，FFM的数学公式表示为：</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.3pj11jz8f720.webp alt=image></p><p>其中fifi和fjfj分别代表第i个特征和第j个特征所属的field。若field有ff个，隐向量的长度为k，则二次项系数共有dfkdfk个，远多于FM模型的dkdk个。此外，隐向量和field相关，并不能像FM模型一样将二次项化简，计算的复杂度是d2kd2k。</p><p>通常情况下，每个隐向量只需要学习特定field的表示，所以有kFFM≪kFMkFFM≪kFM。</p><h3 id=ffm-模型学习>FFM 模型学习<a hidden class=anchor aria-hidden=true href=#ffm-模型学习>#</a></h3><p>为了方便推导，这里省略FFM的一次项和常数项，公式为：</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.3izlnlm6cfy0.webp alt=image></p><p>注意到∂Lerr∂ϕ∂Lerr∂ϕ和参数无关，每次更新模型时，只需要计算一次，之后直接调用结果即可。对于总共有dfkdfk个模型参数的计算来说，使用这种方式能极大提升运算效率。</p><p>第二个trick是FFM的学习率是随迭代次数变化的，具体的是采用<a href=https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad>AdaGrad</a>算法，这里进行简单的介绍。</p><p>Adagrad算法能够在训练中自动的调整学习率，<strong>对于稀疏的参数增加学习率，而稠密的参数则降低学习率。因此，Adagrad非常适合处理稀疏数据。</strong></p><p>设gt,jgt,j为第t轮第j个参数的梯度，则SGD和采用Adagrad的参数更新公式分别如下：</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.686f681rg2o0.webp alt=image></p><p>可以看出，Adagrad在学习率ηη上还除以一项√Gt,jj+ϵGt,jj+ϵ，这是什么意思呢？ϵϵ为平滑项，防止分母为0，Gt,jj=∑tι=1g2ι,jjGt,jj=∑ι=1tgι,jj2即Gt,jjGt,jj为对角矩阵，每个对角线位置j,jj,j的值为参数wjwj每一轮的平方和，可以看出，随着迭代的进行，每个参数的历史梯度累加到一起，使得每个参数的学习率逐渐减小。</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.30c9v20hth40.webp alt=image></p><h3 id=实现的trick>实现的trick<a hidden class=anchor aria-hidden=true href=#实现的trick>#</a></h3><p>除了上面提到的梯度分步计算和自适应学习率两个trick外，还有：</p><blockquote><ol><li>OpenMP多核并行计算。OpenMP是用于共享内存并行系统的多处理器程序设计的编译方案，便于移植和多核扩展[<a href=http://openmp.org/wp/openmp-specifications/>12]</a>。FFM的源码采用了OpenMP的API，对参数训练过程SGD进行了多线程扩展，支持多线程编译。因此，OpenMP技术极大地提高了FFM的训练效率和多核CPU的利用率。在训练模型时，输入的训练参数ns_threads指定了线程数量，一般设定为CPU的核心数，便于完全利用CPU资源。</li><li>SSE3指令并行编程。SSE3全称为数据流单指令多数据扩展指令集3，是CPU对数据层并行的关键指令，主要用于多媒体和游戏的应用程序中[<a href=http://blog.csdn.net/gengshenghong/article/details/7008704>13]</a>。SSE3指令采用128位的寄存器，同时操作4个单精度浮点数或整数。SSE3指令的功能非常类似于向量运算。例如，a和b采用SSE3指令相加（a和b分别包含4个数据），其功能是a种的4个元素与b中4个元素对应相加，得到4个相加后的值。采用SSE3指令后，向量运算的速度更加快捷，这对包含大量向量运算的FFM模型是非常有利的。</li></ol></blockquote><p>除了上面的技巧之外，FFM的实现中还有很多调优技巧需要探索。例如，代码是按field和特征的编号申请参数空间的，如果选取了非连续或过大的编号，就会造成大量的内存浪费；在每个样本中加入值为1的新特征，相当于引入了因子化的一次项，避免了缺少一次项带来的模型偏差等。</p><h3 id=适用范围和使用技巧>适用范围和使用技巧<a hidden class=anchor aria-hidden=true href=#适用范围和使用技巧>#</a></h3><p>在FFM原论文中，作者指出，FFM模型对于one-hot后类别特征十分有效，但是如果数据不够稀疏，可能相比其它模型提升没有稀疏的时候那么大，此外，对于数值型的数据效果不是特别的好。</p><p>在Github上有FFM的<a href=https://github.com/guestwalk/libffm>开源实现</a>，要使用FFM模型，特征需要转化为“<strong>field_id:feature_id:value</strong>”格式，相比LibSVM的格式多了field_id，即特征所属的field的编号，feature_id是特征编号，value为特征的值。</p><p>此外，美团点评的文章中，提到了训练FFM时的一些注意事项：</p><blockquote><p>第一，样本归一化。FFM默认是进行样本数据的归一化的 。若不进行归一化，很容易造成数据inf溢出，进而引起梯度计算的nan错误。因此，样本层面的数据是推荐进行归一化的。</p><p>第二，特征归一化。CTR/CVR模型采用了多种类型的源特征，包括数值型和categorical类型等。但是，categorical类编码后的特征取值只有0或1，较大的数值型特征会造成样本归一化后categorical类生成特征的值非常小，没有区分性。例如，一条用户-商品记录，用户为“男”性，商品的销量是5000个（假设其它特征的值为零），那么归一化后特征“sex=male”（性别为男）的值略小于0.0002，而“volume”（销量）的值近似为1。特征“sex=male”在这个样本中的作用几乎可以忽略不计，这是相当不合理的。因此，将源数值型特征的值归一化到[0,1]是非常必要的。</p><p>第三，省略零值特征。从FFM模型的表达式(3-1)可以看出，零值特征对模型完全没有贡献。包含零值特征的一次项和组合项均为零，对于训练模型参数或者目标值预估是没有作用的。因此，可以省去零值特征，提高FFM模型训练和预测的速度，这也是稀疏样本采用FFM的显著优势。</p></blockquote><hr><p>参考: <a href=https://www.hrwhisper.me/machine-learning-fm-ffm-deepfm-deepffm/>https://www.hrwhisper.me/machine-learning-fm-ffm-deepfm-deepffm/</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://reid00.github.io/en/tags/fm/>FM</a></li><li><a href=https://reid00.github.io/en/tags/ffm/>FFM</a></li></ul><nav class=paginav><a class=prev href=https://reid00.github.io/en/posts/ml/ctr%E5%8F%91%E5%B1%95/><span class=title>« Prev</span><br><span>CTR发展</span></a>
<a class=next href=https://reid00.github.io/en/posts/os_network/%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/><span class=title>Next »</span><br><span>进程与线程基础知识</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2023 <a href=https://reid00.github.io/en/>Reid's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>