<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>机器学习之常见损失函数 | Reid's Blog</title>
<meta name=keywords content="损失函数"><meta name=description content="机器学习之常见损失函数"><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/en/posts/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%B8%B8%E8%A7%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><head><meta name=referrer content="no-referrer"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK",{anonymize_ip:!1})}</script><meta property="og:title" content="机器学习之常见损失函数"><meta property="og:description" content="机器学习之常见损失函数"><meta property="og:type" content="article"><meta property="og:url" content="https://reid00.github.io/en/posts/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%B8%B8%E8%A7%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-16T19:35:23+08:00"><meta property="article:modified_time" content="2023-03-16T19:35:23+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="机器学习之常见损失函数"><meta name=twitter:description content="机器学习之常见损失函数"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://reid00.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"机器学习，深度学习，知识图谱相关","item":"https://reid00.github.io/en/posts/ml/"},{"@type":"ListItem","position":3,"name":"机器学习之常见损失函数","item":"https://reid00.github.io/en/posts/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%B8%B8%E8%A7%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"机器学习之常见损失函数","name":"机器学习之常见损失函数","description":"机器学习之常见损失函数","keywords":["损失函数"],"articleBody":"简介 损失函数用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。\n损失函数分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。\n常见的损失函数以及其优缺点如下：\n1. 0-1损失函数(zero-one loss) 0-1损失是指预测值和目标值不相等为1， 否则为0:\n特点：\n(1) 0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用.\n(2) 感知机就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足 时认为相等，\n2. 绝对值损失函数 绝对值损失函数是计算预测值与目标值的差的绝对值：\n3. log对数损失函数 log对数损失函数的标准形式如下：\n特点：\n(1) log对数损失函数能非常好的表征概率分布，在很多场景尤其是多分类，如果需要知道结果属于每个类别的置信度，那它非常适合。\n(2) 健壮性不强，相比于hinge loss对噪声更敏感。\n(3) 辑回归的损失函数就是log对数损失函数。\n4. 平方损失函数 平方损失函数标准形式如下：\n特点：\n(1)经常应用与回归问题\n5. 指数损失函数（exponential loss） 指数损失函数的标准形式如下：\n特点：\n(1)对离群点、噪声非常敏感。经常用在AdaBoost算法中。\n6. Hinge 损失函数 Hinge损失函数标准形式如下：\n特点：\n(1) hinge损失函数表示如果被分类正确，损失为0，否则损失就为 。SVM就是使用这个损失函数。\n(2) 一般的 是预测值，在-1到1之间， 是目标值(-1或1)。其含义是， 的值在-1和+1之间就可以了，并不鼓励 ，即并不鼓励分类器过度自信，让某个正确分类的样本距离分割线超过1并不会有任何奖励，从而使分类器可以更专注于整体的误差。\n(3) 健壮性相对较高，对异常点、噪声不敏感，但它没太好的概率解释。\n7. 感知损失(perceptron loss)函数 感知损失函数的标准形式如下：\n特点：\n(1)是Hinge损失函数的一个变种，Hinge loss对判定边界附近的点(正确端)惩罚力度很高。而perceptron loss只要样本的判定类别正确的话，它就满意，不管其判定边界的距离。它比Hinge loss简单，因为不是max-margin boundary，所以模型的泛化能力没 hinge loss强。\n8. 交叉熵损失函数 (Cross-entropy loss function) 交叉熵损失函数的标准形式如下:\n注意公式中 表示样本， 表示实际的标签， 表示预测的输出， 表示样本总数量。\n特点：\n(1)本质上也是一种对数似然函数，可用于二分类和多分类任务中。\n二分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）：\n多分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）：\n(2)当使用sigmoid作为激活函数的时候，常用交叉熵损失函数而不用均方误差损失函数，因为它可以完美解决平方损失函数权重更新过慢的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。\n最后奉献上交叉熵损失函数的实现代码：cross_entropy.\n这里需要更正一点，对数损失函数和交叉熵损失函数应该是等价的！！！（此处感谢\n@Areshyy\n的指正，下面说明也是由他提供）\n下面来具体说明：\n相关高频问题： 1.交叉熵函数与最大似然函数的联系和区别？ 区别：交叉熵函数使用来描述模型预测值和真实值的差距大小，越大代表越不相近；似然函数的本质就是衡量在某个参数下，整体的估计和真实的情况一样的概率，越大代表越相近。\n联系：交叉熵函数可以由最大似然函数在伯努利分布的条件下推导出来，或者说最小化交叉熵函数的本质就是对数似然函数的最大化。\n怎么推导的呢？我们具体来看一下。\n设一个随机变量 满足伯努利分布，\n则 的概率密度函数为：\n因为我们只有一组采样数据 ，我们可以统计得到 和 的值，但是 的概率是未知的，接下来我们就用极大似然估计的方法来估计这个 值。\n对于采样数据 ，其对数似然函数为:\n可以看到上式和交叉熵函数的形式几乎相同，极大似然估计就是要求这个式子的最大值。而由于上面函数的值总是小于0，一般像神经网络等对于损失函数会用最小化的方法进行优化，所以一般会在前面加一个负号，得到交叉熵函数（或交叉熵损失函数）：\n这个式子揭示了交叉熵函数与极大似然估计的联系，最小化交叉熵函数的本质就是对数似然函数的最大化。\n现在我们可以用求导得到极大值点的方法来求其极大似然估计，首先将对数似然函数对 进行求导，并令导数为0，得到\n消去分母，得：\n所以:\n这就是伯努利分布下最大似然估计求出的概率 。\n2. 在用sigmoid作为激活函数的时候，为什么要用交叉熵损失函数，而不用均方误差损失函数？ 其实这个问题求个导，分析一下两个误差函数的参数更新过程就会发现原因了。\n对于均方误差损失函数，常常定义为：\n其中 是我们期望的输出， 为神经元的实际输出（ ）。在训练神经网络的时候我们使用梯度下降的方法来更新 和 ，因此需要计算代价函数对 和 的导数：\n然后更新参数 和 ：\n因为sigmoid的性质，导致 在 取大部分值时会很小（如下图标出来的两端，几乎接近于平坦），这样会使得 很小，导致参数 和 更新非常慢。\n那么为什么交叉熵损失函数就会比较好了呢？同样的对于交叉熵损失函数，计算一下参数更新的梯度公式就会发现原因。交叉熵损失函数一般定义为：\n其中 是我们期望的输出， 为神经元的实际输出（ ）。同样可以看看它的导数：\n另外，\n所以有：\n所以参数更新公式为：\n可以看到参数更新公式中没有 这一项，权重的更新受 影响，受到误差的影响，所以当误差大的时候，权重更新快；当误差小的时候，权重更新慢。这是一个很好的性质。\n所以当使用sigmoid作为激活函数的时候，常用交叉熵损失函数而不用均方误差损失函数。\n","wordCount":"2323","inLanguage":"en","datePublished":"2023-03-16T19:35:23+08:00","dateModified":"2023-03-16T19:35:23+08:00","author":[{"@type":"Person","name":"Reid"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://reid00.github.io/en/posts/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%B8%B8%E8%A7%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},"publisher":{"@type":"Organization","name":"Reid's Blog","logo":{"@type":"ImageObject","url":"https://reid00.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://reid00.github.io/en/ accesskey=h title="Reid's Blog (Alt + H)">Reid's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://reid00.github.io/en/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://reid00.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://reid00.github.io/en/categories/ title=Categorys><span>Categorys</span></a></li><li><a href=https://reid00.github.io/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://reid00.github.io/en/>Home</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/ml/>机器学习，深度学习，知识图谱相关</a></div><h1 class=post-title>机器学习之常见损失函数</h1><div class=post-description>机器学习之常见损失函数</div><div class=post-meta><span title='2023-03-16 19:35:23 +0800 +0800'>2023-03-16 19:35</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;2323 words&nbsp;·&nbsp;Reid</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e7%ae%80%e4%bb%8b aria-label=简介>简介</a></li><li><a href=#1-0-1%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0zero-one-loss aria-label="1. 0-1损失函数(zero-one loss)">1. 0-1损失函数(zero-one loss)</a></li><li><a href=#2-%e7%bb%9d%e5%af%b9%e5%80%bc%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0 aria-label="2. 绝对值损失函数">2. 绝对值损失函数</a></li><li><a href=#3-log%e5%af%b9%e6%95%b0%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0 aria-label="3. log对数损失函数">3. log对数损失函数</a></li><li><a href=#4-%e5%b9%b3%e6%96%b9%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0 aria-label="4. 平方损失函数">4. 平方损失函数</a></li><li><a href=#5-%e6%8c%87%e6%95%b0%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0exponential-loss aria-label="5. 指数损失函数（exponential loss）">5. 指数损失函数（exponential loss）</a></li><li><a href=#6-hinge-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0 aria-label="6. Hinge 损失函数">6. Hinge 损失函数</a></li><li><a href=#7-%e6%84%9f%e7%9f%a5%e6%8d%9f%e5%a4%b1perceptron-loss%e5%87%bd%e6%95%b0 aria-label="7. 感知损失(perceptron loss)函数">7. 感知损失(perceptron loss)函数</a></li><li><a href=#8-%e4%ba%a4%e5%8f%89%e7%86%b5%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0-cross-entropy-loss-function aria-label="8. 交叉熵损失函数 (Cross-entropy loss function)">8. 交叉熵损失函数 (Cross-entropy loss function)</a></li><li><a href=#%e7%9b%b8%e5%85%b3%e9%ab%98%e9%a2%91%e9%97%ae%e9%a2%98 aria-label=相关高频问题：>相关高频问题：</a><ul><li><a href=#1%e4%ba%a4%e5%8f%89%e7%86%b5%e5%87%bd%e6%95%b0%e4%b8%8e%e6%9c%80%e5%a4%a7%e4%bc%bc%e7%84%b6%e5%87%bd%e6%95%b0%e7%9a%84%e8%81%94%e7%b3%bb%e5%92%8c%e5%8c%ba%e5%88%ab aria-label=1.交叉熵函数与最大似然函数的联系和区别？>1.交叉熵函数与最大似然函数的联系和区别？</a></li><li><a href=#2-%e5%9c%a8%e7%94%a8sigmoid%e4%bd%9c%e4%b8%ba%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0%e7%9a%84%e6%97%b6%e5%80%99%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e7%94%a8%e4%ba%a4%e5%8f%89%e7%86%b5%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e8%80%8c%e4%b8%8d%e7%94%a8%e5%9d%87%e6%96%b9%e8%af%af%e5%b7%ae%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0 aria-label="2. 在用sigmoid作为激活函数的时候，为什么要用交叉熵损失函数，而不用均方误差损失函数？">2. 在用sigmoid作为激活函数的时候，为什么要用交叉熵损失函数，而不用均方误差损失函数？</a></li></ul></li></ul></div></details></div><div class=post-content><h3 id=简介>简介<a hidden class=anchor aria-hidden=true href=#简介>#</a></h3><p>损失函数用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。</p><p>损失函数分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。</p><p>常见的损失函数以及其优缺点如下：</p><h3 id=1-0-1损失函数zero-one-loss>1. 0-1损失函数(zero-one loss)<a hidden class=anchor aria-hidden=true href=#1-0-1损失函数zero-one-loss>#</a></h3><p>0-1损失是指预测值和目标值不相等为1， 否则为0:</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PAHWcPnIKojZaRNiapm2gQ9vL5TibKIf6TgvRdF07gTuERYt7ibFuZdaBmCZ93rxqbDW/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>特点：</p><p>(1) 0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用.</p><p>(2) 感知机就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PEiaZXEibljgib0JFSYUclfOQOwu9wZSsUEUmibGlrPAy9mSJFDzruS7gFclUxgHWPYEv/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
时认为相等，</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PfUUbMY31Mq3Mam4jlopo9zsiaXGqbLYcmAlzJwOQ9dHX7VH05FCFCXWxib3iauFFoY9/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><h3 id=2-绝对值损失函数>2. 绝对值损失函数<a hidden class=anchor aria-hidden=true href=#2-绝对值损失函数>#</a></h3><p>绝对值损失函数是计算预测值与目标值的差的绝对值：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PqqNxtic3kLa4vYcib5nNB3lTMHEDjPXAElB9xoXs0aeBhuStmETzxVS0EPzSfA0uWh/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><h3 id=3-log对数损失函数>3. log对数损失函数<a hidden class=anchor aria-hidden=true href=#3-log对数损失函数>#</a></h3><p>log对数损失函数的标准形式如下：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PpJ6R3pqjoFLLrTWaVfytyrJ6H4Hcw3wTjcqnCQhry1h3toP6RuGjyLSRaDJ74Nqn/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>特点：</p><p>(1) log对数损失函数能非常好的表征概率分布，在很多场景尤其是多分类，如果需要知道结果属于每个类别的置信度，那它非常适合。</p><p>(2) 健壮性不强，相比于hinge loss对噪声更敏感。</p><p>(3) 辑回归的损失函数就是log对数损失函数。</p><h3 id=4-平方损失函数>4. 平方损失函数<a hidden class=anchor aria-hidden=true href=#4-平方损失函数>#</a></h3><p>平方损失函数标准形式如下：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423P579bRBVDZH3kdnV8ng8iaWqDDRsuhoyRd7ehbn5PbiaAoq9SMz3JSYMKCJLOmsmNAo/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>特点：</p><p>(1)经常应用与回归问题</p><h3 id=5-指数损失函数exponential-loss>5. 指数损失函数（exponential loss）<a hidden class=anchor aria-hidden=true href=#5-指数损失函数exponential-loss>#</a></h3><p>指数损失函数的标准形式如下：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PyEGzwtbyLibWJAlgstoO1SoX8lepgkt73esz0nht681l5icwQ4l9DVyTUCUNibnBOh0/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>特点：</p><p>(1)对离群点、噪声非常敏感。经常用在AdaBoost算法中。</p><h3 id=6-hinge-损失函数>6. Hinge 损失函数<a hidden class=anchor aria-hidden=true href=#6-hinge-损失函数>#</a></h3><p>Hinge损失函数标准形式如下：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PCOLBC18NUlBAE7BLicXibNA32tRmScoDXF0GLbvgn7k3LzQ555HHdKPcwwFUK6eMt5/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>特点：</p><p>(1) hinge损失函数表示如果被分类正确，损失为0，否则损失就为 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PyXKaGP2GxfmgicevnHDTBlic7E4d6Oremd0MfF8mmtQmCAlBuZIGBogV9vF5X9Tt8p/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
。SVM就是使用这个损失函数。</p><p>(2) 一般的 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PibEDGpaLkvqynkmNCpWrKhzwNvw130IjjjMkh7nOiaFEsQ97r8ZsicfGGhDAhjKLDxw/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
是预测值，在-1到1之间， <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423Pn2s1e1VW5syWX3WzKUtnGQ3ecib4ibZLtCfUjg8aHd8n57ypJr14yhHOwF4ZJW0rFu/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
是目标值(-1或1)。其含义是， <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PibEDGpaLkvqynkmNCpWrKhzwNvw130IjjjMkh7nOiaFEsQ97r8ZsicfGGhDAhjKLDxw/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
的值在-1和+1之间就可以了，并不鼓励 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PIKCdeIsCNfaOkGPZkh0fNfKGvN1H1zV5N3xMUXZVoxoNGCGUFqbzhlh8k16AlwPA/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
，即并不鼓励分类器过度自信，让某个正确分类的样本距离分割线超过1并不会有任何奖励，从而使分类器可以更专注于整体的误差。</p><p>(3) 健壮性相对较高，对异常点、噪声不敏感，但它没太好的概率解释。</p><h3 id=7-感知损失perceptron-loss函数>7. 感知损失(perceptron loss)函数<a hidden class=anchor aria-hidden=true href=#7-感知损失perceptron-loss函数>#</a></h3><p>感知损失函数的标准形式如下：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PqP49Gg8aBhZhM93j7NntWbicoQfL6CVmrlAlicZsJxsF46nD2BPw0rYrb47nfYeena/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>特点：</p><p>(1)是Hinge损失函数的一个变种，Hinge loss对判定边界附近的点(正确端)惩罚力度很高。而perceptron loss只要样本的判定类别正确的话，它就满意，不管其判定边界的距离。它比Hinge loss简单，因为不是max-margin boundary，所以模型的泛化能力没 hinge loss强。</p><h3 id=8-交叉熵损失函数-cross-entropy-loss-function>8. 交叉熵损失函数 (Cross-entropy loss function)<a hidden class=anchor aria-hidden=true href=#8-交叉熵损失函数-cross-entropy-loss-function>#</a></h3><p>交叉熵损失函数的标准形式如下:</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PXrovpAAhG5B3VN8fR2BDQcGVRPMFSnbUAt0lRIicTkM1CQE06IcrMFGicBGBHiaQmP7/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>注意公式中 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PzMcQdCjSaPKEsuGA3qVia2V01iaiavG9vqGrnXGIU4bAFyDzRAcI1ziakGdGibxf9h480/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
表示样本， <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423Pn2s1e1VW5syWX3WzKUtnGQ3ecib4ibZLtCfUjg8aHd8n57ypJr14yhHOwF4ZJW0rFu/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
表示实际的标签， <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PpKZVbiaplnUZSB9d5x53qDXYwv0qv0aIyqoWqUT5tZxZyISBzJftiaR9Rl6TkeARuz/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
表示预测的输出， <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PFGMYb5H2ZiaWic0iad06zycVSAugTlBWY8gTqn2nwau6MuTu4dK2RuS43y8qgp4yyo3/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
表示样本总数量。</p><p>特点：</p><p>(1)本质上也是一种对数似然函数，可用于二分类和多分类任务中。</p><p>二分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PKrdLEmnP7jU3ITsicjmcibLeJcdYKdb6xyt4wOQX6KltU8R6XXNddyYHic4MAbUibach/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>多分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423P4wq5FsjAPz3LVIsG737np2C93hW27L7O0dkiahjotq5nHic5lfjlwuNGib2Z34O3869/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>(2)当使用sigmoid作为激活函数的时候，常用交叉熵损失函数而不用均方误差损失函数，因为它可以完美解决平方损失函数权重更新过慢的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。</p><p>最后奉献上交叉熵损失函数的实现代码：cross_entropy.</p><hr><p>这里需要更正一点，对数损失函数和交叉熵损失函数应该是等价的！！！（此处感谢</p><p>@Areshyy</p><p>的指正，下面说明也是由他提供）</p><p>下面来具体说明：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_jpg/nJZZib3qIQW4utAZSNewpcT9VHQXxgcdul66FX1VTXeIAC8nSm5BqR1eYp72ibDEAmDGCUC99jHI2vdrFdW4wfNQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><h3 id=相关高频问题>相关高频问题：<a hidden class=anchor aria-hidden=true href=#相关高频问题>#</a></h3><h4 id=1交叉熵函数与最大似然函数的联系和区别>1.交叉熵函数与最大似然函数的联系和区别？<a hidden class=anchor aria-hidden=true href=#1交叉熵函数与最大似然函数的联系和区别>#</a></h4><p>区别：交叉熵函数使用来描述模型预测值和真实值的差距大小，越大代表越不相近；似然函数的本质就是衡量在某个参数下，整体的估计和真实的情况一样的概率，越大代表越相近。</p><p>联系：交叉熵函数可以由最大似然函数在伯努利分布的条件下推导出来，或者说最小化交叉熵函数的本质就是对数似然函数的最大化。</p><p>怎么推导的呢？我们具体来看一下。</p><p>设一个随机变量 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PnuQMlBzdwEh5l9f1rMMlUEEMtvBZJl74pLxo9QUU35GOwmdRjykH502aZWBhGxmO/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
满足伯努利分布，</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423P7DgvTgxhorZuacuzVMzD2RC76VP5ibuIy5NTib78JAWibKPxmOncW5KVPWtR4GNdFrb/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>则 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PnuQMlBzdwEh5l9f1rMMlUEEMtvBZJl74pLxo9QUU35GOwmdRjykH502aZWBhGxmO/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
的概率密度函数为：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PxNyATb3sIZtaEa0UAJXungHlkGfibMxeQd8gOH9XZQOCnHYgcZtfQXq20pr2CM79t/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>因为我们只有一组采样数据 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423P4vsqzRcgClhJ3jqiawogELMHIiaxscOMpRpofibLIntWPFOAqJataDPXC6jKXiarqaeg/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
，我们可以统计得到 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PnuQMlBzdwEh5l9f1rMMlUEEMtvBZJl74pLxo9QUU35GOwmdRjykH502aZWBhGxmO/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
和 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PPUBLkgicLsR2NBUTccmRUrYlYoicPE3A5qZxebBu708UDCbicsgqe42EnFciciaibyBu6u/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
的值，但是 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PNx8Tvcrc1cDXRXs7aZAvLaJYaOoxJ1qTcVUY2ib7c6shc9jaljpickcIc4iaUG1MLkn/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
的概率是未知的，接下来我们就用极大似然估计的方法来估计这个 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PNx8Tvcrc1cDXRXs7aZAvLaJYaOoxJ1qTcVUY2ib7c6shc9jaljpickcIc4iaUG1MLkn/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
值。</p><p>对于采样数据 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423P4vsqzRcgClhJ3jqiawogELMHIiaxscOMpRpofibLIntWPFOAqJataDPXC6jKXiarqaeg/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
，其对数似然函数为:</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PicTZUMfP251kiabm571PwDhr4LcMekP32tmpb57HgiaKgVtsDHicapU56rR5vhia4HHiaj/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>可以看到上式和交叉熵函数的形式几乎相同，极大似然估计就是要求这个式子的最大值。而由于上面函数的值总是小于0，一般像神经网络等对于损失函数会用最小化的方法进行优化，所以一般会在前面加一个负号，得到交叉熵函数（或交叉熵损失函数）：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423Pahk7Bfhbwly5YXibzcRnV9uX5Xb40mcQZmwNiab5icP7D9nxl95icuz0VI2gQhg2Gpck/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>这个式子揭示了交叉熵函数与极大似然估计的联系，最小化交叉熵函数的本质就是对数似然函数的最大化。</p><p>现在我们可以用求导得到极大值点的方法来求其极大似然估计，首先将对数似然函数对 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PNx8Tvcrc1cDXRXs7aZAvLaJYaOoxJ1qTcVUY2ib7c6shc9jaljpickcIc4iaUG1MLkn/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
进行求导，并令导数为0，得到</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PibJaDKzogq6wf9O0vU9yC8LhUqTw0yZYPXRBlNzDiaTtG0IEbe0eh1PBcHrtpKO4A2/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>消去分母，得：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PiajibtOLUrQrEAsXIMX44v81G8qBvj6uibXfy7ZIHCFactRlrXoGmibt8DLWPP8Ll9NU/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>所以:</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PL9Vt79xuC2dgJo8IwmSxQWqFsOUvBnpWY5vSxzvn64Uh3qXibWDmnDjNKVic78OJ9v/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>这就是伯努利分布下最大似然估计求出的概率 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PNx8Tvcrc1cDXRXs7aZAvLaJYaOoxJ1qTcVUY2ib7c6shc9jaljpickcIc4iaUG1MLkn/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
。</p><h4 id=2-在用sigmoid作为激活函数的时候为什么要用交叉熵损失函数而不用均方误差损失函数>2. 在用sigmoid作为激活函数的时候，为什么要用交叉熵损失函数，而不用均方误差损失函数？<a hidden class=anchor aria-hidden=true href=#2-在用sigmoid作为激活函数的时候为什么要用交叉熵损失函数而不用均方误差损失函数>#</a></h4><p>其实这个问题求个导，分析一下两个误差函数的参数更新过程就会发现原因了。</p><p>对于均方误差损失函数，常常定义为：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PCg3P8SWeNrO6ib3sMMZfEibW6d9QFV5RnYmcUWTz6OiaNIkAmFgddupggGbwxWhWzO5/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>其中 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423Pn2s1e1VW5syWX3WzKUtnGQ3ecib4ibZLtCfUjg8aHd8n57ypJr14yhHOwF4ZJW0rFu/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
是我们期望的输出， <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PpKZVbiaplnUZSB9d5x53qDXYwv0qv0aIyqoWqUT5tZxZyISBzJftiaR9Rl6TkeARuz/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
为神经元的实际输出（ <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PL1Lzf4T6Hoia1Xmib2xpo2S9GTlvwj7Df5R42KfFRd78SQsGgfBJgvlQMvkVE1ic3zj/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
）。在训练神经网络的时候我们使用梯度下降的方法来更新 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PrVCdNOJK2m87tFKH1TWX841lwd6AfiaP4psKhk0ZHkTaEBfPo2JOevjzicAh8eEvKic/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
和 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PTIXUib7GqicHcJmDHOldZ7hjl12GpMMKRJfNBpfUxHcRibC3fNMMka37zxDKztO2Jmic/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
，因此需要计算代价函数对 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PrVCdNOJK2m87tFKH1TWX841lwd6AfiaP4psKhk0ZHkTaEBfPo2JOevjzicAh8eEvKic/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
和 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PTIXUib7GqicHcJmDHOldZ7hjl12GpMMKRJfNBpfUxHcRibC3fNMMka37zxDKztO2Jmic/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
的导数：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PDm01kHIeko6Afxa3yDgzyFER14ibnRZ1jic47LNsvUncJUl1TW3VhiaHpM38TPhGxRZ/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>然后更新参数 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PrVCdNOJK2m87tFKH1TWX841lwd6AfiaP4psKhk0ZHkTaEBfPo2JOevjzicAh8eEvKic/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
和 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PTIXUib7GqicHcJmDHOldZ7hjl12GpMMKRJfNBpfUxHcRibC3fNMMka37zxDKztO2Jmic/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PFNBMUvfhvdlFEOILkNBia2Bhp3gHU1BXCXHL9PEx4icMVrLqxP7Gm1KpvKIjibkicxRm/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>因为sigmoid的性质，导致 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PQpbFhwuoiampfAGKRDfC4jc8N5mUy4yROhzcJrD2TRMn3PAFutX2TVauFgYlt93U2/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
在 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PnFNNkISGj0pRAEUsWmqaiaTTf1WLjgUoN3LXJH7Apib1ibJtxiaZBHTGIApQRjpJqqGY/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
取大部分值时会很小（如下图标出来的两端，几乎接近于平坦），这样会使得 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423P6Yu8racxticu6sSvic55owan6B5CP8zHNhC5rSyvAO9tUx7zQORZTy0YrZB6hTX3Ub/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
很小，导致参数 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PrVCdNOJK2m87tFKH1TWX841lwd6AfiaP4psKhk0ZHkTaEBfPo2JOevjzicAh8eEvKic/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
和 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PTIXUib7GqicHcJmDHOldZ7hjl12GpMMKRJfNBpfUxHcRibC3fNMMka37zxDKztO2Jmic/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
更新非常慢。</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_jpg/nJZZib3qIQW4utAZSNewpcT9VHQXxgcduALUh9Ml3Z0lV7XL0pkrUaj7X2nF7y82Yl7dMAKxe1OxicrfRPEpmWsA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>那么为什么交叉熵损失函数就会比较好了呢？同样的对于交叉熵损失函数，计算一下参数更新的梯度公式就会发现原因。交叉熵损失函数一般定义为：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PXrovpAAhG5B3VN8fR2BDQcGVRPMFSnbUAt0lRIicTkM1CQE06IcrMFGicBGBHiaQmP7/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>其中 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423Pn2s1e1VW5syWX3WzKUtnGQ3ecib4ibZLtCfUjg8aHd8n57ypJr14yhHOwF4ZJW0rFu/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
是我们期望的输出， <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PpKZVbiaplnUZSB9d5x53qDXYwv0qv0aIyqoWqUT5tZxZyISBzJftiaR9Rl6TkeARuz/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
为神经元的实际输出（ <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PL1Lzf4T6Hoia1Xmib2xpo2S9GTlvwj7Df5R42KfFRd78SQsGgfBJgvlQMvkVE1ic3zj/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
）。同样可以看看它的导数：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423Pahk7BfhbwlzgGUsXaPkPXD7lSqf888oj1Mw4wXr9tvpS32EngppHZ67LQ9aQKye1/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>另外，</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PMTPNdT9MKZ7ZiacKMxwfNuSiaDQTeFcycZQuu5fiasS82hg3OHHOb7v5WjPYpdInux9/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>所以有：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PqhNlauSG2kdS1Ija8z8LtOnSGZjLBB8ORTVMRgcUiapEbnPkI14MBDNibrMsUUnYbc/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PFZejk3NKYXFo1ZvKeJCYDD62uycTHEloAdlImalafNChDOA4ngibgqsca8QLJeukq/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>所以参数更新公式为：</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423P5UWVKPmoa3bTkibC61ajRIj6PyRx3V3mOC3cZwTSLcGAtmB7QsYX8IPOOvCv7ib9tM/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p>可以看到参数更新公式中没有 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PQpbFhwuoiampfAGKRDfC4jc8N5mUy4yROhzcJrD2TRMn3PAFutX2TVauFgYlt93U2/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
这一项，权重的更新受 <img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PgDF8dZGdicIMd8diaVvgsk1crpUzhEAplssiahhKIebibtzObrWaBIXSORT4B3D9TSx7/640?wx_fmt=svg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img>
影响，受到误差的影响，<strong>所以<em>当误差大的时候，权重更新快；当误差小的时候，权重更新慢</em></strong>。这是一个很好的性质。</p><p>所以当使用sigmoid作为激活函数的时候，常用交叉熵损失函数而不用均方误差损失函数。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://reid00.github.io/en/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/>损失函数</a></li></ul><nav class=paginav><a class=prev href=https://reid00.github.io/en/posts/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B9%8B%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/><span class=title>« Prev</span><br><span>特征工程之特征选择</span>
</a><a class=next href=https://reid00.github.io/en/posts/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98/><span class=title>Next »</span><br><span>机器学习面试题</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main></body></html>