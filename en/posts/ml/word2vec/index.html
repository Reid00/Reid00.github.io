<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Word2vec | Reid's Blog</title>
<meta name=keywords content="word2vec,词向量"><meta name=description content="Word2vec"><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/en/posts/ml/word2vec/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://reid00.github.io/en/posts/ml/word2vec/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><head><meta name=referrer content="no-referrer"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script><script>var doNotTrack=!1,dnt;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK")}</script><meta property="og:title" content="Word2vec"><meta property="og:description" content="Word2vec"><meta property="og:type" content="article"><meta property="og:url" content="https://reid00.github.io/en/posts/ml/word2vec/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-16T19:35:19+08:00"><meta property="article:modified_time" content="2023-03-16T19:35:19+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="Word2vec"><meta name=twitter:description content="Word2vec"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://reid00.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"机器学习，深度学习，知识图谱相关","item":"https://reid00.github.io/en/posts/ml/"},{"@type":"ListItem","position":3,"name":"Word2vec","item":"https://reid00.github.io/en/posts/ml/word2vec/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Word2vec","name":"Word2vec","description":"Word2vec","keywords":["word2vec","词向量"],"articleBody":"Word2vec 介绍 Word2Vec是google在2013年推出的一个NLP工具，它的特点是能够将单词转化为向量来表示。首先，word2vec可以在百万数量级的词典和上亿的数据集上进行高效地训练；其次，该工具得到的训练结果——词向量（word embedding），可以很好地度量词与词之间的相似性。随着深度学习（Deep Learning）在自然语言处理中应用的普及，很多人误以为word2vec是一种深度学习算法。其实word2vec算法的背后是一个浅层神经网络(有一个隐含层的神经元网络)。另外需要强调的一点是，word2vec是一个计算word vector的开源工具。当我们在说word2vec算法或模型的时候，其实指的是其背后用于计算word vector的CBOW模型和Skip-gram模型。很多人以为word2vec指的是一个算法或模型，这也是一种谬误。\n用词向量来表示词并不是Word2Vec的首创，在很久之前就出现了。最早的词向量采用One-Hot编码，又称为一位有效编码，每个词向量维度大小为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1。转化为N维向量。\n采用One-Hot编码方式来表示词向量非常简单，但缺点也是显而易见的，一方面我们实际使用的词汇表很大，经常是百万级以上，这么高维的数据处理起来会消耗大量的计算资源与时间。另一方面，One-Hot编码中所有词向量之间彼此正交，没有体现词与词之间的相似关系。\nWord2vec 是 Word Embedding 方式之一，属于 NLP 领域。他是将词转化为「可计算」「结构化」的向量的过程。本文将讲解 Word2vec 的原理和优缺点。\n什么是 Word2vec ？ 什么是 Word Embedding ？ 在说明 Word2vec 之前，需要先解释一下 Word Embedding。 它就是将「不可计算」「非结构化」的词转化为「可计算」「结构化」的向量。\n这一步解决的是”将现实问题转化为数学问题“，是人工智能非常关键的一步。 将现实问题转化为数学问题只是第一步，后面还需要求解这个数学问题。所以 Word Embedding 的模型本身并不重要，重要的是生成出来的结果——词向量。因为在后续的任务中会直接用到这个词向量。\n什么是 Word2vec ？ Word2vec 是 Word Embedding 的方法之一。他是 2013 年由谷歌的 Mikolov 提出了一套新的词嵌入方法。\nWord2vec 在整个 NLP 里的位置可以用下图表示： Word2vec 的 2 种训练模式 CBOW(Continuous Bag-of-Words Model)和Skip-gram (Continuous Skip-gram Model)，是Word2vec 的两种训练模式。CBOW适合于数据集较小的情况，而Skip-Gram在大型语料中表现更好。下面简单做一下解释：\n词向量训练的预处理步骤：\n1. 对输入的文本生成一个词汇表，每个词统计词频，按照词频从高到低排序，取最频繁的V个词，构成一个词汇表。每个词存在一个one-hot向量，向量的维度是V，如果该词在词汇表中出现过，则向量中词汇表中对应的位置为1，其他位置全为0。如果词汇表中不出现，则向量为全0 2. 将输入文本的每个词都生成一个one-hot向量，此处注意保留每个词的原始位置，因为是上下文相关的 3. 确定词向量的维数N CBOW 通过上下文来预测当前值。相当于一句话中扣掉一个词，让你猜这个词是什么。 CBOW的处理步骤：\n确定窗口大小window，对每个词生成2*window个训练样本，(i-window, i)，(i-window+1, i)，…，(i+window-1, i)，(i+window, i) 确定batch_size，注意batch_size的大小必须是2*window的整数倍，这确保每个batch包含了一个词汇对应的所有样本 训练算法有两种：层次 Softmax 和 Negative Sampling 神经网络迭代训练一定次数，得到输入层到隐藏层的参数矩阵，矩阵中每一行的转置即是对应词的词向量 Skip-gram 用当前词来预测上下文。相当于给你一个词，让你猜前面和后面可能出现什么词。 Skip-gram处理步骤：\n确定窗口大小window，对每个词生成2*window个训练样本，(i, i-window)，(i, i-window+1)，…，(i, i+window-1)，(i, i+window) 确定batch_size，注意batch_size的大小必须是2*window的整数倍，这确保每个batch包含了一个词汇对应的所有样本 训练算法有两种：层次 Softmax 和 Negative Sampling 神经网络迭代训练一定次数，得到输入层到隐藏层的参数矩阵，矩阵中每一行的转置即是对应词的词向量 我们先来看个最简单的例子。上面说到， y 是 x 的上下文，所以 y 只取上下文里一个词语的时候，语言模型就变成： 用当前词 x 预测它的下一个词 y 但如上面所说，一般的数学模型只接受数值型输入，这里的 x 该怎么表示呢？ 显然不能用 Word2vec，因为这是我们训练完模型的产物，现在我们想要的是 x 的一个原始输入形式。\n答案是：one-hot encoder\n所谓 one-hot encoder，其思想跟特征工程里处理类别变量的 one-hot 一样。本质上是用一个只含一个 1、其他都是 0 的向量来唯一表示词语。\n我举个例子，假设全世界所有的词语总共有 V 个，这 V 个词语有自己的先后顺序，假设『吴彦祖』这个词是第1个词，『我』这个单词是第2个词，那么『吴彦祖』就可以表示为一个 V 维全零向量、把第1个位置的0变成1，而『我』同样表示为 V 维全零向量、把第2个位置的0变成1。这样，每个词语都可以找到属于自己的唯一表示。\nOK，那我们接下来就可以看看 Skip-gram 的网络结构了，x 就是上面提到的 one-hot encoder 形式的输入，y 是在这 V 个词上输出的概率，我们希望跟真实的 y 的 one-hot encoder 一样。 首先说明一点：隐层的激活函数其实是线性的，相当于没做任何处理（这也是 Word2vec 简化之前语言模型的独到之处），我们要训练这个神经网络，用反向传播算法，本质上是链式求导，在此不展开说明了，\n首先说明一点：隐层的激活函数其实是线性的，相当于没做任何处理（这也是 Word2vec 简化之前语言模型的独到之处），我们要训练这个神经网络，用反向传播算法，本质上是链式求导，在此不展开说明了，\n当模型训练完后，最后得到的其实是神经网络的权重，比如现在输入一个 x 的 one-hot encoder: [1,0,0,…,0]，对应刚说的那个词语『吴彦祖』，则在输入层到隐含层的权重里，只有对应 1 这个位置的权重被激活，这些权重的个数，跟隐含层节点数是一致的，从而这些权重组成一个向量 vx 来表示x，而因为每个词语的 one-hot encoder 里面 1 的位置是不同的，所以，这个向量 vx 就可以用来唯一表示 x。\n所以 Word2vec 本质上是一种降维操作——把词语从 one-hot encoder 形式的表示降维到 Word2vec 形式的表示。\n隐层细节 假如词汇表长度为10000，首先使用one-hot形式表示每一个单词，经过隐层300个神经元计算，最后使用Softmax层对单词概率输出。每一对单词组，前者作为x输入，后者作为y标签。\n假如我们想要学习的词向量维度为300，则需要将隐层的神经元个数设置为300(300是Google在其发布的训练模型中使用的维度，可调)。\n隐层的权重矩阵就是词向量，我们模型学习到的就是隐层的权重矩阵。 之所以这样，来看一下one-hot输入后与隐层的计算就明白了。 当使用One-hot去乘以矩阵的时候，会将某一行选择出来，即查表操作，所以权重矩阵是所有词向量组成的列表。\nCBOW 详解: CBOW 是 Continuous Bag-of-Words 的缩写，与神经网络语言模型不同的是，CBOW去掉了最耗时的非线性隐藏层\n从图中可以看出，CBOW模型预测的是 ，由于图中目标词 前后只取了各两个词，所以窗口的总大小是2。假设目标词 前后各取k个词，即窗口的大小是k，那么CBOW模型预测的将是 输入层到隐藏层\n以图2为例，输入层是四个词的one-hot向量表示，分别为 （维度都为V x 1，V是模型的训练本文中所有词的个数），记输入层到隐藏层的权重矩阵为 （维度为V x d，d是认为给定的词向量维度），隐藏层的向量为 （维度为d x 1），那么\n其实这里就是一个简单地求和平均。\n隐藏层到输出层\n记隐藏层到输出层的权重矩阵为 （维度为d x V），输出层的向量为 （维度为V x 1），那么\n注意，输出层的向量 与输入层的向量为 虽然维度是一样的，但是 并不是one-hot向量，并且向量 中的每个元素都是有意义的。例如，我们假设训练样本只有一句话“I like to eat apple”，此刻我们正在使用 I、like、eat、apple 四个词来预测 to ，输出层的结果如图3所示。\n图3 向量y的例子\n向量y中的每个元素表示我用 I、like、eat、apple 四个词预测出来的词是当元素对应的词的概率，比如是like的概率为0.05，是to的概率是0.80。由于我们想让模型预测出来的词是to，那么我们就要尽量让to的概率尽可能的大，所以我们目标是最大化函数 有了最大化的目标函数，我们接下来要做的就是求解这个目标函数，首先求 ，然后求梯度，再梯度下降，具体细节在此省略，因为这种方法涉及到softmax层，softmax每次计算都要遍历整个词表，代价十分昂贵，所以实现的时候我们不用这种方法，次softmax或者负采样来替换掉输出层，降低复杂度。\n优化方法 为了提高速度，Word2vec 经常采用 2 种加速方式：\nNegative Sample（负采样）\n本质是预测总体类别的一个子集 Hierarchical Softmax （层次Softmax, huffman树）\n本质是把 N 分类问题变成 log(N)次二分类 Word2vec 的优缺点 需要说明的是：Word2vec 是上一代的产物（18 年之前）， 18 年之后想要得到最好的效果，已经不使用 Word Embedding 的方法了，所以也不会用到 Word2vec。\n优点： 由于 Word2vec 会考虑上下文，跟之前的 Embedding 方法相比，效果要更好（但不如 18 年之后的方法） 比之前的 Embedding方 法维度更少，所以速度更快 通用性很强，可以用在各种 NLP 任务中 缺点： 由于词和向量是一对一的关系，所以多义词的问题无法解决。 Word2vec 是一种静态的方式，虽然通用性强，但是无法针对特定任务做动态优化 问题 假如使用词向量维度为300，词汇量为10000个单词，那么神经网络输入层与隐层，隐层与输出层的参数量会达到惊人的300x10000=300万！训练如词庞大的神经网络需要庞大的数据量，还要避免过拟合。因此，Google在其第二篇论文中说明了训练的trick，其创新点如下：\n将常用词对或短语视为模型中的单个”word”。 对频繁的词进行子采样以减少训练样例的数量。 在损失函数中使用”负采样(Negative Sampling)”的技术，使每个训练样本仅更新模型权重的一小部分。 子采样和负采样技术不仅降低了计算量，还提升了词向量的效果。\n对频繁词子采样 在以上例子中，可以看到频繁单词’the’的两个问题:\n对于单词对(‘fox’,’the’)，其对单词’fox’的语义表达并没有什么有效帮助，’the’在每个单词的上下文中出现都非常频繁。 预料中有很多单词对(‘the’,…)，我们应更好的学习单词’the’ Word2vec使用子采样技术来解决以上问题，根据单词的频次来削减该单词的采样率。以window size为10为例子，我们删除’the’：\n当我们训练其余单词时候，’the’不会出现在他们的上下文中。 当中心词为’the’时，训练样本数量少于10。 负采样(Negative Sampling) 训练一个网络是说，计算训练样本然后轻微调整所有的神经元权重来提高准确率。换句话说，每一个训练样本都需要更新所有神经网络的权重。\n就像如上所说，当词汇表特别大的时候，如此多的神经网络参数在如此大的数据量下，每次都要进行权重更新，负担很大。\n在每个样本训练时，只修改部分的网络参数，负采样是通过这种方式来解决这个问题的。\n当我们的神经网络训练到单词组(‘fox’, ‘quick’)时候，得到的输出或label都是一个one-hot向量，也就是说，在表示’quick’的位置数值为1，其它全为0。\n负采样是随机选择较小数量的’负(Negative)’单词(比如5个)，来做参数更新。这里的’负’表示的是网络输出向量种位置为0表示的单词。当然，’正(Positive)’(即正确单词’quick’)权重也会更新。\n论文中表述，小数量级上采用5-20，大数据集使用2-5个单词。\n我们的模型权重矩阵为300x10000，更新的单词为5个’负’词和一个’正’词，共计1800个参数，这是输出层全部3M参数的0.06%！！\n负采样的选取是和频次相关的，频次越高，负采样的概率越大： $$P(w_i) = \\frac{f(w_i)^{3/4}}{\\sum_{j=0}^n(f(w_j)^{3/4})}$$ 论文选择0.75作为指数是因为实验效果好。C语言实现的代码很有意思：首先用索引值填充多次填充词汇表中的每个单词，单词索引出现的次数为$P(w_i) * \\text{table_size}$。然后负采样只需要生成一个1到100M的整数，并用于索引表中数据。由于概率高的单词在表中出现的次数多，很可能会选择这些词。\nGloVe 模型 模型目标：进行词的向量化表示，使得向量之间尽可能多地蕴含语义和语法的信息。\n输入：语料库\n输出：词向量\n方法概述：首先基于语料库构建词的共现矩阵，然后基于共现矩阵和GloVe模型学习词向量。\nGlobal Vector融合了矩阵分解的全局统计信息和上下文信息\n常见的问题 1、文本表示哪些方法？ 基于 one-hot、tf-idf、textrank 等的 bag-of-words； 主题模型：LSA（SVD）、pLSA、LDA； 基于词向量的固定表征：Word2vec、FastText、GloVe 基于词向量的动态表征：ELMo、GPT、BERT 2、怎么从语言模型理解词向量？怎么理解分布式假设？ 上面给出的 4 个类型也是 nlp 领域最为常用的文本表示了，文本是由每个单词构成的，而谈起词向量，one-hot 是可认为是最为简单的词向量，但存在维度灾难和语义鸿沟等问题；通过构建共现矩阵并利用 SVD 求解构建词向量，则计算复杂度高；而早期词向量的研究通常来源于语言模型，比如 NNLM 和 RNNLM，其主要目的是语言模型，而词向量只是一个副产物。\n所谓分布式假设，用一句话可以表达：相同上下文语境的词有似含义。而由此引申出了 Word2vec、FastText，在此类词向量中，虽然其本质仍然是语言模型，但是它的目标并不是语言模型本身，而是词向量，其所作的一系列优化，都是为了更快更好的得到词向量。GloVe 则是基于全局语料库、并结合上下文语境构建词向量，结合了 LSA 和 Word2vec 的优点。\n3、传统的词向量有什么问题？怎么解决？各种词向量的特点是什么？ 上述方法得到的词向量是固定表征的，无法解决一词多义等问题，如“川普”。为此引入基于语言模型的动态表征方法：ELMo、GPT、BERT。\n各种词向量的特点：\nOne-hot 表示 ：维度灾难、语义鸿沟； 分布式表示 (distributed representation) 矩阵分解（LSA）：利用全局语料特征，但 SVD 求解计算复杂度大； 基于 NNLM/RNNLM 的词向量：词向量为副产物，存在效率不高等问题； Word2vec、FastText：优化效率高，但是基于局部语料； GloVe：基于全局预料，结合了 LSA 和 Word2vec 的优点； ELMo、GPT、BERT：动态特征； 4、Word2vec 和 NNLM 对比有什么区别？（Word2vecvs NNLM） 1）其本质都可以看作是语言模型；\n2）词向量只不过 NNLM 一个产物，Word2vec 虽然其本质也是语言模型，但是其专注于词向量本身，因此做了许多优化来提高计算效率：\n与 NNLM 相比，词向量直接 sum，不再拼接，并舍弃隐层； 考虑到 sofmax 归一化需要遍历整个词汇表，采用 hierarchical softmax 和 negative sampling 进行优化，hierarchical softmax 实质上生成一颗带权路径最小的哈夫曼树，让高频词搜索路劲变小；negative sampling 更为直接，实质上对每一个样本中每一个词都进行负例采样； 5、Word2vec 和 FastText 对比有什么区别？（Word2vec vs FastText） 1）都可以无监督学习词向量， FastText 训练词向量时会考虑 subword；\n2） FastText 还可以进行有监督学习进行文本分类，其主要特点：\n结构与 CBOW 类似，但学习目标是人工标注的分类结果； 采用 hierarchical softmax 对输出的分类标签建立哈夫曼树，样本中标签多的类别被分配短的搜寻路径 引入 N-gram，考虑词序特征 引入 subword 来处理长词，处理未登陆词问题； 6、GloVe 和 Word2vec、 LSA 对比有什么区别？（Word2vecvs GloVe vs LSA） 1）GloVe vs LSA\nLSA（Latent Semantic Analysis）可以基于 co-occurance matrix 构建词向量，实质上是基于全局语料采用 SVD 进行矩阵分解，然而 SVD 计算复杂度高；\nGloVe 可看作是对 LSA 一种优化的高效矩阵分解算法，采用 Adagrad 对最小平方损失进行优化；\n2）Word2vecvs GloVe\nWord2vec 是局部语料库训练的，其特征提取是基于滑窗的；而 GloVe 的滑窗是为了构建 co-occurance matrix，是基于全局语料的，可见 GloVe 需要事先统计共现概率；因此，Word2vec 可以进行在线学习，GloVe 则需要统计固定语料信息。\nWord2vec 是无监督学习，同样由于不需要人工标注；GloVe 通常被认为是无监督学习，但实际上 GloVe 还是有 label 的，即共现次数 [公式]。\nWord2vec 损失函数实质上是带权重的交叉熵，权重固定；GloVe 的损失函数是最小平方损失函数，权重可以做映射变换。\n总体来看，GloVe 可以被看作是更换了目标函数和权重函数的全局 Word2vec。\n7、 Word2vec 的两种优化方法是什么？它们的目标函数怎样确定的？训练过程又是怎样的？ 不经过优化的 CBOW 和 Skip-gram 中 , 在每个样本中每个词的训练过程都要遍历整个词汇表，也就是都需要经过 softmax 归一化，计算误差向量和梯度以更新两个词向量矩阵（这两个词向量矩阵实际上就是最终的词向量，可认为初始化不一样），当语料库规模变大、词汇表增长时，训练变得不切实际。为了解决这个问题，Word2vec 支持两种优化方法：hierarchical softmax 和 negative sampling。\n（1）基于 hierarchical softmax 的 CBOW 和 Skip-gram\nhierarchical softmax 使用一颗二叉树表示词汇表中的单词，每个单词都作为二叉树的叶子节点。对于一个大小为 V 的词汇表，其对应的二叉树包含 V-1 非叶子节点。假如每个非叶子节点向左转标记为 1，向右转标记为 0，那么每个单词都具有唯一的从根节点到达该叶子节点的由｛0 1｝组成的代号（实际上为哈夫曼编码，为哈夫曼树，是带权路径长度最短的树，哈夫曼树保证了词频高的单词的路径短，词频相对低的单词的路径长，这种编码方式很大程度减少了计算量）。\n（2）基于 negative sampling 的 CBOW 和 Skip-gram\nnegative sampling 是一种不同于 hierarchical softmax 的优化策略，相比于 hierarchical softmax，negative sampling 的想法更直接——为每个训练实例都提供负例。\n负采样算法实际上就是一个带权采样过程，负例的选择机制是和单词词频联系起来的。\n具体做法是以 N+1 个点对区间 [0,1] 做非等距切分，并引入的一个在区间 [0,1] 上的 M 等距切分，其中 M » N。源码中取 M = 10^8。然后对两个切分做投影，得到映射关系：采样时，每次生成一个 [1, M-1] 之间的整数 i，则 Table(i) 就对应一个样本；当采样到正例时，跳过（拒绝采样）。\n参考： https://zhuanlan.zhihu.com/p/44599645\n","wordCount":"556","inLanguage":"en","datePublished":"2023-03-16T19:35:19+08:00","dateModified":"2023-03-16T19:35:19+08:00","author":[{"@type":"Person","name":"Reid"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://reid00.github.io/en/posts/ml/word2vec/"},"publisher":{"@type":"Organization","name":"Reid's Blog","logo":{"@type":"ImageObject","url":"https://reid00.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://reid00.github.io/en/ accesskey=h title="Reid's Blog (Alt + H)">Reid's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://reid00.github.io/en/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://reid00.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://reid00.github.io/en/categories/ title=Categorys><span>Categorys</span></a></li><li><a href=https://reid00.github.io/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://reid00.github.io/en/>Home</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/ml/>机器学习，深度学习，知识图谱相关</a></div><h1 class=post-title>Word2vec</h1><div class=post-description>Word2vec</div><div class=post-meta><span title='2023-03-16 19:35:19 +0800 +0800'>2023-03-16 19:35</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;556 words&nbsp;·&nbsp;Reid</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#word2vec-%e4%bb%8b%e7%bb%8d aria-label="Word2vec 介绍">Word2vec 介绍</a></li><li><a href=#%e4%bb%80%e4%b9%88%e6%98%af-word2vec- aria-label="什么是 Word2vec ？">什么是 Word2vec ？</a><ul><li><a href=#%e4%bb%80%e4%b9%88%e6%98%af-word-embedding- aria-label="什么是 Word Embedding ？"><strong>什么是 Word Embedding ？</strong></a></li><li><a href=#%e4%bb%80%e4%b9%88%e6%98%af-word2vec--1 aria-label="什么是 Word2vec ？"><strong>什么是 Word2vec ？</strong></a></li></ul></li><li><a href=#word2vec-%e7%9a%84-2-%e7%a7%8d%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%bc%8f aria-label="Word2vec 的 2 种训练模式">Word2vec 的 2 种训练模式</a><ul><li><a href=#cbow aria-label=CBOW><strong>CBOW</strong></a></li><li><a href=#skip-gram aria-label=Skip-gram><strong>Skip-gram</strong></a></li></ul></li><li><a href=#%e9%9a%90%e5%b1%82%e7%bb%86%e8%8a%82 aria-label=隐层细节>隐层细节</a><ul><li><a href=#cbow-%e8%af%a6%e8%a7%a3 aria-label="CBOW 详解:">CBOW 详解:</a></li><li><a href=#%e4%bc%98%e5%8c%96%e6%96%b9%e6%b3%95 aria-label=优化方法>优化方法</a></li></ul></li><li><a href=#word2vec-%e7%9a%84%e4%bc%98%e7%bc%ba%e7%82%b9 aria-label="Word2vec 的优缺点">Word2vec 的优缺点</a><ul><li><a href=#%e4%bc%98%e7%82%b9 aria-label=优点：>优点：</a></li><li><a href=#%e7%bc%ba%e7%82%b9 aria-label=缺点：>缺点：</a></li></ul></li><li><a href=#%e9%97%ae%e9%a2%98 aria-label=问题>问题</a></li><li><a href=#%e5%af%b9%e9%a2%91%e7%b9%81%e8%af%8d%e5%ad%90%e9%87%87%e6%a0%b7 aria-label=对频繁词子采样>对频繁词子采样</a></li><li><a href=#%e8%b4%9f%e9%87%87%e6%a0%b7negative-sampling aria-label="负采样(Negative Sampling)">负采样(Negative Sampling)</a></li><li><a href=#glove-%e6%a8%a1%e5%9e%8b aria-label="GloVe 模型">GloVe 模型</a></li><li><a href=#%e5%b8%b8%e8%a7%81%e7%9a%84%e9%97%ae%e9%a2%98 aria-label=常见的问题>常见的问题</a><ul><li><a href=#1%e6%96%87%e6%9c%ac%e8%a1%a8%e7%a4%ba%e5%93%aa%e4%ba%9b%e6%96%b9%e6%b3%95 aria-label=1、文本表示哪些方法？>1、文本表示哪些方法？</a></li><li><a href=#2%e6%80%8e%e4%b9%88%e4%bb%8e%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%90%86%e8%a7%a3%e8%af%8d%e5%90%91%e9%87%8f%e6%80%8e%e4%b9%88%e7%90%86%e8%a7%a3%e5%88%86%e5%b8%83%e5%bc%8f%e5%81%87%e8%ae%be aria-label=2、怎么从语言模型理解词向量？怎么理解分布式假设？>2、怎么从语言模型理解词向量？怎么理解分布式假设？</a></li><li><a href=#3%e4%bc%a0%e7%bb%9f%e7%9a%84%e8%af%8d%e5%90%91%e9%87%8f%e6%9c%89%e4%bb%80%e4%b9%88%e9%97%ae%e9%a2%98%e6%80%8e%e4%b9%88%e8%a7%a3%e5%86%b3%e5%90%84%e7%a7%8d%e8%af%8d%e5%90%91%e9%87%8f%e7%9a%84%e7%89%b9%e7%82%b9%e6%98%af%e4%bb%80%e4%b9%88 aria-label=3、传统的词向量有什么问题？怎么解决？各种词向量的特点是什么？>3、传统的词向量有什么问题？怎么解决？各种词向量的特点是什么？</a></li><li><a href=#4word2vec-%e5%92%8c-nnlm-%e5%af%b9%e6%af%94%e6%9c%89%e4%bb%80%e4%b9%88%e5%8c%ba%e5%88%abword2vecvs-nnlm aria-label="4、Word2vec 和 NNLM 对比有什么区别？（Word2vecvs NNLM）">4、Word2vec 和 NNLM 对比有什么区别？（Word2vecvs NNLM）</a></li><li><a href=#5word2vec-%e5%92%8c-fasttext-%e5%af%b9%e6%af%94%e6%9c%89%e4%bb%80%e4%b9%88%e5%8c%ba%e5%88%abword2vec-vs-fasttext aria-label="5、Word2vec 和 FastText 对比有什么区别？（Word2vec vs FastText）">5、Word2vec 和 FastText 对比有什么区别？（Word2vec vs FastText）</a></li><li><a href=#6glove-%e5%92%8c-word2vec-lsa-%e5%af%b9%e6%af%94%e6%9c%89%e4%bb%80%e4%b9%88%e5%8c%ba%e5%88%abword2vecvs-glove-vs-lsa aria-label="6、GloVe 和 Word2vec、 LSA 对比有什么区别？（Word2vecvs GloVe vs LSA）">6、GloVe 和 Word2vec、 LSA 对比有什么区别？（Word2vecvs GloVe vs LSA）</a></li><li><a href=#7-word2vec-%e7%9a%84%e4%b8%a4%e7%a7%8d%e4%bc%98%e5%8c%96%e6%96%b9%e6%b3%95%e6%98%af%e4%bb%80%e4%b9%88%e5%ae%83%e4%bb%ac%e7%9a%84%e7%9b%ae%e6%a0%87%e5%87%bd%e6%95%b0%e6%80%8e%e6%a0%b7%e7%a1%ae%e5%ae%9a%e7%9a%84%e8%ae%ad%e7%bb%83%e8%bf%87%e7%a8%8b%e5%8f%88%e6%98%af%e6%80%8e%e6%a0%b7%e7%9a%84 aria-label="7、 Word2vec 的两种优化方法是什么？它们的目标函数怎样确定的？训练过程又是怎样的？">7、 Word2vec 的两种优化方法是什么？它们的目标函数怎样确定的？训练过程又是怎样的？</a></li></ul></li></ul></div></details></div><div class=post-content><h3 id=word2vec-介绍>Word2vec 介绍<a hidden class=anchor aria-hidden=true href=#word2vec-介绍>#</a></h3><p>Word2Vec是google在2013年推出的一个NLP工具，它的特点是能够将单词转化为向量来表示。首先，word2vec可以在百万数量级的词典和上亿的数据集上进行高效地训练；其次，该工具得到的训练结果——词向量（word embedding），<strong>可以很好地度量词与词之间的相似性</strong>。随着深度学习（Deep Learning）在自然语言处理中应用的普及，很多人误以为word2vec是一种深度学习算法。其实word2vec算法的背后是一个<strong>浅层神经网络</strong>(有一个隐含层的神经元网络)。另外需要强调的一点是，word2vec是一个计算word vector的开源工具。当我们在说word2vec算法或模型的时候，其实指的是其背后用于计算word vector的CBOW模型和Skip-gram模型。很多人以为word2vec指的是一个算法或模型，这也是一种谬误。</p><p>用词向量来表示词并不是Word2Vec的首创，在很久之前就出现了。最早的词向量采用One-Hot编码，又称为一位有效编码，每个词向量维度大小为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1。转化为N维向量。</p><p>采用One-Hot编码方式来表示词向量非常简单，但缺点也是显而易见的，一方面我们实际使用的词汇表很大，经常是百万级以上，这么高维的数据处理起来会消耗大量的计算资源与时间。另一方面，One-Hot编码中所有词向量之间彼此正交，没有体现词与词之间的相似关系。</p><p>Word2vec 是 Word Embedding 方式之一，属于 NLP 领域。他是将词转化为「可计算」「结构化」的向量的过程。本文将讲解 Word2vec 的原理和优缺点。</p><h3 id=什么是-word2vec->什么是 Word2vec ？<a hidden class=anchor aria-hidden=true href=#什么是-word2vec->#</a></h3><h4 id=什么是-word-embedding-><strong>什么是 Word Embedding ？</strong><a hidden class=anchor aria-hidden=true href=#什么是-word-embedding->#</a></h4><p>在说明 Word2vec 之前，需要先解释一下 Word Embedding。 它就是将「不可计算」「非结构化」的词转化为「可计算」「结构化」的向量。</p><p><strong>这一步解决的是”将现实问题转化为数学问题“</strong>，是人工智能非常关键的一步。
<img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.5j52y8l79200.webp alt=convert></p><p>将现实问题转化为数学问题只是第一步，后面还需要求解这个数学问题。所以 Word Embedding 的模型本身并不重要，<strong>重要的是生成出来的结果——词向量</strong>。因为在后续的任务中会直接用到这个词向量。</p><h4 id=什么是-word2vec--1><strong>什么是 Word2vec ？</strong><a hidden class=anchor aria-hidden=true href=#什么是-word2vec--1>#</a></h4><p><strong>Word2vec 是 Word Embedding 的方法之一</strong>。他是 2013 年由谷歌的 Mikolov 提出了一套新的词嵌入方法。</p><p>Word2vec 在整个 NLP 里的位置可以用下图表示：
<img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.53ewwh95a440.webp alt=pos></p><h3 id=word2vec-的-2-种训练模式>Word2vec 的 2 种训练模式<a hidden class=anchor aria-hidden=true href=#word2vec-的-2-种训练模式>#</a></h3><p>CBOW(Continuous Bag-of-Words Model)和Skip-gram (Continuous Skip-gram Model)，是Word2vec 的两种训练模式。CBOW适合于数据集较小的情况，而Skip-Gram在大型语料中表现更好。下面简单做一下解释：</p><p>词向量训练的预处理步骤：</p><pre><code>1. 对输入的文本生成一个词汇表，每个词统计词频，按照词频从高到低排序，取最频繁的V个词，构成一个词汇表。每个词存在一个one-hot向量，向量的维度是V，如果该词在词汇表中出现过，则向量中词汇表中对应的位置为1，其他位置全为0。如果词汇表中不出现，则向量为全0
2. 将输入文本的每个词都生成一个one-hot向量，此处注意保留每个词的原始位置，因为是上下文相关的
3. 确定词向量的维数N
</code></pre><h4 id=cbow><strong>CBOW</strong><a hidden class=anchor aria-hidden=true href=#cbow>#</a></h4><p>通过上下文来预测当前值。相当于一句话中扣掉一个词，让你猜这个词是什么。
<img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.10wotocignf4.webp alt=cbow></p><p>CBOW的处理步骤：</p><ol><li>确定窗口大小window，对每个词生成2*window个训练样本，(i-window, i)，(i-window+1, i)，&mldr;，(i+window-1, i)，(i+window, i)</li><li>确定batch_size，注意batch_size的大小必须是2*window的整数倍，这确保每个batch包含了一个词汇对应的所有样本</li><li>训练算法有两种：层次 Softmax 和 Negative Sampling
神经网络迭代训练一定次数，得到输入层到隐藏层的参数矩阵，矩阵中每一行的转置即是对应词的词向量</li></ol><h4 id=skip-gram><strong>Skip-gram</strong><a hidden class=anchor aria-hidden=true href=#skip-gram>#</a></h4><p>用当前词来预测上下文。相当于给你一个词，让你猜前面和后面可能出现什么词。
<img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.2z49docswrq0.webp alt=skip-gram></p><p>Skip-gram处理步骤：</p><ol><li>确定窗口大小window，对每个词生成2*window个训练样本，(i, i-window)，(i, i-window+1)，&mldr;，(i, i+window-1)，(i, i+window)</li><li>确定batch_size，注意batch_size的大小必须是2*window的整数倍，这确保每个batch包含了一个词汇对应的所有样本</li><li>训练算法有两种：层次 Softmax 和 Negative Sampling
神经网络迭代训练一定次数，得到输入层到隐藏层的参数矩阵，矩阵中每一行的转置即是对应词的词向量</li></ol><p>我们先来看个最简单的例子。上面说到， y 是 x 的上下文，所以 y 只取上下文里一个词语的时候，语言模型就变成：
<code>用当前词 x 预测它的下一个词 y</code>
但如上面所说，一般的数学模型只接受数值型输入，这里的 x 该怎么表示呢？ 显然不能用 Word2vec，因为这是我们训练完模型的产物，现在我们想要的是 x 的一个原始输入形式。</p><p>答案是：<strong>one-hot encoder</strong></p><p>所谓 one-hot encoder，其思想跟特征工程里处理类别变量的 one-hot 一样。本质上是用一个只含一个 1、其他都是 0 的向量来唯一表示词语。</p><p>我举个例子，假设全世界所有的词语总共有 V 个，这 V 个词语有自己的先后顺序，假设『吴彦祖』这个词是第1个词，『我』这个单词是第2个词，那么『吴彦祖』就可以表示为一个 V 维全零向量、把第1个位置的0变成1，而『我』同样表示为 V 维全零向量、把第2个位置的0变成1。这样，每个词语都可以找到属于自己的唯一表示。</p><p>OK，那我们接下来就可以看看 Skip-gram 的网络结构了，x 就是上面提到的 one-hot encoder 形式的输入，y 是在这 V 个词上输出的概率，我们希望跟真实的 y 的 one-hot encoder 一样。
<img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.2u9omasrzqk0.webp alt=encoder></p><p>首先说明一点：<strong>隐层的激活函数其实是线性的</strong>，相当于没做任何处理（这也是 Word2vec 简化之前语言模型的独到之处），我们要训练这个神经网络，用<strong>反向传播算法</strong>，本质上是<em>链式求导</em>，在此不展开说明了，</p><p>首先说明一点：<strong>隐层的激活函数其实是线性的</strong>，相当于没做任何处理（这也是 Word2vec 简化之前语言模型的独到之处），我们要训练这个神经网络，用<strong>反向传播算法</strong>，本质上是<em>链式求导</em>，在此不展开说明了，</p><p>当模型训练完后，最后得到的其实是<strong>神经网络的权重</strong>，比如现在输入一个 x 的 one-hot encoder: [1,0,0,…,0]，对应刚说的那个词语『吴彦祖』，则在输入层到隐含层的权重里，只有对应 1 这个位置的权重被激活，这些权重的个数，跟隐含层节点数是一致的，从而这些权重组成一个向量 vx 来表示x，而因为每个词语的 one-hot encoder 里面 1 的位置是不同的，所以，这个向量 vx 就可以用来唯一表示 x。</p><p>所以 Word2vec 本质上是一种<strong>降维</strong>操作——把词语从 one-hot encoder 形式的表示降维到 Word2vec 形式的表示。</p><h3 id=隐层细节>隐层细节<a hidden class=anchor aria-hidden=true href=#隐层细节>#</a></h3><p>假如词汇表长度为10000，首先使用one-hot形式表示每一个单词，经过隐层300个神经元计算，最后使用Softmax层对单词概率输出。每一对单词组，前者作为x输入，后者作为y标签。</p><p>假如我们想要学习的词向量维度为300，则需要将隐层的神经元个数设置为300(300是Google在其发布的训练模型中使用的维度，可调)。</p><p>隐层的权重矩阵就是词向量，我们模型学习到的就是隐层的权重矩阵。
<img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.kz1dltz718w.webp alt=matrix></p><p>之所以这样，来看一下one-hot输入后与隐层的计算就明白了。
<img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.33ejxiz5lc20.webp alt=vector></p><p>当使用One-hot去乘以矩阵的时候，会将某一行选择出来，即查表操作，所以权重矩阵是所有词向量组成的列表。</p><h4 id=cbow-详解>CBOW 详解:<a hidden class=anchor aria-hidden=true href=#cbow-详解>#</a></h4><p>CBOW 是 Continuous Bag-of-Words 的缩写，与神经网络语言模型不同的是，CBOW去掉了最耗时的非线性隐藏层</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.6sa4qxwkx180.webp alt=cbow></p><p>从图中可以看出，CBOW模型预测的是 <img loading=lazy src="https://www.zhihu.com/equation?tex=p%28w_t%7Cw_%7Bt-2%7D%2Cw_%7Bt-1%7D%2Cw_%7Bt%2B1%7D%2Cw_%7Bt%2B2%7D%29" alt=[公式]>
，由于图中目标词 <img loading=lazy src="https://www.zhihu.com/equation?tex=w_t" alt=[公式]>
前后只取了各两个词，所以窗口的总大小是2。假设目标词 <img loading=lazy src="https://www.zhihu.com/equation?tex=w_t" alt=[公式]>
前后各取k个词，即窗口的大小是k，那么CBOW模型预测的将是<img loading=lazy src="https://www.zhihu.com/equation?tex=p%28w_t%7Cw_%7Bt-k%7D%2C+w_%7Bt-%28k-1%29%7D%2C%5Ccdot%5Ccdot%5Ccdot%2Cw_%7Bt-1%7D%2Cw_%7Bt%2B1%7D%2C%5Ccdot%5Ccdot%5Ccdot%2Cw_%7Bt%2B%28k-1%29%7D%2C+w_%7Bt%2Bk%7D%29" alt=[公式]></p><p><strong>输入层到隐藏层</strong></p><p>以图2为例，输入层是四个词的one-hot向量表示，分别为 <img loading=lazy src="https://www.zhihu.com/equation?tex=x_%7Bt-2%7D%2Cx_%7Bt-1%7D%2Cx_%7Bt%2B1%7D%2Cx_%7Bt%2B2%7D" alt=[公式]>
（维度都为V x 1，V是模型的训练本文中所有词的个数），记输入层到隐藏层的权重矩阵为 <img loading=lazy src="https://www.zhihu.com/equation?tex=W" alt=[公式]>
（维度为V x d，d是认为给定的词向量维度），隐藏层的向量为 <img loading=lazy src="https://www.zhihu.com/equation?tex=h" alt=[公式]>
（维度为d x 1），那么</p><p><img loading=lazy src="https://www.zhihu.com/equation?tex=h+%3D+%5Cfrac%7BW%5ET%5Ccdot+x_%7Bt-2%7D%2BW%5ET%5Ccdot+x_%7Bt-1%7D%2BW%5ET%5Ccdot+x_%7Bt%2B1%7D%2BW%5ET%5Ccdot+x_%7Bt%2B2%7D%7D%7B4%7D" alt=[公式]></p><p>其实这里就是一个简单地求和平均。</p><p><strong>隐藏层到输出层</strong></p><p>记隐藏层到输出层的权重矩阵为 <img loading=lazy src="https://www.zhihu.com/equation?tex=U" alt=[公式]>
（维度为d x V），输出层的向量为 <img loading=lazy src="https://www.zhihu.com/equation?tex=y" alt=[公式]>
（维度为V x 1），那么</p><p><img loading=lazy src="https://www.zhihu.com/equation?tex=y%3Dsoftmax%28U%5ET%5Ccdot+h%29" alt=[公式]></p><p><strong>注意</strong>，输出层的向量 <img loading=lazy src="https://www.zhihu.com/equation?tex=y" alt=[公式]>
与输入层的向量为 <img loading=lazy src="https://www.zhihu.com/equation?tex=x_%7B%2A%7D" alt=[公式]>
虽然维度是一样的，但是 <img loading=lazy src="https://www.zhihu.com/equation?tex=y" alt=[公式]>
并不是one-hot向量，并且向量 <img loading=lazy src="https://www.zhihu.com/equation?tex=y" alt=[公式]>
中的每个元素都是有意义的。例如，我们假设训练样本只有一句话“I like to eat apple”，此刻我们正在使用 I、like、eat、apple 四个词来预测 to ，输出层的结果如图3所示。</p><p><img loading=lazy src=https://pic4.zhimg.com/80/v2-918b97c077fe15b4a67e0afddb62bfa3_720w.jpg alt=img>
图3 向量y的例子</p><p>向量y中的每个元素表示我用 I、like、eat、apple 四个词预测出来的词是当元素对应的词的概率，比如是like的概率为0.05，是to的概率是0.80。由于我们想让模型预测出来的词是to，那么我们就要尽量让to的概率尽可能的大，所以我们目标是最大化函数 <img loading=lazy src="https://www.zhihu.com/equation?tex=L" alt=[公式]></p><p><img loading=lazy src="https://www.zhihu.com/equation?tex=L%3D%5Cprod_%7Bt%3D1%7D%5E%7BV%7Dp%28w_t%7Cw_%7Bt-k%7D%2C+w_%7Bt-%28k-1%29%7D%2C%5Ccdot%5Ccdot%5Ccdot%2Cw_%7Bt-1%7D%2Cw_%7Bt%2B1%7D%2C%5Ccdot%5Ccdot%5Ccdot%2Cw_%7Bt%2B%28k-1%29%7D%2C+w_%7Bt%2Bk%7D%29" alt=[公式]></p><p>有了最大化的目标函数，我们接下来要做的就是求解这个目标函数，首先求 <img loading=lazy src="https://www.zhihu.com/equation?tex=-log%28L%29" alt=[公式]>
，然后求梯度，再梯度下降，具体细节在此省略，因为这种方法涉及到softmax层，softmax每次计算都要遍历整个词表，代价十分昂贵，所以实现的时候我们不用这种方法，次softmax或者负采样来替换掉输出层，降低复杂度。</p><h4 id=优化方法>优化方法<a hidden class=anchor aria-hidden=true href=#优化方法>#</a></h4><p>为了提高速度，Word2vec 经常采用 2 种加速方式：</p><ol><li><p>Negative Sample（负采样）</p><ul><li>本质是预测总体类别的一个子集</li></ul></li><li><p>Hierarchical Softmax （层次Softmax, huffman树）</p><ul><li>本质是把 N 分类问题变成 log(N)次二分类</li></ul></li></ol><h3 id=word2vec-的优缺点>Word2vec 的优缺点<a hidden class=anchor aria-hidden=true href=#word2vec-的优缺点>#</a></h3><p>需要说明的是：Word2vec 是上一代的产物（18 年之前）， 18 年之后想要得到最好的效果，已经不使用 Word Embedding 的方法了，所以也不会用到 Word2vec。</p><h4 id=优点>优点：<a hidden class=anchor aria-hidden=true href=#优点>#</a></h4><ol><li>由于 Word2vec 会考虑上下文，跟之前的 Embedding 方法相比，效果要更好（但不如 18 年之后的方法）</li><li>比之前的 Embedding方 法维度更少，所以速度更快</li><li>通用性很强，可以用在各种 NLP 任务中</li></ol><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.2oguavba1ny0.webp alt=advantages></p><h4 id=缺点>缺点：<a hidden class=anchor aria-hidden=true href=#缺点>#</a></h4><ol><li>由于词和向量是一对一的关系，所以多义词的问题无法解决。</li><li>Word2vec 是一种静态的方式，虽然通用性强，但是无法针对特定任务做动态优化</li></ol><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.73l6jzq5tko0.webp alt=disadvantages></p><h3 id=问题>问题<a hidden class=anchor aria-hidden=true href=#问题>#</a></h3><p>假如使用词向量维度为300，词汇量为10000个单词，那么神经网络输入层与隐层，隐层与输出层的参数量会达到惊人的300x10000=300万！训练如词庞大的神经网络需要庞大的数据量，还要避免过拟合。因此，Google在其第二篇论文中说明了训练的trick，其创新点如下：</p><ul><li>将常用词对或短语视为模型中的单个”word”。</li><li>对频繁的词进行子采样以减少训练样例的数量。</li><li>在损失函数中使用”负采样(<strong>Negative Sampling</strong>)”的技术，使每个训练样本仅更新模型权重的一小部分。</li></ul><p>子采样和负采样技术不仅降低了计算量，还提升了词向量的效果。</p><h3 id=对频繁词子采样>对频繁词子采样<a hidden class=anchor aria-hidden=true href=#对频繁词子采样>#</a></h3><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.6hglwcslxoo0.webp alt=sample></p><p>在以上例子中，可以看到频繁单词’the’的两个问题:</p><ul><li>对于单词对(‘fox’,’the’)，其对单词’fox’的语义表达并没有什么有效帮助，’the’在每个单词的上下文中出现都非常频繁。</li><li>预料中有很多单词对(‘the’,…)，我们应更好的学习单词’the’</li></ul><p>Word2vec使用子采样技术来解决以上问题，根据单词的频次来削减该单词的采样率。以window size为10为例子，我们删除’the’：</p><ul><li>当我们训练其余单词时候，’the’不会出现在他们的上下文中。</li><li>当中心词为’the’时，训练样本数量少于10。</li></ul><h3 id=负采样negative-sampling>负采样(Negative Sampling)<a hidden class=anchor aria-hidden=true href=#负采样negative-sampling>#</a></h3><p>训练一个网络是说，计算训练样本然后轻微调整所有的神经元权重来提高准确率。换句话说，每一个训练样本都需要更新所有神经网络的权重。</p><p>就像如上所说，当词汇表特别大的时候，如此多的神经网络参数在如此大的数据量下，每次都要进行权重更新，负担很大。</p><p>在每个样本训练时，<strong>只修改部分的网络参数，负采样是通过这种方式来解决这个问题的。</strong></p><p>当我们的神经网络训练到单词组(‘fox’, ‘quick’)时候，得到的输出或label都是一个one-hot向量，也就是说，在表示’quick’的位置数值为1，其它全为0。</p><p><strong>负采样是随机选择较小数量的’负(Negative)’单词(比如5个)，来做参数更新</strong>。这里的’负’表示的是网络输出向量种位置为0表示的单词。当然，’正(Positive)’(即正确单词’quick’)权重也会更新。</p><blockquote><p>论文中表述，小数量级上采用5-20，大数据集使用2-5个单词。</p></blockquote><p>我们的模型权重矩阵为300x10000，更新的单词为5个’负’词和一个’正’词，共计1800个参数，这是输出层全部3M参数的0.06%！！</p><p>负采样的选取是和频次相关的，频次越高，负采样的概率越大：
$$P(w_i) = \frac{f(w_i)^{3/4}}{\sum_{j=0}^n(f(w_j)^{3/4})}$$
论文选择0.75作为指数是因为实验效果好。C语言实现的代码很有意思：首先用索引值填充多次填充词汇表中的每个单词，单词索引出现的次数为$P(w_i) * \text{table_size}$。然后负采样只需要生成一个1到100M的整数，并用于索引表中数据。由于概率高的单词在表中出现的次数多，很可能会选择这些词。</p><h3 id=glove-模型>GloVe 模型<a hidden class=anchor aria-hidden=true href=#glove-模型>#</a></h3><ul><li><p>模型目标：进行词的向量化表示，使得向量之间尽可能多地蕴含语义和语法的信息。</p></li><li><p>输入：语料库</p></li><li><p>输出：词向量</p></li><li><p>方法概述：首先基于语料库构建词的共现矩阵，然后基于共现矩阵和GloVe模型学习词向量。</p></li></ul><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.5ix0a9sfbo80.webp alt=glove></p><p>Global Vector融合了矩阵分解的全局统计信息和上下文信息</p><h3 id=常见的问题>常见的问题<a hidden class=anchor aria-hidden=true href=#常见的问题>#</a></h3><h4 id=1文本表示哪些方法>1、文本表示哪些方法？<a hidden class=anchor aria-hidden=true href=#1文本表示哪些方法>#</a></h4><ul><li>基于 one-hot、tf-idf、textrank 等的 bag-of-words；</li><li>主题模型：LSA（SVD）、pLSA、LDA；</li><li>基于词向量的固定表征：Word2vec、FastText、GloVe</li><li>基于词向量的动态表征：ELMo、GPT、BERT</li></ul><h4 id=2怎么从语言模型理解词向量怎么理解分布式假设>2、怎么从语言模型理解词向量？怎么理解分布式假设？<a hidden class=anchor aria-hidden=true href=#2怎么从语言模型理解词向量怎么理解分布式假设>#</a></h4><p>上面给出的 4 个类型也是 nlp 领域最为常用的文本表示了，文本是由每个单词构成的，而谈起词向量，one-hot 是可认为是最为简单的词向量，但存在维度灾难和语义鸿沟等问题；通过构建共现矩阵并利用 SVD 求解构建词向量，则计算复杂度高；而早期词向量的研究通常来源于语言模型，比如 NNLM 和 RNNLM，其主要目的是语言模型，而词向量只是一个副产物。</p><p>所谓分布式假设，用一句话可以表达：<strong>相同上下文语境的词有似含义</strong>。而由此引申出了 Word2vec、FastText，在此类词向量中，虽然其本质仍然是语言模型，但是它的目标并不是语言模型本身，而是词向量，其所作的一系列优化，都是为了更快更好的得到词向量。GloVe 则是基于全局语料库、并结合上下文语境构建词向量，结合了 LSA 和 Word2vec 的优点。</p><h4 id=3传统的词向量有什么问题怎么解决各种词向量的特点是什么>3、传统的词向量有什么问题？怎么解决？各种词向量的特点是什么？<a hidden class=anchor aria-hidden=true href=#3传统的词向量有什么问题怎么解决各种词向量的特点是什么>#</a></h4><p><strong>上述方法得到的词向量是固定表征的，无法解决一词多义等问题</strong>，如“川普”。为此引入基于语言模型的动态表征方法：ELMo、GPT、BERT。</p><p>各种词向量的特点：</p><ul><li>One-hot 表示 ：维度灾难、语义鸿沟；</li><li>分布式表示 (distributed representation)</li><li>矩阵分解（LSA）：利用全局语料特征，但 SVD 求解计算复杂度大；</li><li>基于 NNLM/RNNLM 的词向量：词向量为副产物，存在效率不高等问题；</li><li>Word2vec、FastText：优化效率高，但是基于局部语料；</li><li>GloVe：基于全局预料，结合了 LSA 和 Word2vec 的优点；</li><li>ELMo、GPT、BERT：动态特征；</li></ul><h4 id=4word2vec-和-nnlm-对比有什么区别word2vecvs-nnlm>4、Word2vec 和 NNLM 对比有什么区别？（Word2vecvs NNLM）<a hidden class=anchor aria-hidden=true href=#4word2vec-和-nnlm-对比有什么区别word2vecvs-nnlm>#</a></h4><p>1）其本质都可以看作是语言模型；</p><p>2）词向量只不过 NNLM 一个产物，Word2vec 虽然其本质也是语言模型，但是其专注于词向量本身，因此做了许多优化来提高计算效率：</p><ul><li>与 NNLM 相比，词向量直接 sum，不再拼接，并舍弃隐层；</li><li>考虑到 sofmax 归一化需要遍历整个词汇表，采用 hierarchical softmax 和 negative sampling 进行优化，hierarchical softmax 实质上生成一颗带权路径最小的哈夫曼树，让高频词搜索路劲变小；negative sampling 更为直接，实质上对每一个样本中每一个词都进行负例采样；</li></ul><h4 id=5word2vec-和-fasttext-对比有什么区别word2vec-vs-fasttext>5、Word2vec 和 FastText 对比有什么区别？（Word2vec vs FastText）<a hidden class=anchor aria-hidden=true href=#5word2vec-和-fasttext-对比有什么区别word2vec-vs-fasttext>#</a></h4><p>1）都可以无监督学习词向量， FastText 训练词向量时会考虑 subword；</p><p>2） FastText 还可以进行有监督学习进行文本分类，其主要特点：</p><ul><li>结构与 CBOW 类似，但学习目标是人工标注的分类结果；</li><li>采用 hierarchical softmax 对输出的分类标签建立哈夫曼树，样本中标签多的类别被分配短的搜寻路径</li><li>引入 N-gram，考虑词序特征</li><li>引入 subword 来处理长词，处理未登陆词问题；</li></ul><h4 id=6glove-和-word2vec-lsa-对比有什么区别word2vecvs-glove-vs-lsa>6、GloVe 和 Word2vec、 LSA 对比有什么区别？（Word2vecvs GloVe vs LSA）<a hidden class=anchor aria-hidden=true href=#6glove-和-word2vec-lsa-对比有什么区别word2vecvs-glove-vs-lsa>#</a></h4><p><strong>1）GloVe vs LSA</strong></p><p>LSA（Latent Semantic Analysis）可以基于 co-occurance matrix 构建词向量，实质上是基于全局语料采用 SVD 进行矩阵分解，然而 SVD 计算复杂度高；</p><p>GloVe 可看作是对 LSA 一种优化的高效矩阵分解算法，采用 Adagrad 对最小平方损失进行优化；</p><p><strong>2）Word2vecvs GloVe</strong></p><p>Word2vec 是局部语料库训练的，其特征提取是基于滑窗的；而 GloVe 的滑窗是为了构建 co-occurance matrix，是基于全局语料的，可见 GloVe 需要事先统计共现概率；因此，Word2vec 可以进行在线学习，GloVe 则需要统计固定语料信息。</p><p>Word2vec 是无监督学习，同样由于不需要人工标注；GloVe 通常被认为是无监督学习，但实际上 GloVe 还是有 label 的，即共现次数 [公式]。</p><p>Word2vec 损失函数实质上是带权重的交叉熵，权重固定；GloVe 的损失函数是最小平方损失函数，权重可以做映射变换。</p><p>总体来看，<strong>GloVe 可以被看作是更换了目标函数和权重函数的全局 Word2vec。</strong></p><h4 id=7-word2vec-的两种优化方法是什么它们的目标函数怎样确定的训练过程又是怎样的>7、 Word2vec 的两种优化方法是什么？它们的目标函数怎样确定的？训练过程又是怎样的？<a hidden class=anchor aria-hidden=true href=#7-word2vec-的两种优化方法是什么它们的目标函数怎样确定的训练过程又是怎样的>#</a></h4><p>不经过优化的 CBOW 和 Skip-gram 中 , 在每个样本中每个词的训练过程都要遍历整个词汇表，也就是都需要经过 softmax 归一化，计算误差向量和梯度以更新两个词向量矩阵（这两个词向量矩阵实际上就是最终的词向量，可认为初始化不一样），当语料库规模变大、词汇表增长时，训练变得不切实际。为了解决这个问题，Word2vec 支持两种优化方法：hierarchical softmax 和 negative sampling。</p><p><strong>（1）基于 hierarchical softmax 的 CBOW 和 Skip-gram</strong></p><p>hierarchical softmax 使用一颗二叉树表示词汇表中的单词，每个单词都作为二叉树的叶子节点。对于一个大小为 V 的词汇表，其对应的二叉树包含 V-1 非叶子节点。假如每个非叶子节点向左转标记为 1，向右转标记为 0，那么每个单词都具有唯一的从根节点到达该叶子节点的由｛0 1｝组成的代号（<strong>实际上为哈夫曼编码，为哈夫曼树，是带权路径长度最短的树，哈夫曼树保证了词频高的单词的路径短，词频相对低的单词的路径长，这种编码方式很大程度减少了计算量</strong>）。</p><p><strong>（2）基于 negative sampling 的 CBOW 和 Skip-gram</strong></p><p>negative sampling 是一种不同于 hierarchical softmax 的优化策略，相比于 hierarchical softmax，negative sampling 的想法更直接——<strong>为每个训练实例都提供负例</strong>。</p><p>负采样算法实际上就是一个<strong>带权采样</strong>过程，负例的选择机制是和单词词频联系起来的。</p><p>具体做法是以 N+1 个点对区间 [0,1] 做非等距切分，并引入的一个在区间 [0,1] 上的 M 等距切分，其中 M &#187; N。源码中取 M = 10^8。然后对两个切分做投影，得到映射关系：采样时，每次生成一个 [1, M-1] 之间的整数 i，则 Table(i) 就对应一个样本；当采样到正例时，跳过（<strong>拒绝采样</strong>）。</p><p>参考： <a href=https://zhuanlan.zhihu.com/p/44599645>https://zhuanlan.zhihu.com/p/44599645</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://reid00.github.io/en/tags/word2vec/>Word2vec</a></li><li><a href=https://reid00.github.io/en/tags/%E8%AF%8D%E5%90%91%E9%87%8F/>词向量</a></li></ul><nav class=paginav><a class=prev href=https://reid00.github.io/en/posts/ml/svm/><span class=title>« Prev</span><br><span>SVM</span>
</a><a class=next href=https://reid00.github.io/en/posts/ml/%E5%86%B3%E7%AD%96%E6%A0%91/><span class=title>Next »</span><br><span>决策树</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://reid00.github.io/en/>Reid's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>