<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>特征工程之数据预处理 | Reid's Blog</title>
<meta name=keywords content="feature engineering,data preprocessing"><meta name=description content="特征工程之数据预处理"><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/en/posts/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://reid00.github.io/en/posts/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><head><meta name=referrer content="no-referrer"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script><script>var doNotTrack=!1,dnt;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK")}</script><meta property="og:title" content="特征工程之数据预处理"><meta property="og:description" content="特征工程之数据预处理"><meta property="og:type" content="article"><meta property="og:url" content="https://reid00.github.io/en/posts/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-16T19:35:24+08:00"><meta property="article:modified_time" content="2023-03-16T19:35:24+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="特征工程之数据预处理"><meta name=twitter:description content="特征工程之数据预处理"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://reid00.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"机器学习，深度学习，知识图谱相关","item":"https://reid00.github.io/en/posts/ml/"},{"@type":"ListItem","position":3,"name":"特征工程之数据预处理","item":"https://reid00.github.io/en/posts/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"特征工程之数据预处理","name":"特征工程之数据预处理","description":"特征工程之数据预处理","keywords":["feature engineering","data preprocessing"],"articleBody":"Summary 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。由此可见，特征工程在机器学习中占有相当重要的地位。在实际应用当中，可以说特征工程是机器学习成功的关键。\n什么是特征工程 特征工程又包含了Data PreProcessing（数据预处理）、Feature Extraction（特征提取）、Feature Selection（特征选择）和Feature construction（特征构造）等子问题，本章内容主要讨论数据预处理的方法及实现。 特征工程是机器学习中最重要的起始步骤，数据预处理是特征工程的最重要的起始步骤，而数据清洗是数据预处理的重要组成部分，会直接影响机器学习的效果。\n数据清洗整体介绍 1. 箱线图分析异常值 箱线图提供了识别异常值的标准，如果一个数下雨 QL-1.5IQR or 大于OU + 1.5 IQR, 则这个值被称为异常值。\nQL 下四分位数，表示四分之一的数据值比它小 QU　上四分位数，表示四分之一的数据值比它大 IRQ　四分位距，是QU－QL　的差值，包含了全部关差值的一般 2. 数据的光滑处理 除了检测出异常值然后再处理异常值外，还可以使用以下方法对异常数据进行光滑处理。\n2.1. 变量分箱（即变量离散化) 离散特征的增加和减少都很容易，易于模型的快速迭代； 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄\u003e30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力； 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问； 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。 可以将缺失作为独立的一类带入模型。 将所有变量变换到相似的尺度上。 2.1.0 变量分箱的方法 2.1.1 无序变量分箱 举个例子，在实际模型建立当中，有个 job 职业的特征，取值为（“国家机关人员”，“专业技术人员”，“商业服务人员”），对于这一类变量，如果我们将其依次赋值为（国家机关人员=1；专业技术人员=2；商业服务人员=3），就很容易产生一个问题，不同种类的职业在数据层面上就有了大小顺序之分，国家机关人员和商业服务人员的差距是2，专业技术人员和商业服务人员的之间的差距是1，而我们原来的中文分类中是不存在这种先后顺序关系的。所以这么简单的赋值是会使变量失去原来的衡量效果。\n怎么处理这个问题呢? “一位有效编码” （one-hot Encoding）可以解决这个问题，通常叫做虚变量或者哑变量（dummpy variable）：比如职业特征有3个不同变量，那么将其生成个2哑变量，分别是“是否国家党政职业人员”，“是否专业技术人员” ，每个虚变量取值（1，0）。 为什么2个哑变量而非3个？ 在模型中引入多个虚拟变量时，虚拟变量的个数应按下列原则确定： 回归模型有截距：一般的，若该特征下n个属性均互斥（如，男/女;儿童/青年/中年/老年），在生成虚拟变量时，应该生成 n-1个虚变量，这样可以避免产生多重共线性 回归模型无截距项：有n个特征，设置n个虚拟变量 python 实现方法pd.get_dummies() 2.1.2 有序变量分箱 有序多分类变量是很常见的变量形式，通常在变量中有多个可能会出现的取值，各取值之间还存在等级关系。比如高血压分级（0=正常，1=正常高值，2=1级高血压，3=2级高血压，4=3级高血压）这类变量处理起来简直不要太省心，使用 pandas 中的 map（）替换相应变量就行。\n1 2 3 4 5 import pandas as pd df= pd.DataFrame(['正常','3级高血压','正常','2级高血压','正常','正常高值','1级高血压'],columns=['blood_pressure']) dic_blood = {'正常':0,'正常高值':1,'1级高血压':2,'2级高血压':3,'3级高血压':4} df['blood_pressure_enc'] = df['blood_pressure'].map(dic_blood) print(df) 2.1.3 连续变量的分箱方式 等宽划分：按照相同宽度将数据分成几等份。缺点是受到异常值的影响比较大。 pandas.cut方法可以进行等宽划分。 等频划分：将数据分成几等份，每等份数据里面的个数是一样的。pandas.qcut方法可以进行等频划分。 1 2 3 4 5 6 import pandas as pd df = pd.DataFrame([[22,1],[13,1],[33,1],[52,0],[16,0],[42,1],[53,1],[39,1],[26,0],[66,0]],columns=['age','Y']) #print(df) df['age_bin_1'] = pd.qcut(df['age'],3) #新增一列存储等频划分的分箱特征 df['age_bin_2'] = pd.cut(df['age'],3) #新增一列存储等距划分的分箱特征 print(df) 2.1.4 有监督学习分箱方法 最小熵法分箱 假设因变量为分类变量，可取值1，… ，J。令pijpij表示第i个分箱内因变量取值为j的观测的比例，i=1，…，k，j=1，…，J；那么第i个分箱的熵值为∑Jj=0−pij×logpij∑j=0J−pij×logpij。如果第i个分箱内因变量各类别的比例相等，即p11=p12=p1J=1/Jp11=p12=p1J=1/J，那么第i个分箱的熵值达到最大值；如果第i个分箱内因变量只有一种取值，即某个pijpij等于1而其他类别的比例等于0，那么第i个分箱的熵值达到最小值。 令riri表示第i个分箱的观测数占所有观测数的比例；那么总熵值为∑ki=0∑Jj=0(−pij×logpij)∑i=0k∑j=0J(−pij×logpij)。需要使总熵值达到最小，也就是使分箱能够最大限度地区分因变量的各类别。 卡方分箱 (常用) 自底向上的(即基于合并的)数据离散化方法。 它依赖于卡方检验:具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。 基本思想: 对于精确的离散化，相对类频率在一个区间内应当完全一致。因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。 2.2 无量纲化 无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。\n2.2.1 标准化 标准化需要计算特征的均值和标准差，公式表达为：\n使用preproccessing库的StandardScaler类对数据进行标准化的代码如下：\n1 2 3 4 from sklearn.preprocessing import StandardScaler #标准化，返回值为标准化后的数据 StandardScaler().fit_transform(iris.data) 2.2.2 区间缩放法 区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，公式表达为：\n使用preproccessing库的MinMaxScaler类对数据进行区间缩放的代码如下：\n1 2 3 4 from sklearn.preprocessing import MinMaxScaler #区间缩放，返回值为缩放到[0, 1]区间的数据 MinMaxScaler().fit_transform(iris.data) 2.1.3 标准化与归一化的区别 简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下：\n什么时候需要进行归一化？\n归一化后加快了梯度下降求最优解的速度 归一化有可能提高精度 什么时候需要进行归一化？\n通常在需要用到梯度下降法的时候。 包括线性回归、逻辑回归、支持向量机、神经网络等模型。\n决策树模型就不适用 例如 C4.5 ，主要根据信息增益比来分裂，归一化不会改变样本在特征 x 上的信息增益\n比较概率大小分布即可，不需要。\n使用preproccessing库的Normalizer类对数据进行归一化的代码如下：\n1 2 3 4 from sklearn.preprocessing import Normalizer #归一化，返回值为归一化后的数据 Normalizer().fit_transform(iris.data) 2.3 对定量特征二值化 定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0，公式表达如下：\n使用preproccessing库的Binarizer类对数据进行二值化的代码如下：\n1 2 3 4 from sklearn.preprocessing import Binarizer #二值化，阈值设置为3，返回值为二值化后的数据 Binarizer(threshold=3).fit_transform(iris.data) 2.4 对定性特征哑编码 由于IRIS数据集的特征皆为定量特征，故使用其目标值进行哑编码（实际上是不需要的）。使用preproccessing库的OneHotEncoder类对数据进行哑编码的代码如下：\n1 2 3 4 from sklearn.preprocessing import OneHotEncoder #哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据 OneHotEncoder().fit_transform(iris.target.reshape((-1,1))) 2.5 缺失值计算 由于IRIS数据集没有缺失值，故对数据集新增一个样本，4个特征均赋值为NaN，表示数据缺失。使用preproccessing库的Imputer类对数据进行缺失值计算的代码如下：\n1 2 3 4 5 6 7 from numpy import vstack, array, nan from sklearn.preprocessing import Imputer #缺失值计算，返回值为计算缺失值后的数据 #参数missing_value为缺失值的表示形式，默认为NaN #参数strategy为缺失值填充方式，默认为mean（均值） Imputer().fit_transform(vstack((array([nan, nan, nan, nan]), iris.data))) 2.6 数据变换 常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。4个特征，度为2的多项式转换公式如下：\n使用preproccessing库的PolynomialFeatures类对数据进行多项式转换的代码如下：\n1 2 3 4 5 from sklearn.preprocessing import PolynomialFeatures #多项式转换 #参数degree为度，默认值为2 PolynomialFeatures().fit_transform(iris.data) 基于单变元函数的数据变换可以使用一个统一的方式完成，使用preproccessing库的FunctionTransformer对数据进行对数函数转换的代码如下：\n1 2 3 4 5 6 from numpy import log1p from sklearn.preprocessing import FunctionTransformer #自定义转换函数为对数函数的数据变换 #第一个参数是单变元函数 FunctionTransformer(log1p).fit_transform(iris.data) 2.7 回归 可以用一个函数（如回归函数）拟合数据来光滑数据。线性回归涉及找出拟合两个属性（或变量）的“最佳”线，是的一个属性可以用来预测另一个。多元线性回归是线性回归的扩展，其中涉及的属性多于两个，并且数据拟合到一个多维曲面。\n3. 异常值处理方法 删除含有异常值的记录； 某些筛选出来的异常样本是否真的是不需要的异常特征样本，最好找懂业务的再确认一下，防止我们将正常的样本过滤掉了。 将异常值视为缺失值，交给缺失值处理方法来处理； 使用均值/中位数/众数来修正； 不处理。 4. 什么是组合特征？如何处理高维组合特征？ 为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合成高阶特征，构成交互特征（Interaction Feature）。以广告点击预估问题为例，如图1所示，原始数据有语言和类型两种离散特征。为了提高拟合能力，语言和类型可以组成二阶特征。\n5. 类别型特征 什么是类别型特征？\n例如：性别（男、女）、血型（A、B、AB、O）\n通常是字符串形式，需要转化成数值型，传递给模型\n如何处理类别型特征？\n序号编码（Ordinal Encoding） 例如学习成绩有高中低三档，也就是不同类别之间关系。\n这时可以用321来表示，保留了大小关系。\n独热编码（One-hot Encoding） 例如血型，它的类别没有大小关系。A 型血表示为（1, 0, 0, 0），B 型血表示为（0, 1, 0, 0）……\n二进制编码（Binary Encoding） 第一步，先用序号编码给每个类别编码\n第二步，将类别 ID 转化为相应的二进制\n","wordCount":"4589","inLanguage":"en","datePublished":"2023-03-16T19:35:24+08:00","dateModified":"2023-03-16T19:35:24+08:00","author":[{"@type":"Person","name":"Reid"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://reid00.github.io/en/posts/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"},"publisher":{"@type":"Organization","name":"Reid's Blog","logo":{"@type":"ImageObject","url":"https://reid00.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://reid00.github.io/en/ accesskey=h title="Reid's Blog (Alt + H)">Reid's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://reid00.github.io/en/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://reid00.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://reid00.github.io/en/categories/ title=Categorys><span>Categorys</span></a></li><li><a href=https://reid00.github.io/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://reid00.github.io/en/>Home</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/ml/>机器学习，深度学习，知识图谱相关</a></div><h1 class=post-title>特征工程之数据预处理</h1><div class=post-description>特征工程之数据预处理</div><div class=post-meta><span title='2023-03-16 19:35:24 +0800 +0800'>2023-03-16 19:35</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;4589 words&nbsp;·&nbsp;Reid</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#summary aria-label=Summary>Summary</a></li><li><a href=#%e4%bb%80%e4%b9%88%e6%98%af%e7%89%b9%e5%be%81%e5%b7%a5%e7%a8%8b aria-label=什么是特征工程>什么是特征工程</a></li><li><a href=#%e6%95%b0%e6%8d%ae%e6%b8%85%e6%b4%97%e6%95%b4%e4%bd%93%e4%bb%8b%e7%bb%8d aria-label=数据清洗整体介绍>数据清洗整体介绍</a><ul><li><a href=#1-%e7%ae%b1%e7%ba%bf%e5%9b%be%e5%88%86%e6%9e%90%e5%bc%82%e5%b8%b8%e5%80%bc aria-label="1. 箱线图分析异常值"><strong>1. 箱线图分析异常值</strong></a></li><li><a href=#2-%e6%95%b0%e6%8d%ae%e7%9a%84%e5%85%89%e6%bb%91%e5%a4%84%e7%90%86 aria-label="2. 数据的光滑处理"><strong>2. 数据的光滑处理</strong></a><ul><li><a href=#21-%e5%8f%98%e9%87%8f%e5%88%86%e7%ae%b1%e5%8d%b3%e5%8f%98%e9%87%8f%e7%a6%bb%e6%95%a3%e5%8c%96 aria-label="2.1. 变量分箱（即变量离散化)">2.1. 变量分箱（即变量离散化)</a></li><li><a href=#210-%e5%8f%98%e9%87%8f%e5%88%86%e7%ae%b1%e7%9a%84%e6%96%b9%e6%b3%95 aria-label="2.1.0 变量分箱的方法">2.1.0 变量分箱的方法</a><ul><li><a href=#211-%e6%97%a0%e5%ba%8f%e5%8f%98%e9%87%8f%e5%88%86%e7%ae%b1 aria-label="2.1.1 无序变量分箱">2.1.1 无序变量分箱</a></li><li><a href=#212-%e6%9c%89%e5%ba%8f%e5%8f%98%e9%87%8f%e5%88%86%e7%ae%b1 aria-label="2.1.2 有序变量分箱">2.1.2 有序变量分箱</a></li><li><a href=#213-%e8%bf%9e%e7%bb%ad%e5%8f%98%e9%87%8f%e7%9a%84%e5%88%86%e7%ae%b1%e6%96%b9%e5%bc%8f aria-label="2.1.3 连续变量的分箱方式">2.1.3 连续变量的分箱方式</a></li><li><a href=#214-%e6%9c%89%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0%e5%88%86%e7%ae%b1%e6%96%b9%e6%b3%95 aria-label="2.1.4 有监督学习分箱方法">2.1.4 有监督学习分箱方法</a><ul><li><a href=#%e6%9c%80%e5%b0%8f%e7%86%b5%e6%b3%95%e5%88%86%e7%ae%b1 aria-label=最小熵法分箱>最小熵法分箱</a></li><li><a href=#%e5%8d%a1%e6%96%b9%e5%88%86%e7%ae%b1-%e5%b8%b8%e7%94%a8 aria-label="卡方分箱 (常用)">卡方分箱 (常用)</a></li></ul></li></ul></li><li><a href=#22-%e6%97%a0%e9%87%8f%e7%ba%b2%e5%8c%96 aria-label="2.2 无量纲化">2.2 <strong>无量纲化</strong></a><ul><li><a href=#221-%e6%a0%87%e5%87%86%e5%8c%96 aria-label="2.2.1 标准化">2.2.1 <strong>标准化</strong></a></li><li><a href=#222--%e5%8c%ba%e9%97%b4%e7%bc%a9%e6%94%be%e6%b3%95 aria-label="2.2.2 区间缩放法">2.2.2 <strong>区间缩放法</strong></a></li><li><a href=#213-%e6%a0%87%e5%87%86%e5%8c%96%e4%b8%8e%e5%bd%92%e4%b8%80%e5%8c%96%e7%9a%84%e5%8c%ba%e5%88%ab aria-label="2.1.3 标准化与归一化的区别">2.1.3 标准化与归一化的区别</a></li><li><a href=#23-%e5%af%b9%e5%ae%9a%e9%87%8f%e7%89%b9%e5%be%81%e4%ba%8c%e5%80%bc%e5%8c%96 aria-label="2.3 对定量特征二值化">2.3 <strong>对定量特征二值化</strong></a></li><li><a href=#24-%e5%af%b9%e5%ae%9a%e6%80%a7%e7%89%b9%e5%be%81%e5%93%91%e7%bc%96%e7%a0%81 aria-label="2.4 对定性特征哑编码">2.4 <strong>对定性特征哑编码</strong></a></li><li><a href=#25-%e7%bc%ba%e5%a4%b1%e5%80%bc%e8%ae%a1%e7%ae%97 aria-label="2.5 缺失值计算">2.5 <strong>缺失值计算</strong></a></li><li><a href=#26--%e6%95%b0%e6%8d%ae%e5%8f%98%e6%8d%a2 aria-label="2.6 数据变换">2.6 <strong>数据变换</strong></a></li><li><a href=#27-%e5%9b%9e%e5%bd%92 aria-label="2.7 回归">2.7 回归</a></li></ul></li><li><a href=#3-%e5%bc%82%e5%b8%b8%e5%80%bc%e5%a4%84%e7%90%86%e6%96%b9%e6%b3%95 aria-label="3. 异常值处理方法"><strong>3. 异常值处理方法</strong></a></li><li><a href=#4-%e4%bb%80%e4%b9%88%e6%98%af%e7%bb%84%e5%90%88%e7%89%b9%e5%be%81%e5%a6%82%e4%bd%95%e5%a4%84%e7%90%86%e9%ab%98%e7%bb%b4%e7%bb%84%e5%90%88%e7%89%b9%e5%be%81 aria-label="4. 什么是组合特征？如何处理高维组合特征？">4. 什么是组合特征？如何处理高维组合特征？</a></li><li><a href=#5-%e7%b1%bb%e5%88%ab%e5%9e%8b%e7%89%b9%e5%be%81 aria-label="5. 类别型特征">5. <strong>类别型特征</strong></a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><h1 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h1><p>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。由此可见，特征工程在机器学习中占有相当重要的地位。在实际应用当中，可以说特征工程是机器学习成功的关键。</p><h1 id=什么是特征工程>什么是特征工程<a hidden class=anchor aria-hidden=true href=#什么是特征工程>#</a></h1><p>特征工程又包含了Data PreProcessing（数据预处理）、Feature Extraction（特征提取）、Feature Selection（特征选择）和Feature construction（特征构造）等子问题，本章内容主要讨论数据预处理的方法及实现。
特征工程是机器学习中最重要的起始步骤，数据预处理是特征工程的最重要的起始步骤，而数据清洗是数据预处理的重要组成部分，会直接影响机器学习的效果。</p><h1 id=数据清洗整体介绍>数据清洗整体介绍<a hidden class=anchor aria-hidden=true href=#数据清洗整体介绍>#</a></h1><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.7jfl0yb65gs0.webp alt="data cleaning" title=数据清洗></p><h2 id=1-箱线图分析异常值><strong>1. 箱线图分析异常值</strong><a hidden class=anchor aria-hidden=true href=#1-箱线图分析异常值>#</a></h2><p>箱线图提供了识别异常值的标准，如果一个数下雨 QL-1.5IQR or 大于OU + 1.5 IQR, 则这个值被称为异常值。</p><ul><li>QL 下四分位数，表示四分之一的数据值比它小</li><li>QU　上四分位数，表示四分之一的数据值比它大</li><li>IRQ　四分位距，是QU－QL　的差值，包含了全部关差值的一般</li></ul><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.5pntixddlb40.webp alt=箱线图展示 title=箱线图展示></p><h2 id=2-数据的光滑处理><strong>2. 数据的光滑处理</strong><a hidden class=anchor aria-hidden=true href=#2-数据的光滑处理>#</a></h2><p>除了检测出异常值然后再处理异常值外，还可以使用以下方法对异常数据进行光滑处理。</p><h3 id=21-变量分箱即变量离散化>2.1. 变量分箱（即变量离散化)<a hidden class=anchor aria-hidden=true href=#21-变量分箱即变量离散化>#</a></h3><ul><li>离散特征的增加和减少都很容易，易于模型的快速迭代；</li><li>稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</li><li>离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</li><li>逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；</li><li>离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；</li><li>特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；</li><li>特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。</li><li>可以将缺失作为独立的一类带入模型。</li><li>将所有变量变换到相似的尺度上。</li></ul><h3 id=210-变量分箱的方法>2.1.0 变量分箱的方法<a hidden class=anchor aria-hidden=true href=#210-变量分箱的方法>#</a></h3><h4 id=211-无序变量分箱>2.1.1 无序变量分箱<a hidden class=anchor aria-hidden=true href=#211-无序变量分箱>#</a></h4><p>举个例子，在实际模型建立当中，有个 job 职业的特征，取值为（“国家机关人员”，“专业技术人员”，“商业服务人员”），对于这一类变量，如果我们将其依次赋值为（国家机关人员=1；专业技术人员=2；商业服务人员=3），就很容易产生一个问题，不同种类的职业在数据层面上就有了大小顺序之分，国家机关人员和商业服务人员的差距是2，专业技术人员和商业服务人员的之间的差距是1，而我们原来的中文分类中是不存在这种先后顺序关系的。所以这么简单的赋值是会使变量失去原来的衡量效果。</p><ul><li>怎么处理这个问题呢?
“一位有效编码” （one-hot Encoding）可以解决这个问题，通常叫做虚变量或者哑变量（dummpy variable）：比如职业特征有3个不同变量，那么将其生成个2哑变量，分别是“是否国家党政职业人员”，“是否专业技术人员” ，每个虚变量取值（1，0）。</li><li>为什么2个哑变量而非3个？
在模型中引入多个虚拟变量时，虚拟变量的个数应按下列原则确定：<ol><li>回归模型有截距：一般的，若该特征下n个属性均互斥（如，男/女;儿童/青年/中年/老年），在生成虚拟变量时，应该生成 n-1个虚变量，这样可以避免产生多重共线性</li><li>回归模型无截距项：有n个特征，设置n个虚拟变量</li><li>python 实现方法pd.get_dummies()</li></ol></li></ul><h4 id=212-有序变量分箱>2.1.2 有序变量分箱<a hidden class=anchor aria-hidden=true href=#212-有序变量分箱>#</a></h4><p>有序多分类变量是很常见的变量形式，通常在变量中有多个可能会出现的取值，各取值之间还存在等级关系。比如高血压分级（0=正常，1=正常高值，2=1级高血压，3=2级高血压，4=3级高血压）这类变量处理起来简直不要太省心，使用 pandas 中的 map（）替换相应变量就行。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>([</span><span class=s1>&#39;正常&#39;</span><span class=p>,</span><span class=s1>&#39;3级高血压&#39;</span><span class=p>,</span><span class=s1>&#39;正常&#39;</span><span class=p>,</span><span class=s1>&#39;2级高血压&#39;</span><span class=p>,</span><span class=s1>&#39;正常&#39;</span><span class=p>,</span><span class=s1>&#39;正常高值&#39;</span><span class=p>,</span><span class=s1>&#39;1级高血压&#39;</span><span class=p>],</span><span class=n>columns</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;blood_pressure&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>dic_blood</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;正常&#39;</span><span class=p>:</span><span class=mi>0</span><span class=p>,</span><span class=s1>&#39;正常高值&#39;</span><span class=p>:</span><span class=mi>1</span><span class=p>,</span><span class=s1>&#39;1级高血压&#39;</span><span class=p>:</span><span class=mi>2</span><span class=p>,</span><span class=s1>&#39;2级高血压&#39;</span><span class=p>:</span><span class=mi>3</span><span class=p>,</span><span class=s1>&#39;3级高血压&#39;</span><span class=p>:</span><span class=mi>4</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=p>[</span><span class=s1>&#39;blood_pressure_enc&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;blood_pressure&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>dic_blood</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=213-连续变量的分箱方式>2.1.3 连续变量的分箱方式<a hidden class=anchor aria-hidden=true href=#213-连续变量的分箱方式>#</a></h4><ul><li><strong>等宽划分</strong>：按照相同宽度将数据分成几等份。缺点是受到异常值的影响比较大。 pandas.cut方法可以进行等宽划分。</li><li><strong>等频划分</strong>：将数据分成几等份，每等份数据里面的个数是一样的。pandas.qcut方法可以进行等频划分。</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>([[</span><span class=mi>22</span><span class=p>,</span><span class=mi>1</span><span class=p>],[</span><span class=mi>13</span><span class=p>,</span><span class=mi>1</span><span class=p>],[</span><span class=mi>33</span><span class=p>,</span><span class=mi>1</span><span class=p>],[</span><span class=mi>52</span><span class=p>,</span><span class=mi>0</span><span class=p>],[</span><span class=mi>16</span><span class=p>,</span><span class=mi>0</span><span class=p>],[</span><span class=mi>42</span><span class=p>,</span><span class=mi>1</span><span class=p>],[</span><span class=mi>53</span><span class=p>,</span><span class=mi>1</span><span class=p>],[</span><span class=mi>39</span><span class=p>,</span><span class=mi>1</span><span class=p>],[</span><span class=mi>26</span><span class=p>,</span><span class=mi>0</span><span class=p>],[</span><span class=mi>66</span><span class=p>,</span><span class=mi>0</span><span class=p>]],</span><span class=n>columns</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;age&#39;</span><span class=p>,</span><span class=s1>&#39;Y&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=c1>#print(df)</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=p>[</span><span class=s1>&#39;age_bin_1&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>qcut</span><span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;age&#39;</span><span class=p>],</span><span class=mi>3</span><span class=p>)</span> <span class=c1>#新增一列存储等频划分的分箱特征</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=p>[</span><span class=s1>&#39;age_bin_2&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>cut</span><span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;age&#39;</span><span class=p>],</span><span class=mi>3</span><span class=p>)</span>  <span class=c1>#新增一列存储等距划分的分箱特征</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>df</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=214-有监督学习分箱方法>2.1.4 有监督学习分箱方法<a hidden class=anchor aria-hidden=true href=#214-有监督学习分箱方法>#</a></h4><ol><li><h5 id=最小熵法分箱>最小熵法分箱<a hidden class=anchor aria-hidden=true href=#最小熵法分箱>#</a></h5><ul><li>假设因变量为分类变量，可取值1，… ，J。令pijpij表示第i个分箱内因变量取值为j的观测的比例，i=1，…，k，j=1，…，J；那么第i个分箱的熵值为∑Jj=0−pij×logpij∑j=0J−pij×logpij。如果第i个分箱内因变量各类别的比例相等，即p11=p12=p1J=1/Jp11=p12=p1J=1/J，那么第i个分箱的熵值达到最大值；如果第i个分箱内因变量只有一种取值，即某个pijpij等于1而其他类别的比例等于0，那么第i个分箱的熵值达到最小值。</li><li>令riri表示第i个分箱的观测数占所有观测数的比例；那么总熵值为∑ki=0∑Jj=0(−pij×logpij)∑i=0k∑j=0J(−pij×logpij)。需要使总熵值达到最小，也就是使分箱能够最大限度地区分因变量的各类别。</li></ul></li><li><h5 id=卡方分箱-常用>卡方分箱 (常用)<a hidden class=anchor aria-hidden=true href=#卡方分箱-常用>#</a></h5><ul><li>自底向上的(即基于合并的)数据离散化方法。</li><li>它依赖于卡方检验:具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。</li><li><strong>基本思想</strong>:<ul><li>对于精确的离散化，相对类频率在一个区间内应当完全一致。因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。</li></ul></li></ul></li></ol><h3 id=22-无量纲化>2.2 <strong>无量纲化</strong><a hidden class=anchor aria-hidden=true href=#22-无量纲化>#</a></h3><p>无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。</p><h4 id=221-标准化>2.2.1 <strong>标准化</strong><a hidden class=anchor aria-hidden=true href=#221-标准化>#</a></h4><p>标准化需要计算特征的均值和标准差，公式表达为：</p><p>　　使用preproccessing库的StandardScaler类对数据进行标准化的代码如下：</p><p><img loading=lazy src=https://pic1.zhimg.com/50/c7e852db6bd05b7bb1017b5425ffeec1_hd.jpg alt=img>
<img loading=lazy src=https://pic1.zhimg.com/80/c7e852db6bd05b7bb1017b5425ffeec1_hd.jpg alt=img></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>StandardScaler</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=c1>#标准化，返回值为标准化后的数据</span>
</span></span><span class=line><span class=cl><span class=n>StandardScaler</span><span class=p>()</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>iris</span><span class=o>.</span><span class=n>data</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=222--区间缩放法>2.2.2 <strong>区间缩放法</strong><a hidden class=anchor aria-hidden=true href=#222--区间缩放法>#</a></h4><p>　区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，公式表达为：</p><p>　　使用preproccessing库的MinMaxScaler类对数据进行区间缩放的代码如下：</p><p><img loading=lazy src=https://pic1.zhimg.com/50/0f119a8e8f69509c5b95ef6a8a01a809_hd.jpg alt=img>
<img loading=lazy src=https://pic1.zhimg.com/80/0f119a8e8f69509c5b95ef6a8a01a809_hd.jpg alt=img></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>MinMaxScaler</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#区间缩放，返回值为缩放到[0, 1]区间的数据</span>
</span></span><span class=line><span class=cl><span class=n>MinMaxScaler</span><span class=p>()</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>iris</span><span class=o>.</span><span class=n>data</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=213-标准化与归一化的区别>2.1.3 标准化与归一化的区别<a hidden class=anchor aria-hidden=true href=#213-标准化与归一化的区别>#</a></h4><p>简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下：</p><p><strong>什么时候需要进行归一化？</strong></p><ul><li>归一化后加快了梯度下降求最优解的速度</li><li>归一化有可能提高精度</li></ul><p><strong>什么时候需要进行归一化？</strong></p><ul><li>通常在需要用到梯度下降法的时候。</li></ul><p>包括线性回归、逻辑回归、支持向量机、神经网络等模型。</p><ul><li>决策树模型就不适用</li></ul><p>例如 C4.5 ，主要根据信息增益比来分裂，归一化不会改变样本在特征 x 上的信息增益</p><p>比较概率大小分布即可，不需要。</p><p><img loading=lazy src=https://pic1.zhimg.com/50/fbb2fd0a163f2fa211829b735194baac_hd.jpg alt=img>
<img loading=lazy src=https://pic1.zhimg.com/80/fbb2fd0a163f2fa211829b735194baac_hd.jpg alt=img></p><p>　　使用preproccessing库的Normalizer类对数据进行归一化的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>Normalizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#归一化，返回值为归一化后的数据</span>
</span></span><span class=line><span class=cl><span class=n>Normalizer</span><span class=p>()</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>iris</span><span class=o>.</span><span class=n>data</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=23-对定量特征二值化>2.3 <strong>对定量特征二值化</strong><a hidden class=anchor aria-hidden=true href=#23-对定量特征二值化>#</a></h4><p>定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0，公式表达如下：</p><p><img loading=lazy src=https://pic2.zhimg.com/50/11111244c5b69c1af6c034496a2591ad_hd.jpg alt=img>
<img loading=lazy src=https://pic2.zhimg.com/80/11111244c5b69c1af6c034496a2591ad_hd.jpg alt=img></p><p>　　使用preproccessing库的Binarizer类对数据进行二值化的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>Binarizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#二值化，阈值设置为3，返回值为二值化后的数据</span>
</span></span><span class=line><span class=cl><span class=n>Binarizer</span><span class=p>(</span><span class=n>threshold</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>iris</span><span class=o>.</span><span class=n>data</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=24-对定性特征哑编码>2.4 <strong>对定性特征哑编码</strong><a hidden class=anchor aria-hidden=true href=#24-对定性特征哑编码>#</a></h4><p>由于IRIS数据集的特征皆为定量特征，故使用其目标值进行哑编码（实际上是不需要的）。使用preproccessing库的OneHotEncoder类对数据进行哑编码的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>OneHotEncoder</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据</span>
</span></span><span class=line><span class=cl><span class=n>OneHotEncoder</span><span class=p>()</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>iris</span><span class=o>.</span><span class=n>target</span><span class=o>.</span><span class=n>reshape</span><span class=p>((</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>)))</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=25-缺失值计算>2.5 <strong>缺失值计算</strong><a hidden class=anchor aria-hidden=true href=#25-缺失值计算>#</a></h4><p>　由于IRIS数据集没有缺失值，故对数据集新增一个样本，4个特征均赋值为NaN，表示数据缺失。使用preproccessing库的Imputer类对数据进行缺失值计算的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>numpy</span> <span class=kn>import</span> <span class=n>vstack</span><span class=p>,</span> <span class=n>array</span><span class=p>,</span> <span class=n>nan</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>Imputer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#缺失值计算，返回值为计算缺失值后的数据</span>
</span></span><span class=line><span class=cl><span class=c1>#参数missing_value为缺失值的表示形式，默认为NaN</span>
</span></span><span class=line><span class=cl><span class=c1>#参数strategy为缺失值填充方式，默认为mean（均值）</span>
</span></span><span class=line><span class=cl><span class=n>Imputer</span><span class=p>()</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>vstack</span><span class=p>((</span><span class=n>array</span><span class=p>([</span><span class=n>nan</span><span class=p>,</span> <span class=n>nan</span><span class=p>,</span> <span class=n>nan</span><span class=p>,</span> <span class=n>nan</span><span class=p>]),</span> <span class=n>iris</span><span class=o>.</span><span class=n>data</span><span class=p>)))</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=26--数据变换>2.6 <strong>数据变换</strong><a hidden class=anchor aria-hidden=true href=#26--数据变换>#</a></h4><p>常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。4个特征，度为2的多项式转换公式如下：</p><p><img loading=lazy src=https://pic1.zhimg.com/50/d1c57a66fad39df90b87cea330efb3f3_hd.jpg alt=img>
<img loading=lazy src=https://pic1.zhimg.com/80/d1c57a66fad39df90b87cea330efb3f3_hd.jpg alt=img></p><p>　　使用preproccessing库的PolynomialFeatures类对数据进行多项式转换的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>PolynomialFeatures</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#多项式转换</span>
</span></span><span class=line><span class=cl><span class=c1>#参数degree为度，默认值为2</span>
</span></span><span class=line><span class=cl><span class=n>PolynomialFeatures</span><span class=p>()</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>iris</span><span class=o>.</span><span class=n>data</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>基于单变元函数的数据变换可以使用一个统一的方式完成，使用preproccessing库的FunctionTransformer对数据进行对数函数转换的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>numpy</span> <span class=kn>import</span> <span class=n>log1p</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>FunctionTransformer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#自定义转换函数为对数函数的数据变换</span>
</span></span><span class=line><span class=cl><span class=c1>#第一个参数是单变元函数</span>
</span></span><span class=line><span class=cl><span class=n>FunctionTransformer</span><span class=p>(</span><span class=n>log1p</span><span class=p>)</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>iris</span><span class=o>.</span><span class=n>data</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=27-回归>2.7 回归<a hidden class=anchor aria-hidden=true href=#27-回归>#</a></h4><p>可以用一个函数（如回归函数）拟合数据来光滑数据。线性回归涉及找出拟合两个属性（或变量）的“最佳”线，是的一个属性可以用来预测另一个。多元线性回归是线性回归的扩展，其中涉及的属性多于两个，并且数据拟合到一个多维曲面。</p><h3 id=3-异常值处理方法><strong>3. 异常值处理方法</strong><a hidden class=anchor aria-hidden=true href=#3-异常值处理方法>#</a></h3><ul><li>删除含有异常值的记录；</li><li>某些筛选出来的异常样本是否真的是不需要的异常特征样本，最好找懂业务的再确认一下，防止我们将正常的样本过滤掉了。</li><li>将异常值视为缺失值，交给缺失值处理方法来处理；</li><li>使用均值/中位数/众数来修正；</li><li>不处理。</li></ul><h3 id=4-什么是组合特征如何处理高维组合特征>4. 什么是组合特征？如何处理高维组合特征？<a hidden class=anchor aria-hidden=true href=#4-什么是组合特征如何处理高维组合特征>#</a></h3><p>为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合成高阶特征，构成交互特征（Interaction Feature）。以广告点击预估问题为例，如图1所示，原始数据有语言和类型两种离散特征。为了提高拟合能力，语言和类型可以组成二阶特征。</p><p><img loading=lazy src=../img/tb_language.png alt=image-20191223183708541></p><p><img loading=lazy src=..%5cimg%5ctb_language2.png alt=image-20191223183826645></p><h3 id=5-类别型特征>5. <strong>类别型特征</strong><a hidden class=anchor aria-hidden=true href=#5-类别型特征>#</a></h3><p><strong>什么是类别型特征？</strong></p><p>例如：性别（男、女）、血型（A、B、AB、O）</p><p>通常是字符串形式，需要转化成数值型，传递给模型</p><p><strong>如何处理类别型特征？</strong></p><ul><li>序号编码（Ordinal Encoding）</li></ul><p>例如学习成绩有高中低三档，也就是不同类别之间关系。</p><p>这时可以用321来表示，保留了大小关系。</p><ul><li>独热编码（One-hot Encoding）</li></ul><p>例如血型，它的类别没有大小关系。A 型血表示为（1, 0, 0, 0），B 型血表示为（0, 1, 0, 0）……</p><ul><li>二进制编码（Binary Encoding）</li></ul><p>第一步，先用序号编码给每个类别编码</p><p>第二步，将类别 ID 转化为相应的二进制</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://reid00.github.io/en/tags/feature-engineering/>Feature Engineering</a></li><li><a href=https://reid00.github.io/en/tags/data-preprocessing/>Data Preprocessing</a></li></ul><nav class=paginav><a class=prev href=https://reid00.github.io/en/posts/ml/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D/><span class=title>« Prev</span><br><span>梯度下降原理介绍</span>
</a><a class=next href=https://reid00.github.io/en/posts/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B9%8B%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/><span class=title>Next »</span><br><span>特征工程之特征选择</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main></body></html>