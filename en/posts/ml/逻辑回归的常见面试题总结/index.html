<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>逻辑回归的常见面试题总结 | Reid's Blog</title>
<meta name=keywords content="面试,逻辑回归,LR"><meta name=description content="逻辑回归的常见面试题总结"><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/en/posts/ml/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><head><meta name=referrer content="no-referrer"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK",{anonymize_ip:!1})}</script><meta property="og:title" content="逻辑回归的常见面试题总结"><meta property="og:description" content="逻辑回归的常见面试题总结"><meta property="og:type" content="article"><meta property="og:url" content="https://reid00.github.io/en/posts/ml/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-16T19:35:26+08:00"><meta property="article:modified_time" content="2023-03-16T19:35:26+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="逻辑回归的常见面试题总结"><meta name=twitter:description content="逻辑回归的常见面试题总结"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://reid00.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"机器学习，深度学习，知识图谱相关","item":"https://reid00.github.io/en/posts/ml/"},{"@type":"ListItem","position":3,"name":"逻辑回归的常见面试题总结","item":"https://reid00.github.io/en/posts/ml/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"逻辑回归的常见面试题总结","name":"逻辑回归的常见面试题总结","description":"逻辑回归的常见面试题总结","keywords":["面试","逻辑回归","LR"],"articleBody":"1.简介 逻辑回归是面试当中非常喜欢问到的一个机器学习算法，因为表面上看逻辑回归形式上很简单，很好掌握，但是一问起来就容易懵逼。所以在面试的时候给大家的第一个建议不要说自己精通逻辑回归，非常容易被问倒，从而减分。下面总结了一些平常我在作为面试官面试别人和被别人面试的时候，经常遇到的一些问题。\nRegression问题的常规步骤为：\n寻找h函数（即假设估计的函数）； 构造J函数（损失函数）； 想办法使得J函数最小并求得回归参数（θ）； 数据拟合问题 2.正式介绍 如何凸显你是一个对逻辑回归已经非常了解的人呢。那就是用一句话概括它！逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。\n这里面其实包含了5个点 1：逻辑回归的假设，2：逻辑回归的损失函数，3：逻辑回归的求解方法，4：逻辑回归的目的，5:逻辑回归如何分类。这些问题是考核你对逻辑回归的基本了解。\n逻辑回归的基本假设 任何的模型都是有自己的假设，在这个假设下模型才是适用的。逻辑回归的第一个基本假设是**假设数据服从伯努利分布。**伯努利分布有一个简单的例子是抛硬币，抛中为正面的概率是pp,抛中为负面的概率是1−p1−p.在逻辑回归这个模型里面是假设 hθ(x)hθ(x) 为样本为正的概率，1−hθ(x)1−hθ(x)为样本为负的概率。那么整个模型可以描述为\nhθ(x;θ)=phθ(x;θ)=p\n逻辑回归的第二个假设是假设样本为正的概率是\np=11+e−θTxp=11+e−θTx\n所以逻辑回归的最终形式\nhθ(x;θ)=11+e−θTx\n逻辑回归的求解方法 由于该极大似然函数无法直接求解，我们一般通过对该函数进行梯度下降来不断逼急最优解。在这个地方其实会有个加分的项，考察你对其他优化方法的了解。因为就梯度下降本身来看的话就有随机梯度下降，批梯度下降，small batch 梯度下降三种方式，面试官可能会问这三种方式的优劣以及如何选择最合适的梯度下降方式。\n简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。\n随机梯度下降是以高方差频繁更新，优点是使得sgd（随机梯度下降）会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。\n如果使用梯度下降法(批量梯度下降法)，那么每次迭代过程中都要对 个样本进行求梯度，所以开销非常大，随机梯度下降的思想就是随机采样一个样本 来更新参数，那么计算开销就从 下降到 。\n随机梯度下降虽然提高了计算效率，降低了计算开销，但是由于每次迭代只随机选择一个样本，因此随机性比较大，所以下降过程中非常曲折\n可以看到多了随机两个字，随机也就是说我们用样本中的一个例子来近似我所有的样本，来调整θ，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，**对于最优化问题，凸问题，**虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。\n小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。小批量梯度下降的开销为 其中 是批量大小。\n其实这里还有一个隐藏的更加深的加分项，看你了不了解诸如Adam，动量法等优化方法。因为上述方法其实还有两个致命的问题。 第一个是如何对模型选择合适的学习率。自始至终保持同样的学习率其实不太合适。因为一开始参数刚刚开始学习的时候，此时的参数和最优解隔的比较远，需要保持一个较大的学习率尽快逼近最优解。但是学习到后面的时候，参数和最优解已经隔的比较近了，你还保持最初的学习率，容易越过最优点，在最优点附近来回振荡，通俗一点说，就很容易学过头了，跑偏了。 第二个是如何对参数选择合适的学习率。在实践中，对每个参数都保持的同样的学习率也是很不合理的。有些参数更新频繁，那么学习率可以适当小一点。有些参数更新缓慢，那么学习率就应该大一点。这里我们不展开，有空我会专门出一个专题介绍。 逻辑回归的目的 该函数的目的便是将数据二分类，提高准确率。 逻辑回归如何分类 逻辑回归作为一个回归(也就是y值是连续的)，如何应用到分类上去呢。y值确实是一个连续的变量。逻辑回归的做法是划定一个阈值，y值大于这个阈值的是一类，y值小于这个阈值的是另外一类。阈值具体如何调整根据实际情况选择。一般会选择0.5做为阈值来划分。 逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？ 损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。至于原因大家可以求出这个式子的梯度更新\n这个式子的更新速度只和相关。和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。\n为什么不选平方损失函数的呢？其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和sigmod函数本身的梯度是很相关的。sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。\n逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？ 先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。\n但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一\n如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。\n为什么我们还是会在训练的过程当中将高度相关的特征去掉？ 去掉高度相关的特征会让模型的可解释性更好 可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。 4.逻辑回归的优缺点总结 优点\n形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。\n模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。\n训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。\n资源占用小,尤其是内存。因为只需要存储各个维度的特征值，。\n方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。\n但是逻辑回归本身也有许多的缺点:\n准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。\n很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。\n处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。\n逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。\n怎么防止过拟合？ 通过正则化方法。正则化方法是指在进行目标函数或代价函数优化时，在目标函数或代价函数后面加上一个正则项，一般有L1正则与L2正则等\n为什么正则化可以防止过拟合？ 过拟合表现在训练数据上的误差非常小，而在测试数据上误差反而增大。其原因一般是模型过于复杂，过分得去拟合数据的噪声**。正则化则是对模型参数添加先验，使得模型复杂度较小，对于噪声扰动相对较小**。\n最简单的解释就是加了先验。在数据少的时候，先验知识可以防止过拟合。\n举个例子：\n硬币，推断正面朝上的概率。如果只能抛5次，很可能5次全正面朝上，这样你就得出错误的结论：正面朝上的概率是1——–过拟合！如果你在模型里加正面朝上概率是0.5的先验，结果就不会那么离谱。这其实就是正则\nL1正则和L2正则有什么区别？ 相同点：都用于避免过拟合\n不同点：L2与L1的区别在于，L1正则是拉普拉斯先验，而L2正则则是高斯先验。L1可以产生稀疏解,可以让一部分特征的系数缩小到0，从而间接实现特征选择。所以L1适用于特征之间有关联的情况。L2让所有特征的系数都缩小，但是不会减为0，它会使优化求解稳定快速。所以L2适用于特征之间没有关联的情况\n因为L1和服从拉普拉斯分布，所以L1在0点处不可导，难以计算，这个方法可以使用Proximal Algorithms或者ADMM来解决。\n如何用LR解决非线性问题？ 将特征离散成高维的01特征可以解决分类模型的非线性问题\n3. 逻辑回归是线性模型么，说下原因？ 狭义线性模型的前提是因变量误差是正态分布，但很多情况下这并不满足，比如对足球比分的预测显然用泊松分布是更好的选择。而广义的”广”在于引入了联系函数，于是误差变成了只要满足指数分布族就行了，因此适用性更强。\n​ 简单来说广义线性模型分为两个部分，第一个部分是描述了自变量和因变量的系统关系，也就是”线性”所在；第二个部分是描述了因变量的误差，这可以建模成各种满足指数分布族的分布。而联系函数就是把这两个部分连接起来的桥梁，也就是把因变量的期望表示为了自变量线性组合的函数。而像逻辑回归这样的简单广义线性模型，实际是将自变量的线性组合变成了联系函数的自然参数，这类联系函数也可以叫做正则联系函数。\n4. 逻辑回归算法为什么用的是sigmoid函数而不用阶跃函数？ 阶跃函数虽然能够直观刻画分类的错误率，但是由于其非凸、非光滑的特点，使得算法很难直接对该函数进行优化。而sigmoid函数本身的特征（光滑无限阶可导），以及完美的映射到概率空间，就用于逻辑回归了。解释上可从三个方面：- 最大熵定理- 伯努利分布假设- 贝叶斯理论\n参考: https://fengxc.me/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E4%B8%AD.html\n","wordCount":"98","inLanguage":"en","datePublished":"2023-03-16T19:35:26+08:00","dateModified":"2023-03-16T19:35:26+08:00","author":[{"@type":"Person","name":"Reid"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://reid00.github.io/en/posts/ml/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/"},"publisher":{"@type":"Organization","name":"Reid's Blog","logo":{"@type":"ImageObject","url":"https://reid00.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://reid00.github.io/en/ accesskey=h title="Reid's Blog (Alt + H)">Reid's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://reid00.github.io/en/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://reid00.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://reid00.github.io/en/categories/ title=Categorys><span>Categorys</span></a></li><li><a href=https://reid00.github.io/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://reid00.github.io/en/>Home</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/ml/>机器学习，深度学习，知识图谱相关</a></div><h1 class=post-title>逻辑回归的常见面试题总结</h1><div class=post-description>逻辑回归的常见面试题总结</div><div class=post-meta>&lt;span title='2023-03-16 19:35:26 +0800 +0800'>2023-03-16&lt;/span>&amp;nbsp;·&amp;nbsp;1 min&amp;nbsp;·&amp;nbsp;Reid</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><ul><li><a href=#1%e7%ae%80%e4%bb%8b aria-label=1.简介>1.简介</a></li><li><a href=#2%e6%ad%a3%e5%bc%8f%e4%bb%8b%e7%bb%8d aria-label=2.正式介绍>2.正式介绍</a><ul><li><a href=#%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%e7%9a%84%e5%9f%ba%e6%9c%ac%e5%81%87%e8%ae%be aria-label=逻辑回归的基本假设>逻辑回归的基本假设</a></li><li><a href=#%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%e7%9a%84%e6%b1%82%e8%a7%a3%e6%96%b9%e6%b3%95 aria-label=逻辑回归的求解方法>逻辑回归的求解方法</a></li><li><a href=#%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%e7%9a%84%e7%9b%ae%e7%9a%84 aria-label=逻辑回归的目的>逻辑回归的目的</a></li><li><a href=#%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%e5%a6%82%e4%bd%95%e5%88%86%e7%b1%bb aria-label=逻辑回归如何分类>逻辑回归如何分类</a></li><li><a href=#%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%e7%9a%84%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e4%bd%bf%e7%94%a8%e6%9e%81%e5%a4%a7%e4%bc%bc%e7%84%b6%e5%87%bd%e6%95%b0%e4%bd%9c%e4%b8%ba%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0 aria-label=逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？>逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？</a></li><li><a href=#%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%e5%9c%a8%e8%ae%ad%e7%bb%83%e7%9a%84%e8%bf%87%e7%a8%8b%e5%bd%93%e4%b8%ad%e5%a6%82%e6%9e%9c%e6%9c%89%e5%be%88%e5%a4%9a%e7%9a%84%e7%89%b9%e5%be%81%e9%ab%98%e5%ba%a6%e7%9b%b8%e5%85%b3%e6%88%96%e8%80%85%e8%af%b4%e6%9c%89%e4%b8%80%e4%b8%aa%e7%89%b9%e5%be%81%e9%87%8d%e5%a4%8d%e4%ba%86100%e9%81%8d%e4%bc%9a%e9%80%a0%e6%88%90%e6%80%8e%e6%a0%b7%e7%9a%84%e5%bd%b1%e5%93%8d aria-label=逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？><strong>逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？</strong></a></li><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e6%88%91%e4%bb%ac%e8%bf%98%e6%98%af%e4%bc%9a%e5%9c%a8%e8%ae%ad%e7%bb%83%e7%9a%84%e8%bf%87%e7%a8%8b%e5%bd%93%e4%b8%ad%e5%b0%86%e9%ab%98%e5%ba%a6%e7%9b%b8%e5%85%b3%e7%9a%84%e7%89%b9%e5%be%81%e5%8e%bb%e6%8e%89 aria-label=为什么我们还是会在训练的过程当中将高度相关的特征去掉？><strong>为什么我们还是会在训练的过程当中将高度相关的特征去掉？</strong></a></li><li><a href=#4%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%e7%9a%84%e4%bc%98%e7%bc%ba%e7%82%b9%e6%80%bb%e7%bb%93 aria-label=4.逻辑回归的优缺点总结><strong>4.逻辑回归的优缺点总结</strong></a></li><li><a href=#%e6%80%8e%e4%b9%88%e9%98%b2%e6%ad%a2%e8%bf%87%e6%8b%9f%e5%90%88 aria-label=怎么防止过拟合？><strong>怎么防止过拟合？</strong></a></li><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e6%ad%a3%e5%88%99%e5%8c%96%e5%8f%af%e4%bb%a5%e9%98%b2%e6%ad%a2%e8%bf%87%e6%8b%9f%e5%90%88 aria-label=为什么正则化可以防止过拟合？><strong>为什么正则化可以防止过拟合？</strong></a></li><li><a href=#l1%e6%ad%a3%e5%88%99%e5%92%8cl2%e6%ad%a3%e5%88%99%e6%9c%89%e4%bb%80%e4%b9%88%e5%8c%ba%e5%88%ab aria-label=L1正则和L2正则有什么区别？><strong>L1正则和L2正则有什么区别？</strong></a></li><li><a href=#%e5%a6%82%e4%bd%95%e7%94%a8lr%e8%a7%a3%e5%86%b3%e9%9d%9e%e7%ba%bf%e6%80%a7%e9%97%ae%e9%a2%98 aria-label=如何用LR解决非线性问题？><strong>如何用LR解决非线性问题？</strong></a></li></ul></li><li><a href=#3-%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%e6%98%af%e7%ba%bf%e6%80%a7%e6%a8%a1%e5%9e%8b%e4%b9%88%e8%af%b4%e4%b8%8b%e5%8e%9f%e5%9b%a0 aria-label="3. 逻辑回归是线性模型么，说下原因？">3. 逻辑回归是线性模型么，说下原因？</a></li></ul><li><a href=#4-%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%e7%ae%97%e6%b3%95%e4%b8%ba%e4%bb%80%e4%b9%88%e7%94%a8%e7%9a%84%e6%98%afsigmoid%e5%87%bd%e6%95%b0%e8%80%8c%e4%b8%8d%e7%94%a8%e9%98%b6%e8%b7%83%e5%87%bd%e6%95%b0 aria-label="4. 逻辑回归算法为什么用的是sigmoid函数而不用阶跃函数？">4. 逻辑回归算法为什么用的是sigmoid函数而不用阶跃函数？</a></li></ul></div></details></div><div class=post-content><h3 id=1简介>1.简介<a hidden class=anchor aria-hidden=true href=#1简介>#</a></h3><p>逻辑回归是面试当中非常喜欢问到的一个机器学习算法，因为表面上看逻辑回归形式上很简单，很好掌握，但是一问起来就容易懵逼。所以在面试的时候给大家的第一个建议不要说自己精通逻辑回归，非常容易被问倒，从而减分。下面总结了一些平常我在作为面试官面试别人和被别人面试的时候，经常遇到的一些问题。</p><p>Regression问题的常规步骤为：</p><ol><li>寻找h函数（即假设估计的函数）；</li><li>构造J函数（损失函数）；</li><li>想办法使得J函数最小并求得回归参数（θ）；</li><li>数据拟合问题</li></ol><h3 id=2正式介绍>2.正式介绍<a hidden class=anchor aria-hidden=true href=#2正式介绍>#</a></h3><p>如何凸显你是一个对逻辑回归已经非常了解的人呢。那就是用一句话概括它！<strong>逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。</strong></p><p>这里面其实包含了5个点 1：逻辑回归的假设，2：逻辑回归的损失函数，3：逻辑回归的求解方法，4：逻辑回归的目的，5:逻辑回归如何分类。这些问题是考核你对逻辑回归的基本了解。</p><h4 id=逻辑回归的基本假设>逻辑回归的基本假设<a hidden class=anchor aria-hidden=true href=#逻辑回归的基本假设>#</a></h4><ul><li><ul><li><p>任何的模型都是有自己的假设，在这个假设下模型才是适用的。逻辑回归的<strong>第一个</strong>基本假设是**假设数据服从伯努利分布。**伯努利分布有一个简单的例子是抛硬币，抛中为正面的概率是pp,抛中为负面的概率是1−p1−p.在逻辑回归这个模型里面是假设 hθ(x)hθ(x) 为样本为正的概率，1−hθ(x)1−hθ(x)为样本为负的概率。那么整个模型可以描述为</p><p>hθ(x;θ)=phθ(x;θ)=p</p></li><li><p>逻辑回归的第二个假设是假设样本为正的概率是</p><p>p=11+e−θTxp=11+e−θTx</p></li></ul></li><li><ul><li><p>所以逻辑回归的最终形式</p><p>hθ(x;θ)=11+e−θTx</p></li></ul></li></ul><h4 id=逻辑回归的求解方法>逻辑回归的求解方法<a hidden class=anchor aria-hidden=true href=#逻辑回归的求解方法>#</a></h4><ul><li><p>由于该极大似然函数无法直接求解，我们一般通过对该函数进行梯度下降来不断逼急最优解。在这个地方其实会有个加分的项，考察你对其他优化方法的了解。因为就梯度下降本身来看的话就有随机梯度下降，批梯度下降，small batch 梯度下降三种方式，面试官可能会问这三种方式的优劣以及如何选择最合适的<strong>梯度下降</strong>方式。</p><ul><li><p>简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。</p></li><li><p>随机梯度下降是以高方差频繁更新，优点是使得sgd（随机梯度下降）会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。</p><p>如果使用梯度下降法(批量梯度下降法)，那么<strong>每次迭代过程中都要对</strong> <img loading=lazy src="https://www.zhihu.com/equation?tex=n" alt=[公式]>
<strong>个样本进行求梯度</strong>，所以开销非常大，<strong>随机梯度下降的思想就是随机采样一个样本</strong> <img loading=lazy src="https://www.zhihu.com/equation?tex=J%28x_i%29" alt=[公式]>
<strong>来更新参数</strong>，那么计算开销就从 <img loading=lazy src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%7B%28n%29%7D" alt=[公式]>
下降到 <img loading=lazy src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%7B%281%29%7D" alt=[公式]>
。</p><p>随机梯度下降虽然提高了计算效率，降低了计算开销，但是由于每次迭代只随机选择一个样本，<strong>因此随机性比较大，所以下降过程中非常曲折</strong></p><p>可以看到多了随机两个字，随机也就是说我们用样本中的一个例子来近似我所有的样本，来调整<em>θ</em>，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，**对于最优化问题，凸问题，**虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。</p></li><li><p>小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。小批量梯度下降的开销为 <img loading=lazy src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%7B%28%5Cleft%7C+%5Cmathscr%7BB%7D+%5Cright%7C%29%7D" alt=[公式]>
其中 <img loading=lazy src="https://www.zhihu.com/equation?tex=%5Cleft%7C+%5Cmathscr%7BB%7D+%5Cright%7C" alt=[公式]>
是批量大小。</p></li></ul></li><li><ul><li>其实这里还有一个隐藏的更加深的加分项，看你了不了解诸如Adam，动量法等优化方法。因为上述方法其实还有两个致命的问题。<ul><li>第一个是如何对模型选择合适的学习率。自始至终保持同样的学习率其实不太合适。因为一开始参数刚刚开始学习的时候，此时的参数和最优解隔的比较远，需要保持一个较大的学习率尽快逼近最优解。但是学习到后面的时候，参数和最优解已经隔的比较近了，你还保持最初的学习率，容易越过最优点，在最优点附近来回振荡，通俗一点说，就很容易学过头了，跑偏了。</li><li>第二个是如何对参数选择合适的学习率。在实践中，对每个参数都保持的同样的学习率也是很不合理的。有些参数更新频繁，那么学习率可以适当小一点。有些参数更新缓慢，那么学习率就应该大一点。这里我们不展开，有空我会专门出一个专题介绍。</li></ul></li></ul></li></ul><h4 id=逻辑回归的目的>逻辑回归的目的<a hidden class=anchor aria-hidden=true href=#逻辑回归的目的>#</a></h4><ul><li>该函数的目的便是将数据二分类，提高准确率。</li></ul><h4 id=逻辑回归如何分类>逻辑回归如何分类<a hidden class=anchor aria-hidden=true href=#逻辑回归如何分类>#</a></h4><ul><li>逻辑回归作为一个回归(也就是y值是连续的)，如何应用到分类上去呢。y值确实是一个连续的变量。逻辑回归的做法是划定一个阈值，y值大于这个阈值的是一类，y值小于这个阈值的是另外一类。阈值具体如何调整根据实际情况选择。一般会选择0.5做为阈值来划分。</li></ul><h4 id=逻辑回归的损失函数为什么要使用极大似然函数作为损失函数>逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？<a hidden class=anchor aria-hidden=true href=#逻辑回归的损失函数为什么要使用极大似然函数作为损失函数>#</a></h4><ul><li><p>损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。至于原因大家可以求出这个式子的梯度更新</p><p>这个式子的更新速度只和相关。和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。</p></li><li><p>为什么不选平方损失函数的呢？其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和sigmod函数本身的梯度是很相关的。sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。</p></li></ul><h4 id=逻辑回归在训练的过程当中如果有很多的特征高度相关或者说有一个特征重复了100遍会造成怎样的影响><strong>逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？</strong><a hidden class=anchor aria-hidden=true href=#逻辑回归在训练的过程当中如果有很多的特征高度相关或者说有一个特征重复了100遍会造成怎样的影响>#</a></h4><p>先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。</p><p>但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一</p><p>如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。</p><h4 id=为什么我们还是会在训练的过程当中将高度相关的特征去掉><strong>为什么我们还是会在训练的过程当中将高度相关的特征去掉？</strong><a hidden class=anchor aria-hidden=true href=#为什么我们还是会在训练的过程当中将高度相关的特征去掉>#</a></h4><ul><li>去掉高度相关的特征会让模型的可解释性更好</li><li>可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。</li></ul><h4 id=4逻辑回归的优缺点总结><strong>4.逻辑回归的优缺点总结</strong><a hidden class=anchor aria-hidden=true href=#4逻辑回归的优缺点总结>#</a></h4><p><strong>优点</strong></p><ul><li><p>形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。</p></li><li><p>模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。</p></li><li><p>训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。</p></li><li><p>资源占用小,尤其是内存。因为只需要存储各个维度的特征值，。</p></li><li><p>方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。</p></li></ul><p><strong>但是逻辑回归本身也有许多的缺点:</strong></p><ul><li><p>准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。</p></li><li><p>很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。</p></li><li><p>处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。</p></li><li><p>逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。</p></li></ul><h4 id=怎么防止过拟合><strong>怎么防止过拟合？</strong><a hidden class=anchor aria-hidden=true href=#怎么防止过拟合>#</a></h4><p>通过正则化方法。正则化方法是指在进行目标函数或代价函数优化时，在目标函数或代价函数后面加上一个正则项，一般有L1正则与L2正则等</p><h4 id=为什么正则化可以防止过拟合><strong>为什么正则化可以防止过拟合？</strong><a hidden class=anchor aria-hidden=true href=#为什么正则化可以防止过拟合>#</a></h4><p>过拟合表现在训练数据上的误差非常小，而在测试数据上误差反而增大。其原因一般是模型过于复杂，过分得去拟合数据的噪声**。正则化则是对模型参数添加先验，使得模型复杂度较小，对于噪声扰动相对较小**。</p><p>最简单的解释就是加了先验。在数据少的时候，先验知识可以防止过拟合。</p><p>举个例子：</p><p>硬币，推断正面朝上的概率。如果只能抛5次，很可能5次全正面朝上，这样你就得出错误的结论：正面朝上的概率是1&mdash;&mdash;&ndash;过拟合！如果你在模型里加正面朝上概率是0.5的先验，结果就不会那么离谱。这其实就是正则</p><h4 id=l1正则和l2正则有什么区别><strong>L1正则和L2正则有什么区别？</strong><a hidden class=anchor aria-hidden=true href=#l1正则和l2正则有什么区别>#</a></h4><p>相同点：都用于避免过拟合</p><p>不同点：<strong>L2与L1的区别在于，L1正则是拉普拉斯先验，而L2正则则是高斯先验</strong>。L1可以产生稀疏解,可以让一部分特征的系数缩小到0，从而间接实现特征选择。所以L1适用于特征之间有关联的情况。L2让所有特征的系数都缩小，但是不会减为0，它会使优化求解稳定快速。所以L2适用于特征之间没有关联的情况</p><p>因为L1和服从拉普拉斯分布，所以L1在0点处不可导，难以计算，这个方法可以使用Proximal Algorithms或者ADMM来解决。</p><h4 id=如何用lr解决非线性问题><strong>如何用LR解决非线性问题？</strong><a hidden class=anchor aria-hidden=true href=#如何用lr解决非线性问题>#</a></h4><p>将特征离散成高维的01特征可以解决分类模型的非线性问题</p><h3 id=3-逻辑回归是线性模型么说下原因>3. 逻辑回归是线性模型么，说下原因？<a hidden class=anchor aria-hidden=true href=#3-逻辑回归是线性模型么说下原因>#</a></h3><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.5phf4bim80g0.webp alt=img></p><p>狭义线性模型的前提是因变量误差是正态分布，但很多情况下这并不满足，比如对足球比分的预测显然用泊松分布是更好的选择。而广义的”广”在于引入了联系函数，于是误差变成了只要满足指数分布族就行了，因此适用性更强。</p><p>​ 简单来说广义线性模型分为两个部分，第一个部分是描述了自变量和因变量的系统关系，也就是”线性”所在；第二个部分是描述了因变量的误差，这可以建模成各种满足指数分布族的分布。而联系函数就是把这两个部分连接起来的桥梁，也就是把因变量的期望表示为了自变量线性组合的函数。而像逻辑回归这样的简单广义线性模型，实际是将自变量的线性组合变成了联系函数的自然参数，这类联系函数也可以叫做正则联系函数。</p><h2 id=4-逻辑回归算法为什么用的是sigmoid函数而不用阶跃函数>4. 逻辑回归算法为什么用的是sigmoid函数而不用阶跃函数？<a hidden class=anchor aria-hidden=true href=#4-逻辑回归算法为什么用的是sigmoid函数而不用阶跃函数>#</a></h2><p>阶跃函数虽然能够直观刻画分类的错误率，但是由于其非凸、非光滑的特点，使得算法很难直接对该函数进行优化。而sigmoid函数本身的特征（光滑无限阶可导），以及完美的映射到概率空间，就用于逻辑回归了。解释上可从三个方面：- 最大熵定理- 伯努利分布假设- 贝叶斯理论</p><p>参考: <a href=https://fengxc.me/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E4%B8%AD.html>https://fengxc.me/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E4%B8%AD.html</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://reid00.github.io/en/tags/%E9%9D%A2%E8%AF%95/>面试</a></li><li><a href=https://reid00.github.io/en/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/>逻辑回归</a></li><li><a href=https://reid00.github.io/en/tags/lr/>LR</a></li></ul><nav class=paginav><a class=prev href=https://reid00.github.io/en/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Bgbdt/><span class=title>« Prev</span><br><span>集成学习之GBD</span>
</a><a class=next href=https://reid00.github.io/en/posts/ml/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%9B%9E%E5%BD%92%E6%A0%91%E6%A8%A1%E5%9E%8B/><span class=title>Next »</span><br><span>随机森林（回归树）模型</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://reid00.github.io/en/>Reid's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>