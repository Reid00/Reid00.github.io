<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>梯度下降原理介绍 | Reid's Blog</title>
<meta name=keywords content="梯度下降"><meta name=description content="梯度下降原理介绍"><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/en/posts/ml/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://reid00.github.io/en/posts/ml/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><head><meta name=referrer content="no-referrer"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script><script>var doNotTrack=!1,dnt;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK")}</script><meta property="og:title" content="梯度下降原理介绍"><meta property="og:description" content="梯度下降原理介绍"><meta property="og:type" content="article"><meta property="og:url" content="https://reid00.github.io/en/posts/ml/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-16T19:35:24+08:00"><meta property="article:modified_time" content="2023-03-16T19:35:24+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="梯度下降原理介绍"><meta name=twitter:description content="梯度下降原理介绍"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://reid00.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"机器学习，深度学习，知识图谱相关","item":"https://reid00.github.io/en/posts/ml/"},{"@type":"ListItem","position":3,"name":"梯度下降原理介绍","item":"https://reid00.github.io/en/posts/ml/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"梯度下降原理介绍","name":"梯度下降原理介绍","description":"梯度下降原理介绍","keywords":["梯度下降"],"articleBody":"Summary 本文将从一个下山的场景开始，先提出梯度下降算法的基本思想，进而从数学上解释梯度下降算法的原理，最后实现一个简单的梯度下降算法的实例！\n梯度下降的场景假设 梯度下降法的基本思想可以类比为一个下山的过程。假设这样一个场景：一个人被困在山上，需要从山上下来(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。具体来说就是，以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走，同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。\n我们同时可以假设这座山最陡峭的地方是无法通过肉眼立马观察出来的，而是需要一个复杂的工具来测量，同时，这个人此时正好拥有测量出最陡峭方向的能力。所以，此人每走一段距离，都需要一段时间来测量所在位置最陡峭的方向，这是比较耗时的。那么为了在太阳下山之前到达山底，就要尽可能的减少测量方向的次数。这是一个两难的选择，如果测量的频繁，可以保证下山的方向是绝对正确的，但又非常耗时，如果测量的过少，又有偏离轨道的风险。所以需要找到一个合适的测量方向的频率，来确保下山的方向不错误，同时又不至于耗时太多！\n梯度下降 首先，我们有一个可微分的函数。这个函数就代表着一座山。我们的目标就是找到这个函数的最小值，也就是山底。根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是找到给定点的梯度 ，然后朝着梯度相反的方向，就能让函数值下降的最快！因为梯度的方向就是函数之变化最快的方向(在后面会详细解释) 所以，我们重复利用这个方法，反复求取梯度，最后就能到达局部的最小值，这就类似于我们下山的过程。而求取梯度就确定了最陡峭的方向，也就是场景中测量方向的手段。那么为什么梯度的方向就是最陡峭的方向呢？接下来，我们从微分开始讲起\n微分 看待微分的意义，可以有不同的角度，最常用的两种是：\n函数图像中，某点的切线的斜率\n函数的变化率 几个微分的例子：\n上面的例子都是单变量的微分，当一个函数有多个变量的时候，就有了多变量的微分，即分别对每个变量进行求微分\n梯度 梯度实际上就是多变量微分的一般化。 下面这个例子：\n我们可以看到，梯度就是分别对每个变量进行微分，然后用逗号分割开，梯度是用\u003c\u003e包括起来，说明梯度其实一个向量。\n梯度是微积分中一个很重要的概念，之前提到过梯度的意义\n在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率 在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向 这也就说明了为什么我们需要千方百计的求取梯度！我们需要到达山底，就需要在每一步观测到此时最陡峭的地方，梯度就恰巧告诉了我们这个方向。梯度的方向是函数在给定点上升最快的方向，那么梯度的反方向就是函数在给定点下降最快的方向，这正是我们所需要的。所以我们只要沿着梯度的方向一直走，就能走到局部的最低点！\n梯度下降算法的数学解释 上面我们花了大量的篇幅介绍梯度下降算法的基本思想和场景假设，以及梯度的概念和思想。下面我们就开始从数学上解释梯度下降算法的计算过程和思想！ 此公式的意义是：J是关于Θ的一个函数，我们当前所处的位置为Θ0点，要从这个点走到J的最小值点，也就是山底。首先我们先确定前进的方向，也就是梯度的反向，然后走一段距离的步长，也就是α，走完这个段步长，就到达了Θ1这个点！\n下面就这个公式的几个常见的疑问：\nα是什么含义？ α在梯度下降算法中被称作为学习率或者步长，意味着我们可以通过α来控制每一步走的距离，以保证不要步子跨的太大扯着蛋，哈哈，其实就是不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！ 为什么要梯度要乘以一个负号？ 梯度前加一个负号，就意味着朝着梯度相反的方向前进！我们在前文提到，梯度的方向实际就是函数在此点上升最快的方向！而我们需要朝着下降最快的方向走，自然就是负的梯度的方向，所以此处需要加上负号\n梯度下降算法的实例 我们已经基本了解了梯度下降算法的计算过程，那么我们就来看几个梯度下降算法的小实例，首先从单变量的函数开始\n单变量函数的梯度下降 我们假设有一个单变量的函数\n函数的微分 初始化，起点为 学习率为 根据梯度下降的计算公式\n我们开始进行梯度下降的迭代计算过程：\nimage.png\n如图，经过四次的运算，也就是走了四步，基本就抵达了函数的最低点，也就是山底\n多变量函数的梯度下降 我们假设有一个目标函数\n现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 (0，0)点。但是接下来，我们会从梯度下降算法开始一步步计算到这个最小值！ 我们假设初始的起点为：\n初始的学习率为：\n函数的梯度为：\n进行多次迭代：\n我们发现，已经基本靠近函数的最小值点\n梯度下降算法的实现 下面我们将用python实现一个简单的梯度下降算法。场景是一个简单的线性回归的例子：假设现在我们有一系列的点，如下图所示\n我们将用梯度下降法来拟合出这条直线！\n首先，我们需要定义一个代价函数，在此我们选用均方误差代价函数\n此公式中\nm是数据集中点的个数\n½是一个常量，这样是为了在求梯度的时候，二次方乘下来就和这里的½抵消了，自然就没有多余的常数系数，方便后续的计算，同时对结果不会有影响\ny 是数据集中每个点的真实y坐标的值\nh 是我们的预测函数，根据每一个输入x，根据Θ 计算得到预测的y值，即\n我们可以根据代价函数看到，代价函数中的变量有两个，所以是一个多变量的梯度下降问题，求解出代价函数的梯度，也就是分别对两个变量进行微分\n明确了代价函数和梯度，以及预测的函数形式。我们就可以开始编写代码了。但在这之前，需要说明一点，就是为了方便代码的编写，我们会将所有的公式都转换为矩阵的形式，python中计算矩阵是非常方便的，同时代码也会变得非常的简洁。\n为了转换为矩阵的计算，我们观察到预测函数的形式\n我们有两个变量，为了对这个公式进行矩阵化，我们可以给每一个点x增加一维，这一维的值固定为1，这一维将会乘到Θ0上。这样就方便我们统一矩阵化的计算\n然后我们将代价函数和梯度转化为矩阵向量相乘的形式\ncoding time 首先，我们需要定义数据集和学习率\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import numpy as np # Size of the points dataset. m = 20 # Points x-coordinate and dummy value (x0, x1). X0 = np.ones((m, 1)) X1 = np.arange(1, m+1).reshape(m, 1) X = np.hstack((X0, X1)) # Points y-coordinate y = np.array([ 3, 4, 5, 5, 2, 4, 7, 8, 11, 8, 12, 11, 13, 13, 16, 17, 18, 17, 19, 21 ]).reshape(m, 1) # The Learning Rate alpha. alpha = 0.01 接下来我们以矩阵向量的形式定义代价函数和代价函数的梯度\n1 2 3 4 5 6 7 8 9 def error_function(theta, X, y): '''Error function J definition.''' diff = np.dot(X, theta) - y return (1./2*m) * np.dot(np.transpose(diff), diff) def gradient_function(theta, X, y): '''Gradient of the function J definition.''' diff = np.dot(X, theta) - y return (1./m) * np.dot(np.transpose(X), diff) 最后就是算法的核心部分，梯度下降迭代计算\n1 2 3 4 5 6 7 8 def gradient_descent(X, y, alpha): '''Perform gradient descent.''' theta = np.array([1, 1]).reshape(2, 1) gradient = gradient_function(theta, X, y) while not np.all(np.absolute(gradient) \u003c= 1e-5): theta = theta - alpha * gradient gradient = gradient_function(theta, X, y) return theta 当梯度小于1e-5时，说明已经进入了比较平滑的状态，类似于山谷的状态，这时候再继续迭代效果也不大了，所以这个时候可以退出循环！\n完整的代码如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import numpy as np # Size of the points dataset. m = 20 # Points x-coordinate and dummy value (x0, x1). X0 = np.ones((m, 1)) X1 = np.arange(1, m+1).reshape(m, 1) X = np.hstack((X0, X1)) # Points y-coordinate y = np.array([ 3, 4, 5, 5, 2, 4, 7, 8, 11, 8, 12, 11, 13, 13, 16, 17, 18, 17, 19, 21 ]).reshape(m, 1) # The Learning Rate alpha. alpha = 0.01 def error_function(theta, X, y): '''Error function J definition.''' diff = np.dot(X, theta) - y return (1./2*m) * np.dot(np.transpose(diff), diff) def gradient_function(theta, X, y): '''Gradient of the function J definition.''' diff = np.dot(X, theta) - y return (1./m) * np.dot(np.transpose(X), diff) def gradient_descent(X, y, alpha): '''Perform gradient descent.''' theta = np.array([1, 1]).reshape(2, 1) gradient = gradient_function(theta, X, y) while not np.all(np.absolute(gradient) \u003c= 1e-5): theta = theta - alpha * gradient gradient = gradient_function(theta, X, y) return theta optimal = gradient_descent(X, y, alpha) print('optimal:', optimal) print('error function:', error_function(optimal, X, y)[0,0]) 运行代码，计算得到的结果如下\n所拟合出的直线如下\n小结 至此，我们就基本介绍完了梯度下降法的基本思想和算法流程，并且用python实现了一个简单的梯度下降算法拟合直线的案例！ 最后，我们回到文章开头所提出的场景假设: 这个下山的人实际上就代表了反向传播算法，下山的路径其实就代表着算法中一直在寻找的参数Θ，山上当前点的最陡峭的方向实际上就是代价函数在这一点的梯度方向，场景中观测最陡峭方向所用的工具就是微分 。在下一次观测之前的时间就是有我们算法中的学习率α所定义的。 可以看到场景假设和梯度下降算法很好的完成了对应！\n","wordCount":"464","inLanguage":"en","datePublished":"2023-03-16T19:35:24+08:00","dateModified":"2023-03-16T19:35:24+08:00","author":[{"@type":"Person","name":"Reid"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://reid00.github.io/en/posts/ml/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D/"},"publisher":{"@type":"Organization","name":"Reid's Blog","logo":{"@type":"ImageObject","url":"https://reid00.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://reid00.github.io/en/ accesskey=h title="Reid's Blog (Alt + H)">Reid's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://reid00.github.io/en/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://reid00.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://reid00.github.io/en/categories/ title=Categorys><span>Categorys</span></a></li><li><a href=https://reid00.github.io/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://reid00.github.io/en/>Home</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/ml/>机器学习，深度学习，知识图谱相关</a></div><h1 class=post-title>梯度下降原理介绍</h1><div class=post-description>梯度下降原理介绍</div><div class=post-meta><span title='2023-03-16 19:35:24 +0800 +0800'>2023-03-16 19:35</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;464 words&nbsp;·&nbsp;Reid</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#summary aria-label=Summary>Summary</a></li><li><a href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e7%9a%84%e5%9c%ba%e6%99%af%e5%81%87%e8%ae%be aria-label=梯度下降的场景假设>梯度下降的场景假设</a></li><li><a href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d aria-label=梯度下降>梯度下降</a></li><li><a href=#%e5%be%ae%e5%88%86 aria-label=微分>微分</a></li><li><a href=#%e6%a2%af%e5%ba%a6 aria-label=梯度>梯度</a></li><li><a href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e7%ae%97%e6%b3%95%e7%9a%84%e6%95%b0%e5%ad%a6%e8%a7%a3%e9%87%8a aria-label=梯度下降算法的数学解释>梯度下降算法的数学解释</a></li><li><a href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e7%ae%97%e6%b3%95%e7%9a%84%e5%ae%9e%e4%be%8b aria-label=梯度下降算法的实例>梯度下降算法的实例</a><ul><li><a href=#%e5%8d%95%e5%8f%98%e9%87%8f%e5%87%bd%e6%95%b0%e7%9a%84%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d aria-label=单变量函数的梯度下降>单变量函数的梯度下降</a></li><li><a href=#%e5%a4%9a%e5%8f%98%e9%87%8f%e5%87%bd%e6%95%b0%e7%9a%84%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d aria-label=多变量函数的梯度下降>多变量函数的梯度下降</a></li></ul></li><li><a href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e7%ae%97%e6%b3%95%e7%9a%84%e5%ae%9e%e7%8e%b0 aria-label=梯度下降算法的实现>梯度下降算法的实现</a><ul><li><a href=#coding-time aria-label="coding time">coding time</a></li></ul></li><li><a href=#%e5%b0%8f%e7%bb%93 aria-label=小结>小结</a></li></ul></div></details></div><div class=post-content><h1 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h1><p>本文将从一个下山的场景开始，先提出梯度下降算法的基本思想，进而从数学上解释梯度下降算法的原理，最后实现一个简单的梯度下降算法的实例！</p><h1 id=梯度下降的场景假设>梯度下降的场景假设<a hidden class=anchor aria-hidden=true href=#梯度下降的场景假设>#</a></h1><p>梯度下降法的基本思想可以类比为一个下山的过程。假设这样一个场景：一个人被困在山上，需要从山上下来(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。具体来说就是，以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走，同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.5ne3f60e6s00.webp alt=img></p><p>我们同时可以假设这座山最陡峭的地方是无法通过肉眼立马观察出来的，而是需要一个复杂的工具来测量，同时，这个人此时正好拥有测量出最陡峭方向的能力。所以，此人每走一段距离，都需要一段时间来测量所在位置最陡峭的方向，这是比较耗时的。那么为了在太阳下山之前到达山底，就要尽可能的减少测量方向的次数。这是一个两难的选择，如果测量的频繁，可以保证下山的方向是绝对正确的，但又非常耗时，如果测量的过少，又有偏离轨道的风险。所以需要找到一个合适的测量方向的频率，来确保下山的方向不错误，同时又不至于耗时太多！</p><h1 id=梯度下降>梯度下降<a hidden class=anchor aria-hidden=true href=#梯度下降>#</a></h1><p>首先，我们有一个可<a href="https://link.jianshu.com?t=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FDifferentiable_function"><em>微分</em></a>的函数。这个函数就代表着一座山。我们的目标就是找到这个函数的最小值，也就是山底。根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是找到给定点的<a href="https://link.jianshu.com?t=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FGradient"><em>梯度</em></a> ，然后朝着梯度相反的方向，就能让函数值下降的最快！因为梯度的方向就是函数之变化最快的方向(在后面会详细解释)
所以，我们重复利用这个方法，反复求取梯度，最后就能到达局部的最小值，这就类似于我们下山的过程。而求取梯度就确定了最陡峭的方向，也就是场景中测量方向的手段。那么为什么梯度的方向就是最陡峭的方向呢？接下来，我们从微分开始讲起</p><h1 id=微分>微分<a hidden class=anchor aria-hidden=true href=#微分>#</a></h1><p>看待微分的意义，可以有不同的角度，最常用的两种是：</p><ul><li><p>函数图像中，某点的切线的斜率</p></li><li><p>函数的变化率
几个微分的例子：</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.1aa0zb1palq8.webp alt=img></p></li></ul><p>上面的例子都是单变量的微分，当一个函数有多个变量的时候，就有了多变量的微分，即分别对每个变量进行求微分</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.47wlwrtv0060.webp alt=img></p><h1 id=梯度>梯度<a hidden class=anchor aria-hidden=true href=#梯度>#</a></h1><p>梯度实际上就是多变量微分的一般化。
下面这个例子：</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.18f8xif49sq.webp alt=img></p><p>我们可以看到，梯度就是分别对每个变量进行微分，然后用逗号分割开，梯度是用&lt;>包括起来，说明梯度其实一个向量。</p><p>梯度是微积分中一个很重要的概念，之前提到过梯度的意义</p><ul><li>在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率</li><li>在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向</li></ul><p>这也就说明了为什么我们需要千方百计的求取梯度！我们需要到达山底，就需要在每一步观测到此时最陡峭的地方，梯度就恰巧告诉了我们这个方向。梯度的方向是函数在给定点上升最快的方向，那么梯度的反方向就是函数在给定点下降最快的方向，这正是我们所需要的。所以我们只要沿着梯度的方向一直走，就能走到局部的最低点！</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.5ne3f60e6s00.webp alt=img></p><h1 id=梯度下降算法的数学解释>梯度下降算法的数学解释<a hidden class=anchor aria-hidden=true href=#梯度下降算法的数学解释>#</a></h1><p>上面我们花了大量的篇幅介绍梯度下降算法的基本思想和场景假设，以及梯度的概念和思想。下面我们就开始从数学上解释梯度下降算法的计算过程和思想！
<img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.6cpvdqqp4j40.webp alt=img></p><p>此公式的意义是：J是关于Θ的一个函数，我们当前所处的位置为Θ0点，要从这个点走到J的最小值点，也就是山底。首先我们先确定前进的方向，也就是梯度的反向，然后走一段距离的步长，也就是α，走完这个段步长，就到达了Θ1这个点！</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.1uglcp2k1jls.webp alt=img></p><p>下面就这个公式的几个常见的疑问：</p><ul><li>α是什么含义？
α在梯度下降算法中被称作为<strong>学习率</strong>或者<strong>步长</strong>，意味着我们可以通过α来控制每一步走的距离，以保证不要步子跨的太大扯着蛋，哈哈，其实就是不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！</li></ul><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.7nbkusi4ul4.webp alt=img></p><p>为什么要梯度要乘以一个负号？
梯度前加一个负号，就意味着朝着梯度相反的方向前进！我们在前文提到，梯度的方向实际就是函数在此点上升最快的方向！而我们需要朝着下降最快的方向走，自然就是负的梯度的方向，所以此处需要加上负号</p><h1 id=梯度下降算法的实例>梯度下降算法的实例<a hidden class=anchor aria-hidden=true href=#梯度下降算法的实例>#</a></h1><p>我们已经基本了解了梯度下降算法的计算过程，那么我们就来看几个梯度下降算法的小实例，首先从单变量的函数开始</p><h2 id=单变量函数的梯度下降>单变量函数的梯度下降<a hidden class=anchor aria-hidden=true href=#单变量函数的梯度下降>#</a></h2><p>我们假设有一个单变量的函数</p><p><img loading=lazy src=https:////upload-images.jianshu.io/upload_images/1234352-abb73822fb6d2a2c.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/127/format/webp alt=img></p><p>函数的微分
<img loading=lazy src=https:////upload-images.jianshu.io/upload_images/1234352-66ce0cdcef5e2686.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/141/format/webp alt=img></p><p>初始化，起点为
<img loading=lazy src=https:////upload-images.jianshu.io/upload_images/1234352-8ee36cc5ce832b17.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/86/format/webp alt=img></p><p>学习率为
<img loading=lazy src=https:////upload-images.jianshu.io/upload_images/1234352-798b134107b6593d.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/105/format/webp alt=img></p><p>根据梯度下降的计算公式</p><p><img loading=lazy src=https://upload-images.jianshu.io/upload_images/1234352-f20521a962005299.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/1047/format/webp alt=img></p><p>我们开始进行梯度下降的迭代计算过程：</p><p><img loading=lazy src=https://upload-images.jianshu.io/upload_images/1234352-57538d21dbb34e65.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/281/format/webp alt=img></p><p>image.png</p><p>如图，经过四次的运算，也就是走了四步，基本就抵达了函数的最低点，也就是山底</p><p><img loading=lazy src=https://upload-images.jianshu.io/upload_images/1234352-bb7fa36d116fcadc.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/601/format/webp alt=img></p><h2 id=多变量函数的梯度下降>多变量函数的梯度下降<a hidden class=anchor aria-hidden=true href=#多变量函数的梯度下降>#</a></h2><p>我们假设有一个目标函数</p><p><img loading=lazy src=https:////upload-images.jianshu.io/upload_images/1234352-a56cfde25c688859.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/171/format/webp alt=img></p><p>现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 (0，0)点。但是接下来，我们会从梯度下降算法开始一步步计算到这个最小值！
我们假设初始的起点为：</p><p><img loading=lazy src=https:////upload-images.jianshu.io/upload_images/1234352-8b1b6f1b200fd7b5.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/128/format/webp alt=img></p><p>初始的学习率为：</p><p><img loading=lazy src=https:////upload-images.jianshu.io/upload_images/1234352-ccc1493848871074.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/89/format/webp alt=img></p><p>函数的梯度为：</p><p><img loading=lazy src=https:////upload-images.jianshu.io/upload_images/1234352-3d744d9364a4ba40.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/221/format/webp alt=img></p><p>进行多次迭代：</p><p><img loading=lazy src=https://upload-images.jianshu.io/upload_images/1234352-b21bf64600c4e32f.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/647/format/webp alt=img></p><p>我们发现，已经基本靠近函数的最小值点</p><p><img loading=lazy src=https://upload-images.jianshu.io/upload_images/1234352-becdcdfdefb4eab7.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/590/format/webp alt=img></p><h1 id=梯度下降算法的实现>梯度下降算法的实现<a hidden class=anchor aria-hidden=true href=#梯度下降算法的实现>#</a></h1><p>下面我们将用python实现一个简单的梯度下降算法。场景是一个简单的<a href="https://link.jianshu.com?t=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FLinear_regression"><em>线性回归</em></a>的例子：假设现在我们有一系列的点，如下图所示</p><p><img loading=lazy src=https:////upload-images.jianshu.io/upload_images/1234352-333f16d34874c230.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/716/format/webp alt=img></p><p>我们将用梯度下降法来拟合出这条直线！</p><p>首先，我们需要定义一个代价函数，在此我们选用<a href="https://link.jianshu.com?t=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FLeast_squares"><em>均方误差代价函数</em></a></p><p><img loading=lazy src=https:////upload-images.jianshu.io/upload_images/1234352-4e4000e69f05af7b.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/451/format/webp alt=img></p><p>此公式中</p><ul><li><p>m是数据集中点的个数</p></li><li><p>½是一个常量，这样是为了在求梯度的时候，二次方乘下来就和这里的½抵消了，自然就没有多余的常数系数，方便后续的计算，同时对结果不会有影响</p></li><li><p>y 是数据集中每个点的真实y坐标的值</p></li><li><p>h 是我们的预测函数，根据每一个输入x，根据Θ 计算得到预测的y值，即</p></li><li><p><img loading=lazy src=https://upload-images.jianshu.io/upload_images/1234352-acea37db1e02004d.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/328/format/webp alt=img></p></li></ul><p>我们可以根据代价函数看到，代价函数中的变量有两个，所以是一个多变量的梯度下降问题，求解出代价函数的梯度，也就是分别对两个变量进行微分</p><p><img loading=lazy src=https:////upload-images.jianshu.io/upload_images/1234352-bfd1c5136eaaa552.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/485/format/webp alt=img></p><p>明确了代价函数和梯度，以及预测的函数形式。我们就可以开始编写代码了。但在这之前，需要说明一点，就是为了方便代码的编写，我们会将所有的公式都转换为矩阵的形式，python中计算矩阵是非常方便的，同时代码也会变得非常的简洁。</p><p>为了转换为矩阵的计算，我们观察到预测函数的形式</p><p><img loading=lazy src=https:////upload-images.jianshu.io/upload_images/1234352-acea37db1e02004d.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/328/format/webp alt=img></p><p>我们有两个变量，为了对这个公式进行矩阵化，我们可以给每一个点x增加一维，这一维的值固定为1，这一维将会乘到Θ0上。这样就方便我们统一矩阵化的计算</p><p><img loading=lazy src=https:////upload-images.jianshu.io/upload_images/1234352-a54d53411f945d95.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/600/format/webp alt=img></p><p>然后我们将代价函数和梯度转化为矩阵向量相乘的形式</p><p><img loading=lazy src=https:////upload-images.jianshu.io/upload_images/1234352-66b04086dd1f8ba9.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/516/format/webp alt=img></p><h2 id=coding-time>coding time<a hidden class=anchor aria-hidden=true href=#coding-time>#</a></h2><p>首先，我们需要定义数据集和学习率</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Size of the points dataset.</span>
</span></span><span class=line><span class=cl><span class=n>m</span> <span class=o>=</span> <span class=mi>20</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Points x-coordinate and dummy value (x0, x1).</span>
</span></span><span class=line><span class=cl><span class=n>X0</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=n>m</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>X1</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>m</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>m</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>hstack</span><span class=p>((</span><span class=n>X0</span><span class=p>,</span> <span class=n>X1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Points y-coordinate</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>7</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>11</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>12</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=mi>11</span><span class=p>,</span> <span class=mi>13</span><span class=p>,</span> <span class=mi>13</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>17</span><span class=p>,</span> <span class=mi>18</span><span class=p>,</span> <span class=mi>17</span><span class=p>,</span> <span class=mi>19</span><span class=p>,</span> <span class=mi>21</span>
</span></span><span class=line><span class=cl><span class=p>])</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>m</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># The Learning Rate alpha.</span>
</span></span><span class=line><span class=cl><span class=n>alpha</span> <span class=o>=</span> <span class=mf>0.01</span>
</span></span></code></pre></td></tr></table></div></div><p>接下来我们以矩阵向量的形式定义代价函数和代价函数的梯度</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>error_function</span><span class=p>(</span><span class=n>theta</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;Error function J definition.&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>diff</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>theta</span><span class=p>)</span> <span class=o>-</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=mf>1.</span><span class=o>/</span><span class=mi>2</span><span class=o>*</span><span class=n>m</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>diff</span><span class=p>),</span> <span class=n>diff</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>gradient_function</span><span class=p>(</span><span class=n>theta</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;Gradient of the function J definition.&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>diff</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>theta</span><span class=p>)</span> <span class=o>-</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=mf>1.</span><span class=o>/</span><span class=n>m</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>X</span><span class=p>),</span> <span class=n>diff</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>最后就是算法的核心部分，梯度下降迭代计算</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>gradient_descent</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>alpha</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;Perform gradient descent.&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>theta</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>gradient</span> <span class=o>=</span> <span class=n>gradient_function</span><span class=p>(</span><span class=n>theta</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=ow>not</span> <span class=n>np</span><span class=o>.</span><span class=n>all</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>absolute</span><span class=p>(</span><span class=n>gradient</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mf>1e-5</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>theta</span> <span class=o>=</span> <span class=n>theta</span> <span class=o>-</span> <span class=n>alpha</span> <span class=o>*</span> <span class=n>gradient</span>
</span></span><span class=line><span class=cl>        <span class=n>gradient</span> <span class=o>=</span> <span class=n>gradient_function</span><span class=p>(</span><span class=n>theta</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>theta</span>
</span></span></code></pre></td></tr></table></div></div><p>当梯度小于1e-5时，说明已经进入了比较平滑的状态，类似于山谷的状态，这时候再继续迭代效果也不大了，所以这个时候可以退出循环！</p><p>完整的代码如下</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Size of the points dataset.</span>
</span></span><span class=line><span class=cl><span class=n>m</span> <span class=o>=</span> <span class=mi>20</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Points x-coordinate and dummy value (x0, x1).</span>
</span></span><span class=line><span class=cl><span class=n>X0</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=n>m</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>X1</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>m</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>m</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>hstack</span><span class=p>((</span><span class=n>X0</span><span class=p>,</span> <span class=n>X1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Points y-coordinate</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>7</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>11</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>12</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=mi>11</span><span class=p>,</span> <span class=mi>13</span><span class=p>,</span> <span class=mi>13</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>17</span><span class=p>,</span> <span class=mi>18</span><span class=p>,</span> <span class=mi>17</span><span class=p>,</span> <span class=mi>19</span><span class=p>,</span> <span class=mi>21</span>
</span></span><span class=line><span class=cl><span class=p>])</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>m</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># The Learning Rate alpha.</span>
</span></span><span class=line><span class=cl><span class=n>alpha</span> <span class=o>=</span> <span class=mf>0.01</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>error_function</span><span class=p>(</span><span class=n>theta</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;Error function J definition.&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>diff</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>theta</span><span class=p>)</span> <span class=o>-</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=mf>1.</span><span class=o>/</span><span class=mi>2</span><span class=o>*</span><span class=n>m</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>diff</span><span class=p>),</span> <span class=n>diff</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>gradient_function</span><span class=p>(</span><span class=n>theta</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;Gradient of the function J definition.&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>diff</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>theta</span><span class=p>)</span> <span class=o>-</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=mf>1.</span><span class=o>/</span><span class=n>m</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>X</span><span class=p>),</span> <span class=n>diff</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>gradient_descent</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>alpha</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;Perform gradient descent.&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>theta</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>gradient</span> <span class=o>=</span> <span class=n>gradient_function</span><span class=p>(</span><span class=n>theta</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=ow>not</span> <span class=n>np</span><span class=o>.</span><span class=n>all</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>absolute</span><span class=p>(</span><span class=n>gradient</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=mf>1e-5</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>theta</span> <span class=o>=</span> <span class=n>theta</span> <span class=o>-</span> <span class=n>alpha</span> <span class=o>*</span> <span class=n>gradient</span>
</span></span><span class=line><span class=cl>        <span class=n>gradient</span> <span class=o>=</span> <span class=n>gradient_function</span><span class=p>(</span><span class=n>theta</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>theta</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>optimal</span> <span class=o>=</span> <span class=n>gradient_descent</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>alpha</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;optimal:&#39;</span><span class=p>,</span> <span class=n>optimal</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;error function:&#39;</span><span class=p>,</span> <span class=n>error_function</span><span class=p>(</span><span class=n>optimal</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p>运行代码，计算得到的结果如下</p><p><img loading=lazy src=https:////upload-images.jianshu.io/upload_images/1234352-af64f7e8e5fb3dfb.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/416/format/webp alt=img></p><p>所拟合出的直线如下</p><p><img loading=lazy src=https:////upload-images.jianshu.io/upload_images/1234352-27806efbd53ced41.png?imageMogr2/auto-orient/strip%7cimageView2/2/w/694/format/webp alt=img></p><h1 id=小结>小结<a hidden class=anchor aria-hidden=true href=#小结>#</a></h1><p>至此，我们就基本介绍完了梯度下降法的基本思想和算法流程，并且用python实现了一个简单的梯度下降算法拟合直线的案例！
最后，我们回到文章开头所提出的场景假设:
<strong>这个下山的人实际上就代表了<a href="https://link.jianshu.com?t=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FBackpropagation">反向传播算法</a>，下山的路径其实就代表着算法中一直在寻找的参数Θ，山上当前点的最陡峭的方向实际上就是代价函数在这一点的梯度方向，场景中观测最陡峭方向所用的工具就是<a href="https://link.jianshu.com?t=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FDerivative">微分</a> 。在下一次观测之前的时间就是有我们算法中的学习率α所定义的。</strong>
可以看到场景假设和梯度下降算法很好的完成了对应！</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://reid00.github.io/en/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/>梯度下降</a></li></ul><nav class=paginav><a class=prev href=https://reid00.github.io/en/posts/ml/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/><span class=title>« Prev</span><br><span>逻辑回归</span>
</a><a class=next href=https://reid00.github.io/en/posts/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/><span class=title>Next »</span><br><span>特征工程之数据预处理</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main></body></html>