<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>集成学习之Bagging,Boosting | Reid's Blog</title>
<meta name=keywords content="集成学习,Bagging,Boosting"><meta name=description content="集成学习之Bagging,Boosting"><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/en/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Bbaggingboosting/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://reid00.github.io/en/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Bbaggingboosting/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><head><meta name=referrer content="no-referrer"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script><script>var doNotTrack=!1,dnt;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK")}</script><meta property="og:title" content="集成学习之Bagging,Boosting"><meta property="og:description" content="集成学习之Bagging,Boosting"><meta property="og:type" content="article"><meta property="og:url" content="https://reid00.github.io/en/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Bbaggingboosting/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-16T19:35:27+08:00"><meta property="article:modified_time" content="2023-03-16T19:35:27+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="集成学习之Bagging,Boosting"><meta name=twitter:description content="集成学习之Bagging,Boosting"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://reid00.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"机器学习，深度学习，知识图谱相关","item":"https://reid00.github.io/en/posts/ml/"},{"@type":"ListItem","position":3,"name":"集成学习之Bagging,Boosting","item":"https://reid00.github.io/en/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Bbaggingboosting/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"集成学习之Bagging,Boosting","name":"集成学习之Bagging,Boosting","description":"集成学习之Bagging,Boosting","keywords":["集成学习","Bagging","Boosting"],"articleBody":"生成子模型的两种取样方式 那么为了造成子模型之间的差距，每个子模型只看样本中的一部分，这就涉及到两种取样方式：\n放回取样：Bagging，在统计学中也被称为bootstrap。 不放回取样：Boosting 在集成学习中我们通常采用 Bagging 的方式，具体原因如下：\n因为取样后放回，所以不受样本数据量的限制，允许对同一种分类器上对训练集进行进行多次采样，可以训练更多的子模型。 在 train_test_split 时，不那么强烈的依赖随机；而 Boosting的方式，会受到随机的影响； Boosting的随机问题：Pasting 的方式等同于将 500 个样本分成 5 份，每份 100 个样本，怎么分，将对子模型有较大影响，进而对集成系统的准确率有较大影响。 什么是Bagging Bagging，即bootstrap aggregating的缩写，每个训练集称为bootstrap。\nBagging是一种根据均匀概率分布从数据中重复抽样（有放回）的技术 。\nBagging能提升机器学习算法的稳定性和准确性，它可以减少模型的方差从而避免overfitting。它通常应用在决策树方法中，其实它可以应用到任何其它机器学习算法中。\nBagging方法在不稳定模型（unstable models）集合中表现比较好。这里说的不稳定的模型，即在训练数据发生微小变化时产生不同泛化行为的模型（高方差模型），如决策树和神经网络。\n但是Bagging在过于简单模型集合中表现并不好，因为Bagging是从总体数据集随机选取样本来训练模型，过于简单的模型可能会产生相同的预测结果，失去了多样性。\n总结一下Bagging方法：\nBagging通过降低基分类器的方差，改善了泛化误差 其性能依赖于基分类器的稳定性；如果基分类器不稳定，bagging有助于降低训练数据的随机波动导致的误差；如果稳定，则集成分类器的误差主要由基分类器的偏差引起 由于每个样本被选中的概率相同，因此bagging并不侧重于训练数据集中的任何特定实例 Bagging的使用 sklearn为Bagging提供了一个简单的API：BaggingClassifier类（回归是BaggingRegressor）。首先需要传入一个模型作为参数，可以使用决策树；然后需要传入参数n_estimator即集成多少个子模型；参数max_samples表示每次从数据集中取多少样本；参数bootstrap设置为True表示使用有放回取样Bagging，设置为False表示使用无放回取样Pasting。可以通过n_jobs参数来分配训练所需CPU核的数量，-1表示会使用所有空闲核（集成学习思路，极易并行化处理）。\nbagging是不能减小模型的偏差的，因此我们要选择具有低偏差的分类器来集成，例如：没有修剪的决策树。\nBootstrap 在每个预测器被训练的子集中引入了更多的分集，所以 Bagging 结束时的偏差比 Pasting 更高，但这也意味着预测因子最终变得不相关，从而减少了集合的方差。总体而言，Bagging 通常会导致更好的模型，这就解释了为什么它通常是首选的。然而，如果你有空闲时间和 CPU 功率，可以使用交叉验证来评估 Bagging 和 Pasting 哪一个更好。\nOut-of-Bag 对于Bagging来说，一些实例可能被一些分类器重复采样，但其他的有可能不会被采样。由于每个bootstrap的M个样本是有放回随机选取的，因此每个样本不被选中的概率为。当N和M都非常大时，比如N=M=10000，一个样本不被选中的概率p = 36.8%。因此一个bootstrap约包含原样本63.2%，约36.8%的样本未被选中。这些没有被采样的训练实例就叫做Out-of-Bag实例。但注意对于每一个的分类器来说，它们各自的未选中部分不是相同的。\n那么这些未选中的样本有什么用呢？\n因为在训练中分类器从来没有看到过Out-of-Bag实例，所以它可以在这些样本上进行预测，就不用分样本测试集和测试数据集了。\n在sklearn中，可以在训练后需要创建一个BaggingClassifier时设置oob_score=True来进行自动评估。\n1 2 3 4 5 bagging_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=5000, max_samples=100, bootstrap=True, oob_score=True) bagging_clf.fit(X, y) bagging_clf.oob_score_ 另一种差异化方式：针对特征取样 我们上面提到的，都是通过对样本进行取样，来得到差异化的子模型。除了取部分样本以外，还可以针对特征进行随机取样。尤其是在样本特征非常多的情况下时，如图像领域中，每个像素点都是一个特征，则Bagging时可以对特征进行取样。\n在BaggingClassifier中有两个超参数，max_features表示每次取多少特征；bootstrap_features设置为True则开启。\n在Bagging中，所有的分类器都可以是并行训练的。与之相对应的，串行训练的Boosting也是集成训练中的一大类别。接下来我们会介绍Boosting算法。\nBoosting思想 Boosting是增强、推动的意思。它是一种迭代的方法，各个子模型彼此之间不是独立的关系，而是相互增强（Boosting）的关系。每一次训练的时候都更加关心分类错误的样例，给这些分类错误的样例增加更大的权重，下一次迭代的目标就是能够更容易辨别出上一轮分类错误的样例。每个模型都在尝试增强整体的效果，最终将这些弱分类器进行加权相加。\n因此与Bagging中各学习器并行处理不同的是：Boosting是串行的，环环相扣且有先后顺序的\nBoosting工作机制 Boosting的工作流程:\n现有原始数据集，首先挑出一些数据，然后在上训练分类器得到。然后用在原始数据集上测试一下，看哪些样本分类对了，哪些样本分类错了。然后把分错的和分对的分别挑出一部分，组成新的数据集（也就是说，刻意筛出有对有错的数据集）。再使用某分类器训练数据集，即专门地、有目的性地学习D1数据集哪些学对了、哪些学错了，得到C2.\n有了C1, C2 两个分类器都在原始数据集D上进行测试，目的是找到C1、C2结果不一致的样本，组成一个新的数据集D3，再用一个分类器训练D3，得到（专门用来解决C1、C2的争端）。\n其算法流程为：\n先从初始训练集训练出一个弱学习器； 再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注； 然后基于调整后的样本分布来训练下一个基学习器； 如此重复进行，直至基学习器数目达到事先指定的值，最终将这个基学习器进行加权结合。使后续模型应该能够补偿早期模型所造成的错误。 先训练一个分类器，根据这个分类器的误差将训练样本重新调整，也就是加权重。原来的每个样本有同样的机会去作为训练样本，在某个样本上犯的错误越多，其权重也就越大。这很好理解，就是让后面的分类器去重点学习前面分错的样本。\nBoosting和Bagging的区别（重要总结） 1、样本选择上：\nBagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。 Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。 2、样例权重：\nBagging：使用均匀取样，每个样例的权重相等。 Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。 3、分类器权重：\nBagging：所有分类器的权重相等。 Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。 4、并行\u0026串行：\nBagging：各个预测函数可以并行生成。 Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。 偏差和方差（重要！） 偏差（bias）衡量了模型的预测值与实际值之间的偏离关系，反映了模型本身的拟合能力；方差（variance）描述的是训练数据在不同迭代阶段的训练模型中，预测值的变化波动情况（或称之为离散情况）。\nBagging用多个分类器进行并行训练的目的就是降低方差。因为相互独立的分类器多了，就会让目标值更聚合。\n而对于Boosting来说，每一轮都针对于上一轮进行学习，力求准确，即可以降低偏差（残差）。\n因此，在实际使用时，我们就要考虑模型的特性。对于方差低的Bagging（随机森林）来说，采用深度较深且不剪枝的决策树，以此降低其偏差。对于偏差低的Boosting（GBDT）要选简单的、深度浅的决策树。\n在Bagging方法中各个基体学习器之间不存在依赖关系，集成多个模型，综合有差异的子模型，融合出比较好的模型，且可以并行处理。如随机森林算法（RF）等。\n而Boosting方法必须串行生成，各个基学习器存在依赖关系,基于前面模型的训练结果误差生成新的模型，代表的算法有：Adaboost、GBDT、XGBoost等。\n","wordCount":"110","inLanguage":"en","datePublished":"2023-03-16T19:35:27+08:00","dateModified":"2023-03-16T19:35:27+08:00","author":[{"@type":"Person","name":"Reid"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://reid00.github.io/en/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Bbaggingboosting/"},"publisher":{"@type":"Organization","name":"Reid's Blog","logo":{"@type":"ImageObject","url":"https://reid00.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://reid00.github.io/en/ accesskey=h title="Reid's Blog (Alt + H)">Reid's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://reid00.github.io/en/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://reid00.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://reid00.github.io/en/categories/ title=Categorys><span>Categorys</span></a></li><li><a href=https://reid00.github.io/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://reid00.github.io/en/>Home</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/ml/>机器学习，深度学习，知识图谱相关</a></div><h1 class=post-title>集成学习之Bagging,Boosting</h1><div class=post-description>集成学习之Bagging,Boosting</div><div class=post-meta></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><ul><li><a href=#%e7%94%9f%e6%88%90%e5%ad%90%e6%a8%a1%e5%9e%8b%e7%9a%84%e4%b8%a4%e7%a7%8d%e5%8f%96%e6%a0%b7%e6%96%b9%e5%bc%8f aria-label=生成子模型的两种取样方式>生成子模型的两种取样方式</a></li><li><a href=#%e4%bb%80%e4%b9%88%e6%98%afbagging aria-label=什么是Bagging>什么是Bagging</a></li><li><a href=#bagging%e7%9a%84%e4%bd%bf%e7%94%a8 aria-label=Bagging的使用>Bagging的使用</a></li><li><a href=#out-of-bag aria-label=Out-of-Bag>Out-of-Bag</a></li><li><a href=#%e5%8f%a6%e4%b8%80%e7%a7%8d%e5%b7%ae%e5%bc%82%e5%8c%96%e6%96%b9%e5%bc%8f%e9%92%88%e5%af%b9%e7%89%b9%e5%be%81%e5%8f%96%e6%a0%b7 aria-label=另一种差异化方式：针对特征取样>另一种差异化方式：针对特征取样</a></li><li><a href=#boosting%e6%80%9d%e6%83%b3 aria-label=Boosting思想>Boosting思想</a></li><li><a href=#boosting%e5%b7%a5%e4%bd%9c%e6%9c%ba%e5%88%b6 aria-label=Boosting工作机制>Boosting工作机制</a></li></ul><li><a href=#boosting%e5%92%8cbagging%e7%9a%84%e5%8c%ba%e5%88%ab%e9%87%8d%e8%a6%81%e6%80%bb%e7%bb%93 aria-label=Boosting和Bagging的区别（重要总结）>Boosting和Bagging的区别（重要总结）</a><ul><li><a href=#%e5%81%8f%e5%b7%ae%e5%92%8c%e6%96%b9%e5%b7%ae%e9%87%8d%e8%a6%81 aria-label=偏差和方差（重要！）><strong>偏差和方差（重要！）</strong></a></li></ul></li></ul></div></details></div><div class=post-content><h4 id=生成子模型的两种取样方式>生成子模型的两种取样方式<a hidden class=anchor aria-hidden=true href=#生成子模型的两种取样方式>#</a></h4><p>那么为了造成子模型之间的差距，每个子模型只看样本中的一部分，这就涉及到两种取样方式：</p><ul><li>放回取样：Bagging，在统计学中也被称为bootstrap。</li><li>不放回取样：Boosting</li></ul><p>在集成学习中我们通常采用 Bagging 的方式，具体原因如下：</p><ul><li>因为取样后放回，所以不受样本数据量的限制，允许对同一种分类器上对训练集进行进行多次采样，可以训练更多的子模型。</li><li>在 train_test_split 时，不那么强烈的依赖随机；而 Boosting的方式，会受到随机的影响；</li><li>Boosting的随机问题：Pasting 的方式等同于将 500 个样本分成 5 份，每份 100 个样本，怎么分，将对子模型有较大影响，进而对集成系统的准确率有较大影响。</li></ul><h4 id=什么是bagging>什么是Bagging<a hidden class=anchor aria-hidden=true href=#什么是bagging>#</a></h4><p>Bagging，即<code>bootstrap aggregating</code>的缩写，每个训练集称为<code>bootstrap</code>。</p><p><strong>Bagging是一种根据均匀概率分布从数据中重复抽样（有放回）的技术</strong> 。</p><p>Bagging能提升机器学习算法的稳定性和准确性，它可以减少模型的方差从而避免overfitting。它通常应用在决策树方法中，其实它可以应用到任何其它机器学习算法中。</p><p>Bagging方法在不稳定模型（unstable models）集合中表现比较好。这里说的不稳定的模型，即在训练数据发生微小变化时产生不同泛化行为的模型（高方差模型），如决策树和神经网络。</p><p>但是Bagging在过于简单模型集合中表现并不好，因为Bagging是从总体数据集随机选取样本来训练模型，过于简单的模型可能会产生相同的预测结果，失去了多样性。</p><p>总结一下Bagging方法：</p><ol><li>Bagging通过降低基分类器的方差，改善了泛化误差</li><li>其性能依赖于基分类器的稳定性；如果基分类器不稳定，bagging有助于降低训练数据的随机波动导致的误差；如果稳定，则集成分类器的误差主要由基分类器的偏差引起</li><li>由于每个样本被选中的概率相同，因此bagging并不侧重于训练数据集中的任何特定实例</li></ol><h4 id=bagging的使用>Bagging的使用<a hidden class=anchor aria-hidden=true href=#bagging的使用>#</a></h4><p>sklearn为Bagging提供了一个简单的API：BaggingClassifier类（回归是BaggingRegressor）。首先需要传入一个模型作为参数，可以使用决策树；然后需要传入参数<code>n_estimator</code>即集成多少个子模型；参数<code>max_samples</code>表示每次从数据集中取多少样本；参数<code>bootstrap</code>设置为True表示使用有放回取样Bagging，设置为False表示使用无放回取样Pasting。可以通过<code>n_jobs</code>参数来分配训练所需CPU核的数量，-1表示会使用所有空闲核（集成学习思路，极易并行化处理）。</p><p>bagging是不能减小模型的偏差的，因此我们要选择具有低偏差的分类器来集成，例如：没有修剪的决策树。</p><p>Bootstrap 在每个预测器被训练的子集中引入了更多的分集，所以 Bagging 结束时的偏差比 Pasting 更高，但这也意味着预测因子最终变得不相关，从而减少了集合的方差。总体而言，Bagging 通常会导致更好的模型，这就解释了为什么它通常是首选的。然而，如果你有空闲时间和 CPU 功率，可以使用交叉验证来评估 Bagging 和 Pasting 哪一个更好。</p><h4 id=out-of-bag>Out-of-Bag<a hidden class=anchor aria-hidden=true href=#out-of-bag>#</a></h4><p>对于Bagging来说，一些实例可能被一些分类器重复采样，但其他的有可能不会被采样。由于每个bootstrap的M个样本是有放回随机选取的，因此每个样本不被选中的概率为。当N和M都非常大时，比如N=M=10000，一个样本不被选中的概率p = 36.8%。因此一个bootstrap约包含原样本63.2%，约36.8%的样本未被选中。这些没有被采样的训练实例就叫做Out-of-Bag实例。但注意对于每一个的分类器来说，它们各自的未选中部分不是相同的。</p><p>那么这些未选中的样本有什么用呢？</p><p>因为在训练中分类器从来没有看到过Out-of-Bag实例，所以它可以在这些样本上进行预测，就不用分样本测试集和测试数据集了。</p><p>在sklearn中，可以在训练后需要创建一个<code>BaggingClassifier</code>时设置<code>oob_score=True</code>来进行自动评估。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>bagging_clf</span> <span class=o>=</span> <span class=n>BaggingClassifier</span><span class=p>(</span><span class=n>DecisionTreeClassifier</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                               <span class=n>n_estimators</span><span class=o>=</span><span class=mi>5000</span><span class=p>,</span> <span class=n>max_samples</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                               <span class=n>bootstrap</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>oob_score</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>bagging_clf</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>bagging_clf</span><span class=o>.</span><span class=n>oob_score_</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=另一种差异化方式针对特征取样>另一种差异化方式：针对特征取样<a hidden class=anchor aria-hidden=true href=#另一种差异化方式针对特征取样>#</a></h4><p>我们上面提到的，都是通过对样本进行取样，来得到差异化的子模型。除了取部分样本以外，还可以针对特征进行随机取样。尤其是在样本特征非常多的情况下时，如图像领域中，每个像素点都是一个特征，则Bagging时可以对特征进行取样。</p><p>在BaggingClassifier中有两个超参数，<code>max_features</code>表示每次取多少特征；<code>bootstrap_features</code>设置为True则开启。</p><p>在Bagging中，所有的分类器都可以是并行训练的。与之相对应的，串行训练的<code>Boosting</code>也是集成训练中的一大类别。接下来我们会介绍<code>Boosting</code>算法。</p><h4 id=boosting思想>Boosting思想<a hidden class=anchor aria-hidden=true href=#boosting思想>#</a></h4><p>Boosting是增强、推动的意思。它是一种迭代的方法，各个子模型彼此之间不是独立的关系，而是相互增强（Boosting）的关系。每一次训练的时候都更加关心分类错误的样例，给这些分类错误的样例增加更大的权重，下一次迭代的目标就是能够更容易辨别出上一轮分类错误的样例。每个模型都在尝试增强整体的效果，最终将这些弱分类器进行加权相加。</p><p><strong>因此与Bagging中各学习器并行处理不同的是：Boosting是串行的，环环相扣且有先后顺序的</strong></p><h4 id=boosting工作机制>Boosting工作机制<a hidden class=anchor aria-hidden=true href=#boosting工作机制>#</a></h4><p>Boosting的工作流程:</p><p>现有原始数据集，首先挑出一些数据，然后在上训练分类器得到。然后用在原始数据集上测试一下，看哪些样本分类对了，哪些样本分类错了。然后<strong>把分错的和分对的分别挑出一部分</strong>，组成新的数据集（也就是说，<strong>刻意筛出有对有错的数据集</strong>）。再使用某分类器训练数据集，<strong>即专门地、有目的性地学习D1数据集哪些学对了、哪些学错了</strong>，得到C2.</p><p>有了C1, C2 两个分类器都在原始数据集D上进行测试，目的是找到C1、C2<strong>结果不一致的样本</strong>，组成一个新的数据集D3，再用一个分类器训练D3，得到（<strong>专门用来解决C1、C2的争端</strong>）。</p><p>其算法流程为：</p><ol><li>先从初始训练集训练出一个弱学习器；</li><li>再<strong>根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注</strong>；</li><li>然后基于调整后的<strong>样本分布</strong>来训练下一个基学习器；</li><li>如此重复进行，直至基学习器数目达到事先指定的值，最终将这个基学习器进行<strong>加权结合</strong>。使后续模型应该能够补偿早期模型所造成的错误。</li></ol><p>先训练一个分类器，根据这个分类器的误差将训练样本重新调整，也就是加权重。原来的每个样本有同样的机会去作为训练样本，在某个样本上犯的错误越多，其权重也就越大。这很好理解，就是让后面的分类器去重点学习前面分错的样本。</p><h3 id=boosting和bagging的区别重要总结>Boosting和Bagging的区别（重要总结）<a hidden class=anchor aria-hidden=true href=#boosting和bagging的区别重要总结>#</a></h3><p>1、样本选择上：</p><ul><li>Bagging：训练集是在原始集中<strong>有放回</strong>选取的，从原始集中选出的<strong>各轮训练集之间是独立</strong>的。</li><li>Boosting：每一轮的<strong>训练集不变</strong>，只是训练集中<strong>每个样例在分类器中的权重</strong>发生变化。而权值是根据上一轮的分类结果进行调整。</li></ul><p>2、样例权重：</p><ul><li>Bagging：使用均匀取样，每个样例的<strong>权重相等</strong>。</li><li>Boosting：根据错误率不断调整样例的权值，<strong>错误率越大则权重越大</strong>。</li></ul><p>3、分类器权重：</p><ul><li>Bagging：所有<strong>分类器的权重相等</strong>。</li><li>Boosting：每个弱分类器都有相应的权重，对于<strong>分类误差小的分类器会有更大的权重</strong>。</li></ul><p>4、并行&串行：</p><ul><li>Bagging：各个预测函数可以<strong>并行</strong>生成。</li><li>Boosting：各个预测函数只能<strong>顺序生成</strong>，因为后一个模型参数需要前一轮模型的结果。</li></ul><h4 id=偏差和方差重要><strong>偏差和方差（重要！）</strong><a hidden class=anchor aria-hidden=true href=#偏差和方差重要>#</a></h4><p>偏差（bias）衡量了模型的<strong>预测值与实际值之间的偏离关系</strong>，反映了模型本身的拟合能力；方差（variance）描述的是训练数据在不同迭代阶段的训练模型中，<strong>预测值的变化波动情况（或称之为离散情况）</strong>。</p><p>Bagging用多个分类器进行并行训练的目的就是<strong>降低方差</strong>。因为相互独立的分类器多了，就会让目标值更聚合。</p><p>而对于Boosting来说，每一轮都针对于上一轮进行学习，力求准确，即可以<strong>降低偏差（残差）</strong>。</p><p>因此，<strong>在实际使用时，我们就要考虑模型的特性。对于方差低的Bagging（随机森林）来说，采用深度较深且不剪枝的决策树，以此降低其偏差。对于偏差低的Boosting（GBDT）要选简单的、深度浅的决策树。</strong></p><p>在Bagging方法中各个基体学习器之间不存在依赖关系，集成多个模型，综合有差异的子模型，融合出比较好的模型，且可以并行处理。如随机森林算法（RF）等。</p><p>而Boosting方法必须串行生成，各个基学习器<strong>存在依赖关系,基于前面模型的训练结果误差生成新的模型</strong>，代表的算法有：Adaboost、GBDT、XGBoost等。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://reid00.github.io/en/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>集成学习</a></li><li><a href=https://reid00.github.io/en/tags/bagging/>Bagging</a></li><li><a href=https://reid00.github.io/en/tags/boosting/>Boosting</a></li></ul><nav class=paginav><a class=prev href=https://reid00.github.io/en/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Badaboost/><span class=title>« Prev</span><br><span>集成学习之AdaBoost</span>
</a><a class=next href=https://reid00.github.io/en/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Bgbdt/><span class=title>Next »</span><br><span>集成学习之GBD</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://reid00.github.io/en/>Reid's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>