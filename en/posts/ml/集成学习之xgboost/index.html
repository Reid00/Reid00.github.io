<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>集成学习之xgboost | Reid's Blog</title>
<meta name=keywords content="xgboost,集成学习"><meta name=description content="集成学习之xgboost"><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/en/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Bxgboost/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://reid00.github.io/en/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Bxgboost/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><head><meta name=referrer content="no-referrer"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script><script>var doNotTrack=!1,dnt;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK")}</script><meta property="og:title" content="集成学习之xgboost"><meta property="og:description" content="集成学习之xgboost"><meta property="og:type" content="article"><meta property="og:url" content="https://reid00.github.io/en/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Bxgboost/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-16T19:35:28+08:00"><meta property="article:modified_time" content="2023-03-16T19:35:28+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="集成学习之xgboost"><meta name=twitter:description content="集成学习之xgboost"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://reid00.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"机器学习，深度学习，知识图谱相关","item":"https://reid00.github.io/en/posts/ml/"},{"@type":"ListItem","position":3,"name":"集成学习之xgboost","item":"https://reid00.github.io/en/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Bxgboost/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"集成学习之xgboost","name":"集成学习之xgboost","description":"集成学习之xgboost","keywords":["xgboost","集成学习"],"articleBody":"一、XGBoost和GBDT xgboost是一种集成学习算法，属于3类常用的集成方法(bagging,boosting,stacking)中的boosting算法类别。它是一个加法模型，基模型一般选择树模型，但也可以选择其它类型的模型如逻辑回归等。\nxgboost属于梯度提升树(GBDT)模型这个范畴，GBDT的基本想法是让新的基模型（GBDT以CART分类回归树为基模型）去拟合前面模型的偏差，从而不断将加法模型的偏差降低。\n相比于经典的GBDT，xgboost做了一些改进，从而在效果和性能上有明显的提升（划重点面试常考）。\n第一，GBDT将目标函数泰勒展开到一阶，而xgboost将目标函数泰勒展开到了二阶。保留了更多有关目标函数的信息，对提升效果有帮助。\n第二，GBDT是给新的基模型寻找新的拟合标签（前面加法模型的负梯度），而xgboost是给新的基模型寻找新的目标函数（目标函数关于新的基模型的二阶泰勒展开）。\n第三，xgboost加入了和叶子权重的L2正则化项，因而有利于模型获得更低的方差。\n**第四，xgboost增加了自动处理缺失值特征的策略。**通过把带缺失值样本分别划分到左子树或者右子树，比较两种方案下目标函数的优劣，从而自动对有缺失值的样本进行划分，无需对缺失特征进行填充预处理。\n此外，xgboost还支持候选分位点切割，特征并行等，可以提升性能。\n二、XGBoost原理概述 面从假设空间，目标函数，优化算法3个角度对xgboost的原理进行概括性的介绍。\n1，假设空间\n2，目标函数\n3，优化算法\n基本思想：贪心法，逐棵树进行学习，每棵树拟合之前模型的偏差。\n三、第t棵树学什么？ 要完成构建xgboost模型，我们需要确定以下一些事情。\n1，如何boost? 如果已经得到了前面t-1棵树构成的加法模型，如何确定第t棵树的学习目标？\n2，如何生成树？已知第t棵树的学习目标的前提下，如何学习这棵树？具体又包括是否进行分裂？选择哪个特征进行分裂？选择什么分裂点位？分裂的叶子节点如何取值？\n我们首先考虑如何boost的问题，顺便解决分裂的叶子节点如何取值的问题。\n四、如何生成第t棵树？ xgboost采用二叉树，开始的时候，全部样本都在一个叶子节点上。然后叶子节点不断通过二分裂，逐渐生成一棵树。\nxgboost使用levelwise的生成策略，即每次对同一层级的全部叶子节点尝试进行分裂。\n对叶子节点分裂生成树的过程有几个基本的问题：是否要进行分裂？选择哪个特征进行分裂？在特征的什么点位进行分裂？以及分裂后新的叶子上取什么值？\n叶子节点的取值问题前面已经解决了。我们重点讨论几个剩下的问题。\n1，是否要进行分裂？ 根据树的剪枝策略的不同，这个问题有两种不同的处理。如果是预剪枝策略，那么只有当存在某种分裂方式使得分裂后目标函数发生下降，才会进行分裂。\n但如果是后剪枝策略，则会无条件进行分裂，等树生成完成后，再从上而下检查树的各个分枝是否对目标函数下降产生正向贡献从而进行剪枝。\nxgboost采用预剪枝策略，只有分裂后的增益大于0才会进行分裂。\n2，选择什么特征进行分裂？\nxgboost采用特征并行的方法进行计算选择要分裂的特征，即用多个线程，尝试把各个特征都作为分裂的特征，找到各个特征的最优分割点，计算根据它们分裂后产生的增益，选择增益最大的那个特征作为分裂的特征。\n3，选择什么分裂点位？\nxgboost选择某个特征的分裂点位的方法有两种，一种是全局扫描法，另一种是候选分位点法。 全局扫描法将所有样本该特征的取值按从小到大排列，将所有可能的分裂位置都试一遍，找到其中增益最大的那个分裂点，其计算复杂度和叶子节点上的样本特征不同的取值个数成正比。 而候选分位点法是一种近似算法，仅选择常数个（如256个）候选分裂位置，然后从候选分裂位置中找出最优的那个。\n五、XGBoost算法原理小结 XGBoost（eXtreme Gradient Boosting）全名叫极端梯度提升，XGBoost是集成学习方法的王牌，在Kaggle数据挖掘比赛中，大部分获胜者用了XGBoost，XGBoost在绝大多数的回归和分类问题上表现的十分顶尖，本文较详细的介绍了XGBoost的算法原理。\n目录\n最优模型的构建方法\nBoosting的回归思想\nXGBoost的目标函数推导\nXGBoost的回归树构建方法\nXGBoost与GDBT的区别\n最优模型的构建方法\n构建最优模型的一般方法是最小化训练数据的损失函数，我们用字母 L表示，如下式：\n式（1）称为经验风险最小化，训练得到的模型复杂度较高。当训练数据较小时，模型很容易出现过拟合问题。\n因此，为了降低模型的复杂度，常采用下式：\n其中J(f)为模型的复杂度，式（2）称为结构风险最小化，结构风险最小化的模型往往对训练数据以及未知的测试数据都有较好的预测 。\n应用：决策树的生成和剪枝分别对应了经验风险最小化和结构风险最小化，XGBoost的决策树生成是结构风险最小化的结果，后续会详细介绍。\nBoosting方法的回归思想\nBoosting法是结合多个弱学习器给出最终的学习结果，不管任务是分类或回归，我们都用回归任务的思想来构建最优Boosting模型 。\n回归思想：把每个弱学习器的输出结果当成连续值，这样做的目的是可以对每个弱学习器的结果进行累加处理，且能更好的利用损失函数来优化模型。\n假设\n是第 t 轮弱学习器的输出结果，\n是模型的输出结果，\n是实际输出结果，表达式如下：\n上面两式就是加法模型，都默认弱学习器的输出结果是连续值。因为回归任务的弱学习器本身是连续值，所以不做讨论，下面详细介绍分类任务的回归思想。\n分类任务的回归思想：\n根据2.1式的结果，得到最终的分类器：\n分类的损失函数一般选择指数函数或对数函数，这里假设损失函数为对数函数，学习器的损失函数是\n若实际输出结果yi=1，则：\n求（2.5）式对\n的梯度，得：\n负梯度方向是损失函数下降最快的方向，（2.6）式取反的值大于0，因此弱学习器是往增大\n的方向迭代的，图形表示为：\n如上图，当样本的实际标记 yi 是 1 时，模型输出结果\n随着迭代次数的增加而增加（红线箭头），模型的损失函数相应的减小；当样本的实际标记 yi 是 -1时，模型输出结果\n随着迭代次数的增加而减小（红线箭头），模型的损失函数相应的减小 。这就是加法模型的原理所在，通过多次的迭代达到减小损失函数的目的。\n小结：Boosting方法把每个弱学习器的输出看成是连续值，使得损失函数是个连续值，因此可以通过弱学习器的迭代达到优化模型的目的，这也是集成学习法加法模型的原理所在 。\nXGBoost算法的目标函数推导\n目标函数，即损失函数，通过最小化损失函数来构建最优模型，由第一节可知， 损失函数应加上表示模型复杂度的正则项，且XGBoost对应的模型包含了多个CART树，因此，模型的目标函数为：\n（3.1）式是正则化的损失函数，等式右边第一部分是模型的训练误差，第二部分是正则化项，这里的正则化项是K棵树的正则化项相加而来的。\nCART树的介绍：\n上图为第K棵CART树，确定一棵CART树需要确定两部分，第一部分就是树的结构，这个结构将输入样本映射到一个确定的叶子节点上，记为\n。第二部分就是各个叶子节点的值，q(x)表示输出的叶子节点序号，\n表示对应叶子节点序号的值。由定义得：\n树的复杂度定义\nXGBoost法对应的模型包含了多棵cart树，定义每棵树的复杂度：\n其中T为叶子节点的个数，||w||为叶子节点向量的模 。γ表示节点切分的难度，λ表示L2正则化系数。\n如下例树的复杂度表示：\n目标函数推导\n根据（3.1）式，共进行t次迭代的学习模型的目标函数为：\n泰勒公式的二阶导近似表示：\n令\n为Δx，则（3.5）式的二阶近似展开：\n其中：\n表示前t-1棵树组成的学习模型的预测误差，gi和hi分别表示预测误差对当前模型的一阶导和二阶导 ，当前模型往预测误差减小的方向进行迭代。\n忽略（3.8）式常数项，并结合（3.4）式，得：\n通过（3.2）式简化（3.9）式：\n（3.10）式第一部分是对所有训练样本集进行累加，因为所有样本都是映射为树的叶子节点，我们换种思维，从叶子节点出发，对所有的叶子节点进行累加，得：\n令\nGj 表示映射为叶子节点 j 的所有输入样本的一阶导之和，同理，Hj表示二阶导之和。\n得：\n对于第 t 棵CART树的某一个确定结构（可用q(x)表示），其叶子节点是相互独立的，Gj和Hj是确定量，因此，（3.12）可以看成是关于叶子节点的一元二次函数 。最小化（3.12）式，得：\n得到最终的目标函数：\n（3.14）也称为打分函数(scoring function)，它是衡量树结构好坏的标准，值越小，代表这样的结构越好 。我们用打分函数选择最佳切分点，从而构建CART树。\nCART回归树的构建方法\n上节推导得到的打分函数是衡量树结构好坏的标准，因此，可用打分函数来选择最佳切分点。首先确定样本特征的所有切分点，对每一个确定的切分点进行切分，切分好坏的标准如下：\nGain表示单节点obj与切分后的两个节点的树obj之差，遍历所有特征的切分点，找到最大Gain的切分点即是最佳分裂点，根据这种方法继续切分节点，得到CART树。若 γ 值设置的过大，则Gain为负，表示不切分该节点，因为切分后的树结构变差了。γ值越大，表示对切分后obj下降幅度要求越严，这个值可以在XGBoost中设定。\nXGBoost与GDBT的区别\nXGBoost生成CART树考虑了树的复杂度，GDBT未考虑，GDBT在树的剪枝步骤中考虑了树的复杂度。\nXGBoost是拟合上一轮损失函数的二阶导展开，GDBT是拟合上一轮损失函数的一阶导展开，因此，XGBoost的准确性更高，且满足相同的训练效果，需要的迭代次数更少。\nXGBoost与GDBT都是逐次迭代来提高模型性能，但是XGBoost在选取最佳切分点时可以开启多线程进行，大大提高了运行速度。\n参考: https://mp.weixin.qq.com/s/cMgd-wBlzjacL21FPK2y7Q https://www.cnblogs.com/pinard/p/10979808.html\n","wordCount":"4009","inLanguage":"en","datePublished":"2023-03-16T19:35:28+08:00","dateModified":"2023-03-16T19:35:28+08:00","author":[{"@type":"Person","name":"Reid"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://reid00.github.io/en/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Bxgboost/"},"publisher":{"@type":"Organization","name":"Reid's Blog","logo":{"@type":"ImageObject","url":"https://reid00.github.io/favicon.ico"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://reid00.github.io/en/ accesskey=h title="Reid's Blog (Alt + H)">Reid's Blog</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://reid00.github.io/en/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://reid00.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://reid00.github.io/en/categories/ title=Categorys><span>Categorys</span></a></li><li><a href=https://reid00.github.io/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://reid00.github.io/en/>Home</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/ml/>机器学习，深度学习，知识图谱相关</a></div><h1 class=post-title>集成学习之xgboost</h1><div class=post-description>集成学习之xgboost</div><div class=post-meta><span title='2023-03-16 19:35:28 +0800 +0800'>2023-03-16 19:35</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;4009 words&nbsp;·&nbsp;Reid</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e4%b8%80xgboost%e5%92%8cgbdt aria-label=一、XGBoost和GBDT><strong>一、XGBoost和GBDT</strong></a></li><li><a href=#%e4%ba%8cxgboost%e5%8e%9f%e7%90%86%e6%a6%82%e8%bf%b0 aria-label=二、XGBoost原理概述>二、XGBoost原理概述</a></li><li><a href=#%e4%b8%89%e7%ac%act%e6%a3%b5%e6%a0%91%e5%ad%a6%e4%bb%80%e4%b9%88 aria-label=三、第t棵树学什么？><strong>三、第t棵树学什么？</strong></a></li><li><a href=#%e5%9b%9b%e5%a6%82%e4%bd%95%e7%94%9f%e6%88%90%e7%ac%act%e6%a3%b5%e6%a0%91 aria-label=四、如何生成第t棵树？><strong>四、如何生成第t棵树？</strong></a></li><li><a href=#%e4%ba%94xgboost%e7%ae%97%e6%b3%95%e5%8e%9f%e7%90%86%e5%b0%8f%e7%bb%93 aria-label=五、XGBoost算法原理小结>五、XGBoost算法原理小结</a></li></ul></div></details></div><div class=post-content><h3 id=一xgboost和gbdt><strong>一、XGBoost和GBDT</strong><a hidden class=anchor aria-hidden=true href=#一xgboost和gbdt>#</a></h3><p>xgboost是一种集成学习算法，属于3类常用的集成方法(bagging,boosting,stacking)中的boosting算法类别。它是一个加法模型，基模型一般选择树模型，但也可以选择其它类型的模型如逻辑回归等。</p><p>xgboost属于梯度提升树(GBDT)模型这个范畴，GBDT的基本想法是让新的基模型（GBDT以CART分类回归树为基模型）去拟合前面模型的偏差，从而不断将加法模型的偏差降低。</p><p>相比于经典的GBDT，xgboost做了一些改进，从而在效果和性能上有明显的提升（<strong>划重点面试常考</strong>）。</p><p><strong>第一，GBDT将目标函数泰勒展开到一阶，而xgboost将目标函数泰勒展开到了二阶</strong>。保留了更多有关目标函数的信息，对提升效果有帮助。</p><p><strong>第二，GBDT是给新的基模型寻找新的拟合标签</strong>（前面加法模型的负梯度），<strong>而xgboost是给新的基模型寻找新的目标函数</strong>（目标函数关于新的基模型的二阶泰勒展开）。</p><p><strong>第三，xgboost加入了和叶子权重的L2正则化项</strong>，因而有利于模型获得更低的方差。</p><p>**第四，xgboost增加了自动处理缺失值特征的策略。**通过把带缺失值样本分别划分到左子树或者右子树，比较两种方案下目标函数的优劣，从而自动对有缺失值的样本进行划分，无需对缺失特征进行填充预处理。</p><p><strong>此外</strong>，xgboost还支持候选分位点切割，特征并行等，可以提升性能。</p><h3 id=二xgboost原理概述>二、XGBoost原理概述<a hidden class=anchor aria-hidden=true href=#二xgboost原理概述>#</a></h3><p>面从假设空间，目标函数，优化算法3个角度对xgboost的原理进行概括性的介绍。</p><p><strong>1，假设空间</strong></p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_png/4WgILHBwVHibhx13Lugx4C4vibEEpKDjR0Fe8QIBoZlJ1FOxl6hMHzqJdDW4qzvibqEs6yoPsLbma2nNkL7gsaNng/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_png/4WgILHBwVHibhx13Lugx4C4vibEEpKDjR0lwXWCdMDP5ibMK3D3t5ianLwCst5opXGRrthdnecPzKAF6VL5Yszuxww/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_png/4WgILHBwVHibhx13Lugx4C4vibEEpKDjR0fx2icma2wLZmppde5SJQbjORDVjEYTeJicpJtQGQnwib4oE0bFUwSplPg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p><strong>2，目标函数</strong></p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_png/4WgILHBwVHibhx13Lugx4C4vibEEpKDjR0mBrvmW8ha6bydPiaErK5FdyyNYbxiaU3GvdRkFSmf2wrb16wWC7DqdHQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_png/4WgILHBwVHibhx13Lugx4C4vibEEpKDjR0xt3jt0LKucp8j0ayMRcBchBOutibTP3uFKj62bEt5ZnSYq9ibhMibXAJQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p><strong>3，优化算法</strong></p><p>基本思想：贪心法，逐棵树进行学习，每棵树拟合之前模型的偏差。</p><h3 id=三第t棵树学什么><strong>三、第t棵树学什么？</strong><a hidden class=anchor aria-hidden=true href=#三第t棵树学什么>#</a></h3><h3 id=heading><a hidden class=anchor aria-hidden=true href=#heading>#</a></h3><p>要完成构建xgboost模型，我们需要确定以下一些事情。</p><p>1，如何boost? 如果已经得到了前面t-1棵树构成的加法模型，如何确定第t棵树的学习目标？</p><p>2，如何生成树？已知第t棵树的学习目标的前提下，如何学习这棵树？具体又包括是否进行分裂？选择哪个特征进行分裂？选择什么分裂点位？分裂的叶子节点如何取值？</p><p>我们首先考虑如何boost的问题，顺便解决分裂的叶子节点如何取值的问题。</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_png/4WgILHBwVHibhx13Lugx4C4vibEEpKDjR0RkRqZUiaaWHR5LOhiau1Ub1NBq95icH2AzxhJGALrPN7mooo2icLU6icC6A/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_png/4WgILHBwVHibhx13Lugx4C4vibEEpKDjR04V9ESutbFZ6eqmY5RUwyu6YohfugwwGpB0F8exfgBc0bJO6ic9Ug8jQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_png/4WgILHBwVHibhx13Lugx4C4vibEEpKDjR09IF4icI2p1HcLibicLaAw3JcYqg1QD64QagqFjkDnMq84Gguoej1QYBtA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_png/4WgILHBwVHibhx13Lugx4C4vibEEpKDjR0gWlcQhTq8iasykFK9BrnvXbFXknOHt9Rl1yaejLz9vSk6EFZ2kvhUFQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><h3 id=四如何生成第t棵树><strong>四、如何生成第t棵树？</strong><a hidden class=anchor aria-hidden=true href=#四如何生成第t棵树>#</a></h3><p>xgboost采用二叉树，开始的时候，全部样本都在一个叶子节点上。然后叶子节点不断通过二分裂，逐渐生成一棵树。</p><p>xgboost使用levelwise的生成策略，即每次对同一层级的全部叶子节点尝试进行分裂。</p><p>对叶子节点分裂生成树的过程有几个基本的问题：是否要进行分裂？选择哪个特征进行分裂？在特征的什么点位进行分裂？以及分裂后新的叶子上取什么值？</p><p>叶子节点的取值问题前面已经解决了。我们重点讨论几个剩下的问题。</p><p><strong>1，是否要进行分裂？</strong>
根据树的剪枝策略的不同，这个问题有两种不同的处理。如果是预剪枝策略，那么只有当存在某种分裂方式使得分裂后目标函数发生下降，才会进行分裂。</p><p>但如果是后剪枝策略，则会无条件进行分裂，等树生成完成后，再从上而下检查树的各个分枝是否对目标函数下降产生正向贡献从而进行剪枝。</p><p>xgboost采用预剪枝策略，只有分裂后的增益大于0才会进行分裂。</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_png/4WgILHBwVHibhx13Lugx4C4vibEEpKDjR0bDu5YWbKfb5k6Np28TfpWcg4FZJpTgb1udWiaxoTAnsJJKBaoxibKMKQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><p><strong>2，选择什么特征进行分裂？</strong></p><p>xgboost采用特征并行的方法进行计算选择要分裂的特征，即用多个线程，尝试把各个特征都作为分裂的特征，找到各个特征的最优分割点，计算根据它们分裂后产生的增益，选择增益最大的那个特征作为分裂的特征。</p><p><strong>3，选择什么分裂点位？</strong></p><p>xgboost选择某个特征的分裂点位的方法有两种，一种是全局扫描法，另一种是候选分位点法。
全局扫描法将所有样本该特征的取值按从小到大排列，将所有可能的分裂位置都试一遍，找到其中增益最大的那个分裂点，其计算复杂度和叶子节点上的样本特征不同的取值个数成正比。
而候选分位点法是一种近似算法，仅选择常数个（如256个）候选分裂位置，然后从候选分裂位置中找出最优的那个。</p><p><img loading=lazy src="https://mmbiz.qpic.cn/mmbiz_png/4WgILHBwVHibhx13Lugx4C4vibEEpKDjR0NEaYoARvShoshdKM5sciaz5ic565grdAWhF2zsRPib5yKImkEk0wkDjYA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=img></p><h3 id=五xgboost算法原理小结>五、XGBoost算法原理小结<a hidden class=anchor aria-hidden=true href=#五xgboost算法原理小结>#</a></h3><p>XGBoost（eXtreme Gradient Boosting）全名叫极端梯度提升，XGBoost是集成学习方法的王牌，在Kaggle数据挖掘比赛中，大部分获胜者用了XGBoost，XGBoost在绝大多数的回归和分类问题上表现的十分顶尖，本文较详细的介绍了XGBoost的算法原理。</p><p>目录</p><ol><li><p>最优模型的构建方法</p></li><li><p>Boosting的回归思想</p></li><li><p>XGBoost的目标函数推导</p></li><li><p>XGBoost的回归树构建方法</p></li><li><p>XGBoost与GDBT的区别</p></li></ol><p><strong>最优模型的构建方法</strong></p><p>构建最优模型的一般方法是最小化训练数据的损失函数，我们用字母 L表示，如下式：</p><p><img loading=lazy src="https://t12.baidu.com/it/u=2088825977,1064375271&amp;fm=173&amp;app=49&amp;f=JPEG?w=275&amp;h=87&amp;s=8D02ED128D214D0156D501DE0000C0B3" alt=img></p><p>式（1）称为经验风险最小化，训练得到的模型复杂度较高。当训练数据较小时，模型很容易出现过拟合问题。</p><p>因此，为了降低模型的复杂度，常采用下式：</p><p><img loading=lazy src="https://t10.baidu.com/it/u=3726621105,2442873595&amp;fm=173&amp;app=49&amp;f=JPEG?w=351&amp;h=52" alt=img></p><p>其中J(f)为模型的复杂度，式（2）称为结构风险最小化，结构风险最小化的模型往往对训练数据以及未知的测试数据都有较好的预测 。</p><p>应用：决策树的生成和剪枝分别对应了经验风险最小化和结构风险最小化，XGBoost的决策树生成是结构风险最小化的结果，后续会详细介绍。</p><p><strong>Boosting方法的回归思想</strong></p><p>Boosting法是结合多个弱学习器给出最终的学习结果，不管任务是分类或回归，我们都用回归任务的思想来构建最优Boosting模型 。</p><p>回归思想：把每个弱学习器的输出结果当成连续值，这样做的目的是可以对每个弱学习器的结果进行累加处理，且能更好的利用损失函数来优化模型。</p><p>假设</p><p><img loading=lazy src="https://t10.baidu.com/it/u=2575575693,679892115&amp;fm=173&amp;app=49&amp;f=JPEG?w=60&amp;h=36&amp;s=8DA2E516CDA6F8111CC0C5D60000B0B3" alt=img></p><p>是第 t 轮弱学习器的输出结果，</p><p><img loading=lazy src="https://t11.baidu.com/it/u=3480491077,4153735365&amp;fm=173&amp;app=49&amp;f=JPEG?w=27&amp;h=47" alt=img></p><p>是模型的输出结果，</p><p><img loading=lazy src="https://t10.baidu.com/it/u=1589424537,273963798&amp;fm=173&amp;app=49&amp;f=JPEG?w=24&amp;h=26" alt=img></p><p>是实际输出结果，表达式如下：</p><p><img loading=lazy src="https://t12.baidu.com/it/u=3934660316,3850604984&amp;fm=173&amp;app=49&amp;f=JPEG?w=269&amp;h=110&amp;s=8D32ED120B61680118DDC0DA0000D0B1" alt=img></p><p>上面两式就是加法模型，都默认弱学习器的输出结果是连续值。因为回归任务的弱学习器本身是连续值，所以不做讨论，下面详细介绍分类任务的回归思想。</p><p><strong>分类任务的回归思想：</strong></p><p>根据2.1式的结果，得到最终的分类器：</p><p><img loading=lazy src="https://t12.baidu.com/it/u=2106315725,639848079&amp;fm=173&amp;app=49&amp;f=JPEG?w=393&amp;h=58" alt=img></p><p>分类的损失函数一般选择指数函数或对数函数，这里假设损失函数为对数函数，学习器的损失函数是</p><p><img loading=lazy src="https://t11.baidu.com/it/u=2757775603,3145979471&amp;fm=173&amp;app=49&amp;f=JPEG?w=345&amp;h=37" alt=img></p><p>若实际输出结果yi=1，则：</p><p><img loading=lazy src="https://t10.baidu.com/it/u=2994708501,2270324777&amp;fm=173&amp;app=49&amp;f=JPEG?w=314&amp;h=48" alt=img></p><p>求（2.5）式对</p><p><img loading=lazy src="https://t11.baidu.com/it/u=3260474260,568114116&amp;fm=173&amp;app=49&amp;f=JPEG?w=39&amp;h=47&amp;s=8DB2ED12EDF548111C78B5C2000070B1" alt=img></p><p>的梯度，得：</p><p><img loading=lazy src="https://t10.baidu.com/it/u=1519027692,1118277381&amp;fm=173&amp;app=49&amp;f=JPEG?w=197&amp;h=95&amp;s=81306D328BB04C014875D0DA0000C0B3" alt=img></p><p>负梯度方向是损失函数下降最快的方向，（2.6）式取反的值大于0，因此弱学习器是往增大</p><p><img loading=lazy src="https://t12.baidu.com/it/u=58039754,2342150744&amp;fm=173&amp;app=49&amp;f=JPEG?w=40&amp;h=54&amp;s=0530ED32EDF548110E68B0C20000C0B3" alt=img></p><p>的方向迭代的，图形表示为：</p><p><img loading=lazy src="https://t11.baidu.com/it/u=538277086,3452942280&amp;fm=173&amp;app=49&amp;f=JPEG?w=389&amp;h=283&amp;s=4C843C728BDF40491E6560DE0000C0B2" alt=img></p><p>如上图，当样本的实际标记 yi 是 1 时，模型输出结果</p><p><img loading=lazy src="https://t12.baidu.com/it/u=58039754,2342150744&amp;fm=173&amp;app=49&amp;f=JPEG?w=40&amp;h=54&amp;s=0530ED32EDF548110E68B0C20000C0B3" alt=img></p><p>随着迭代次数的增加而增加（红线箭头），模型的损失函数相应的减小；当样本的实际标记 yi 是 -1时，模型输出结果</p><p><img loading=lazy src="https://t12.baidu.com/it/u=58039754,2342150744&amp;fm=173&amp;app=49&amp;f=JPEG?w=40&amp;h=54&amp;s=0530ED32EDF548110E68B0C20000C0B3" alt=img></p><p>随着迭代次数的增加而减小（红线箭头），模型的损失函数相应的减小 。这就是加法模型的原理所在，通过多次的迭代达到减小损失函数的目的。</p><p>小结：Boosting方法把每个弱学习器的输出看成是连续值，使得损失函数是个连续值，因此可以通过弱学习器的迭代达到优化模型的目的，这也是集成学习法加法模型的原理所在 。</p><p><strong>XGBoost算法的目标函数推导</strong></p><p>目标函数，即损失函数，通过最小化损失函数来构建最优模型，由第一节可知， 损失函数应加上表示模型复杂度的正则项，且XGBoost对应的模型包含了多个CART树，因此，模型的目标函数为：</p><p><img loading=lazy src="https://t10.baidu.com/it/u=1096985463,1889276164&amp;fm=173&amp;app=49&amp;f=JPEG?w=357&amp;h=54" alt=img></p><p>（3.1）式是正则化的损失函数，等式右边第一部分是模型的训练误差，第二部分是正则化项，这里的正则化项是K棵树的正则化项相加而来的。</p><p><strong>CART树的介绍：</strong></p><p><img loading=lazy src="https://t12.baidu.com/it/u=1604336024,2048239596&amp;fm=173&amp;app=49&amp;f=JPEG?w=355&amp;h=131&amp;s=9988783391E67D035E7C10CF000080B2" alt=img></p><p>上图为第K棵CART树，确定一棵CART树需要确定两部分，第一部分就是树的结构，这个结构将输入样本映射到一个确定的叶子节点上，记为</p><p><img loading=lazy src="https://t11.baidu.com/it/u=1080851515,2092106391&amp;fm=173&amp;app=49&amp;f=JPEG?w=56&amp;h=33&amp;s=8922CD16CF8EE8114AF548DA0200B0B3" alt=img></p><p>。第二部分就是各个叶子节点的值，q(x)表示输出的叶子节点序号，</p><p><img loading=lazy src="https://t11.baidu.com/it/u=3855316450,52407482&amp;fm=173&amp;app=49&amp;f=JPEG?w=49&amp;h=31&amp;s=8C02ED16CD95E81108D4D8D20200D0B3" alt=img></p><p>表示对应叶子节点序号的值。由定义得：</p><p><img loading=lazy src="https://t10.baidu.com/it/u=1851728262,1452004307&amp;fm=173&amp;app=49&amp;f=JPEG?w=202&amp;h=36&amp;s=8D42ED124567CB204A55E0D2000090B1" alt=img></p><p><strong>树的复杂度定义</strong></p><p>XGBoost法对应的模型包含了多棵cart树，定义每棵树的复杂度：</p><p><img loading=lazy src="https://t11.baidu.com/it/u=810269563,562881171&amp;fm=173&amp;app=49&amp;f=JPEG?w=281&amp;h=47&amp;s=8D22ED12C55AEE335C5C20D60000D0B1" alt=img></p><p>其中T为叶子节点的个数，||w||为叶子节点向量的模 。γ表示节点切分的难度，λ表示L2正则化系数。</p><p>如下例树的复杂度表示：</p><p><img loading=lazy src="https://t10.baidu.com/it/u=2863031412,2342096706&amp;fm=173&amp;app=49&amp;f=JPEG?w=640&amp;h=370&amp;s=0812CE121D5045CA0CCD68D8000050B2" alt=img></p><p><strong>目标函数推导</strong></p><p>根据（3.1）式，共进行t次迭代的学习模型的目标函数为：</p><p><img loading=lazy src="https://t11.baidu.com/it/u=236689006,1474271316&amp;fm=173&amp;app=49&amp;f=JPEG?w=550&amp;h=220&amp;s=AC32ED1297D06DC25CFC0DC2000080B3" alt=img></p><p>泰勒公式的二阶导近似表示：</p><p><img loading=lazy src="https://t10.baidu.com/it/u=959253417,1988296487&amp;fm=173&amp;app=49&amp;f=JPEG?w=512&amp;h=53" alt=img></p><p>令</p><p><img loading=lazy src="https://t10.baidu.com/it/u=686929623,2192080821&amp;fm=173&amp;app=49&amp;f=JPEG?w=53&amp;h=29" alt=img></p><p>为Δx，则（3.5）式的二阶近似展开：</p><p><img loading=lazy src="https://t11.baidu.com/it/u=1862472463,3969586164&amp;fm=173&amp;app=49&amp;f=JPEG?w=640&amp;h=153&amp;s=8D72E512C1354C23525D20DB000080B1" alt=img></p><p>其中：</p><p><img loading=lazy src="https://t10.baidu.com/it/u=602419326,77643855&amp;fm=173&amp;app=49&amp;f=JPEG?w=204&amp;h=35&amp;s=8D60ED12C553FE325AF911C30000F0B1" alt=img></p><p><img loading=lazy src="https://t11.baidu.com/it/u=3021833638,1350476957&amp;fm=173&amp;app=49&amp;f=JPEG?w=200&amp;h=34&amp;s=8C40F512C553EF305ADD10C20000E0B1" alt=img></p><p><img loading=lazy src="https://t12.baidu.com/it/u=1551381323,1490568674&amp;fm=173&amp;app=49&amp;f=JPEG?w=107&amp;h=33&amp;s=8D42CD14C51FC6304E7439D60300C0B1" alt=img></p><p>表示前t-1棵树组成的学习模型的预测误差，gi和hi分别表示预测误差对当前模型的一阶导和二阶导 ，当前模型往预测误差减小的方向进行迭代。</p><p>忽略（3.8）式常数项，并结合（3.4）式，得：</p><p><img loading=lazy src="https://t11.baidu.com/it/u=833462836,1613458304&amp;fm=173&amp;app=49&amp;f=JPEG?w=541&amp;h=62" alt=img></p><p>通过（3.2）式简化（3.9）式：</p><p><img loading=lazy src="https://t10.baidu.com/it/u=4272700476,2093594111&amp;fm=173&amp;app=49&amp;f=JPEG?w=550&amp;h=61" alt=img></p><p>（3.10）式第一部分是对所有训练样本集进行累加，因为所有样本都是映射为树的叶子节点，我们换种思维，从叶子节点出发，对所有的叶子节点进行累加，得：</p><p><img loading=lazy src="https://t11.baidu.com/it/u=2042896387,2160999246&amp;fm=173&amp;app=49&amp;f=JPEG?w=488&amp;h=61" alt=img></p><p>令</p><p><img loading=lazy src="https://t12.baidu.com/it/u=1090269392,1428256557&amp;fm=173&amp;app=49&amp;f=JPEG?w=201&amp;h=50&amp;s=8D02ED12CD16E4130EDCB0CE0000D0B1" alt=img></p><p>Gj 表示映射为叶子节点 j 的所有输入样本的一阶导之和，同理，Hj表示二阶导之和。</p><p>得：</p><p><img loading=lazy src="https://t10.baidu.com/it/u=2162289948,3953406118&amp;fm=173&amp;app=49&amp;f=JPEG?w=431&amp;h=53" alt=img></p><p>对于第 t 棵CART树的某一个确定结构（可用q(x)表示），其叶子节点是相互独立的，Gj和Hj是确定量，因此，（3.12）可以看成是关于叶子节点的一元二次函数 。最小化（3.12）式，得：</p><p><img loading=lazy src="https://t10.baidu.com/it/u=978046209,189317784&amp;fm=173&amp;app=49&amp;f=JPEG?w=229&amp;h=60&amp;s=84306532C5356423487CE1DA0000D0B3" alt=img></p><p>得到最终的目标函数：</p><p><img loading=lazy src="https://t10.baidu.com/it/u=303000120,2319644979&amp;fm=173&amp;app=49&amp;f=JPEG?w=321&amp;h=66&amp;s=8C32ED12CDF6EC035E7CE0D60000D0B3" alt=img></p><p>（3.14）也称为打分函数(scoring function)，它是衡量树结构好坏的标准，值越小，代表这样的结构越好 。我们用打分函数选择最佳切分点，从而构建CART树。</p><p><strong>CART回归树的构建方法</strong></p><p>上节推导得到的打分函数是衡量树结构好坏的标准，因此，可用打分函数来选择最佳切分点。首先确定样本特征的所有切分点，对每一个确定的切分点进行切分，切分好坏的标准如下：</p><p><img loading=lazy src="https://t11.baidu.com/it/u=2513959336,3320328419&amp;fm=173&amp;app=49&amp;f=JPEG?w=456&amp;h=78&amp;s=8530E432C5674F304AF475DB0000E0B1" alt=img></p><p>Gain表示单节点obj<em>与切分后的两个节点的树obj</em>之差，遍历所有特征的切分点，找到最大Gain的切分点即是最佳分裂点，根据这种方法继续切分节点，得到CART树。若 γ 值设置的过大，则Gain为负，表示不切分该节点，因为切分后的树结构变差了。γ值越大，表示对切分后obj下降幅度要求越严，这个值可以在XGBoost中设定。</p><p><strong>XGBoost与GDBT的区别</strong></p><ol><li><p>XGBoost生成CART树考虑了树的复杂度，GDBT未考虑，GDBT在树的剪枝步骤中考虑了树的复杂度。</p></li><li><p>XGBoost是拟合上一轮损失函数的二阶导展开，GDBT是拟合上一轮损失函数的一阶导展开，因此，XGBoost的准确性更高，且满足相同的训练效果，需要的迭代次数更少。</p></li><li><p>XGBoost与GDBT都是逐次迭代来提高模型性能，但是XGBoost在选取最佳切分点时可以开启多线程进行，大大提高了运行速度。</p></li></ol><hr><p>参考:
<a href=https://mp.weixin.qq.com/s/cMgd-wBlzjacL21FPK2y7Q>https://mp.weixin.qq.com/s/cMgd-wBlzjacL21FPK2y7Q</a>
<a href=https://www.cnblogs.com/pinard/p/10979808.html>https://www.cnblogs.com/pinard/p/10979808.html</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://reid00.github.io/en/tags/xgboost/>Xgboost</a></li><li><a href=https://reid00.github.io/en/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/>集成学习</a></li></ul><nav class=paginav><a class=prev href=https://reid00.github.io/en/posts/os_network/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/><span class=title>« Prev</span><br><span>操作系统之内存管理</span>
</a><a class=next href=https://reid00.github.io/en/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Badaboost/><span class=title>Next »</span><br><span>集成学习之AdaBoost</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main></body></html>