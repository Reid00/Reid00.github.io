<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GBDT+LR | Reid's Blog</title>
<meta name=keywords content="GBDT,LR,逻辑回归"><meta name=description content="GBDT+LR"><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/en/posts/ml/gbdt+lr/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://reid00.github.io/en/posts/ml/gbdt+lr/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><head><meta name=referrer content="no-referrer"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script><script>var doNotTrack=!1,dnt;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK")}</script><meta property="og:title" content="GBDT+LR"><meta property="og:description" content="GBDT+LR"><meta property="og:type" content="article"><meta property="og:url" content="https://reid00.github.io/en/posts/ml/gbdt+lr/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-16T19:35:17+08:00"><meta property="article:modified_time" content="2023-03-16T19:35:17+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="GBDT+LR"><meta name=twitter:description content="GBDT+LR"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://reid00.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"机器学习，深度学习，知识图谱相关","item":"https://reid00.github.io/en/posts/ml/"},{"@type":"ListItem","position":3,"name":"GBDT+LR","item":"https://reid00.github.io/en/posts/ml/gbdt+lr/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GBDT+LR","name":"GBDT\u002bLR","description":"GBDT+LR","keywords":["GBDT","LR","逻辑回归"],"articleBody":"概述 GBDT的加入，是为了弥补LR难以实现特征组合的缺点。\nLR LR作为一个线性模型，以概率形式输出结果，在工业上得到了十分广泛的应用。 其具有简单快速高效，结果可解释，可以分布式计算。搭配L1，L2正则，可以有很好地鲁棒性以及挑选特征的能力。\n但由于其简单，也伴随着拟合能力不足，无法做特征组合的缺点。\n通过梯度下降法可以优化参数\n可以称之上是 CTR 预估模型的开山鼻祖，也是工业界使用最为广泛的 CTR 预估模型\n但是在CTR领域，单纯的LR虽然可以快速处理海量高维离散特征，但是由于线性模型的局限性，其在特征组合方面仍有不足，所以后续才发展出了FM来引入特征交叉。在此之前，业界也有使用GBDT来作为特征组合的工具，其结果输出给LR。\nLR 优缺点 优点：由于 LR 模型简单，训练时便于并行化，在预测时只需要对特征进行线性加权，所以性能比较好，往往适合处理海量 id 类特征，用 id 类特征有一个很重要的好处，就是防止信息损失（相对于范化的 CTR 特征），对于头部资源会有更细致的描述。\n缺点：LR 的缺点也很明显，首先对连续特征的处理需要先进行离散化，如上文所说，人工分桶的方式会引入多种问题。另外 LR 需要进行人工特征组合，这就需要开发者有非常丰富的领域经验，才能不走弯路。这样的模型迁移起来比较困难，换一个领域又需要重新进行大量的特征工程。\nGBDT+LR 首先，GBDT是一堆树的组合，假设有k棵树 。 对于第i棵树 ，其存在 个叶子节点。而从根节点到叶子节点，可以认为是一条路径，这条路径是一些特征的组合，例如从根节点到某一个叶子节点的路径可能是“ ”这就是一组特征组合。到达这个叶子节点的样本都拥有这样的组合特征，而这个组合特征使得这个样本得到了GBDT的预测结果。 所以对于GBDT子树 ，会返回一个 维的one-hot向量 对于整个GBDT，会返回一个 维的向量 ，这个向量由0-1组成。\n然后，这个 ,会作为输入，送进LR模型，最终输出结果\n模型大致如图所示。上图中由两棵子树，分别有3和2个叶子节点。对于一个样本x，最终可以落入第一棵树的某一个叶子和第二棵树的某一个叶子，得到两个独热编码的结果例如 [0,0,1],[1,0]组合得[0,0,1,1,0]输入到LR模型最后输出结果。\n由于LR善于处理离散特征，GBDT善于处理连续特征。所以也可以交由GBDT处理连续特征，输出结果拼接上离散特征一起输入LR。\n讨论 至于GBDT为何不善于处理高维离散特征？\nhttps://cloud.tencent.com/developer/article/1005416\n缺点：对于海量的 id 类特征，GBDT 由于树的深度和棵树限制（防止过拟合），不能有效的存储；另外海量特征在也会存在性能瓶颈，经笔者测试，当 GBDT 的 one hot 特征大于 10 万维时，就必须做分布式的训练才能保证不爆内存。所以 GBDT 通常配合少量的反馈 CTR 特征来表达，这样虽然具有一定的范化能力，但是同时会有信息损失，对于头部资源不能有效的表达。\nhttps://www.zhihu.com/question/35821566\n后来思考后发现原因是因为现在的模型普遍都会带着正则项，而 lr 等线性模型的正则项是对权重的惩罚，也就是 W1一旦过大，惩罚就会很大，进一步压缩 W1的值，使他不至于过大，而树模型则不一样，树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于上面这种 case，树只需要一个节点就可以完美分割9990和10个样本，惩罚项极其之小. 这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：带正则化的线性模型比较不容易对稀疏特征过拟合。\nGBDT当树深度\u003e2时，其实组合的是多元特征了，而且由于子树规模的限制，导致其特征组合的能力并不是很强，所以才有了后续FM，FFM的发展x\nGBDT + LR 改进 Facebook 的方案在实际使用中，发现并不可行，因为广告系统往往存在上亿维的 id 类特征(用户 guid10 亿维，广告 aid 上百万维)，而 GBDT 由于树的深度和棵树的限制，无法存储这么多 id 类特征，导致信息的损失。有如下改进方案供读者参考：\n**方案一：**GBDT 训练除 id 类特征以外的所有特征，其他 id 类特征在 LR 阶段再加入。这样的好处很明显，既利用了 GBDT 对连续特征的自动离散化和特征组合，同时 LR 又有效利用了 id 类离散特征，防止信息损失。\n**方案二：**GBDT 分别训练 id 类树和非 id 类树，并把组合特征传入 LR 进行二次训练。对于 id 类树可以有效保留头部资源的信息不受损失；对于非 id 类树，长尾资源可以利用其范化信息（反馈 CTR 等）。但这样做有一个缺点是，介于头部资源和长尾资源中间的一部分资源，其有效信息即包含在范化信息(反馈 CTR) 中，又包含在 id 类特征中，而 GBDT 的非 id 类树只存的下头部的资源信息，所以还是会有部分信息损失。\n优缺点:\n优点：GBDT 可以自动进行特征组合和离散化，LR 可以有效利用海量 id 类离散特征，保持信息的完整性。\n缺点：LR 预测的时候需要等待 GBDT 的输出，一方面 GBDT在线预测慢于单 LR，另一方面 GBDT 目前不支持在线算法，只能以离线方式进行更新。\n","wordCount":"139","inLanguage":"en","datePublished":"2023-03-16T19:35:17+08:00","dateModified":"2023-03-16T19:35:17+08:00","author":[{"@type":"Person","name":"Reid"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://reid00.github.io/en/posts/ml/gbdt+lr/"},"publisher":{"@type":"Organization","name":"Reid's Blog","logo":{"@type":"ImageObject","url":"https://reid00.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://reid00.github.io/en/ accesskey=h title="Reid's Blog (Alt + H)">Reid's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://reid00.github.io/en/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://reid00.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://reid00.github.io/en/categories/ title=Categorys><span>Categorys</span></a></li><li><a href=https://reid00.github.io/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://reid00.github.io/en/>Home</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/ml/>机器学习，深度学习，知识图谱相关</a></div><h1 class=post-title>GBDT+LR</h1><div class=post-description>GBDT+LR</div><div class=post-meta><span title='2023-03-16 19:35:17 +0800 +0800'>2023-03-16 19:35</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;139 words&nbsp;·&nbsp;Reid</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e6%a6%82%e8%bf%b0 aria-label=概述>概述</a></li><li><a href=#lr aria-label=LR>LR</a></li><li><a href=#lr-%e4%bc%98%e7%bc%ba%e7%82%b9 aria-label="LR 优缺点">LR 优缺点</a></li><li><a href=#gbdtlr aria-label=GBDT+LR>GBDT+LR</a></li><li><a href=#%e8%ae%a8%e8%ae%ba aria-label=讨论>讨论</a></li><li><a href=#gbdt--lr-%e6%94%b9%e8%bf%9b aria-label="GBDT + LR 改进">GBDT + LR 改进</a></li></ul></div></details></div><div class=post-content><h4 id=概述>概述<a hidden class=anchor aria-hidden=true href=#概述>#</a></h4><p>GBDT的加入，是为了弥补LR难以实现特征组合的缺点。</p><h4 id=lr>LR<a hidden class=anchor aria-hidden=true href=#lr>#</a></h4><p>LR作为一个线性模型，以概率形式输出结果，在工业上得到了十分广泛的应用。
其具有简单快速高效，结果可解释，可以分布式计算。搭配L1，L2正则，可以有很好地鲁棒性以及挑选特征的能力。</p><p>但由于其简单，也伴随着拟合能力不足，无法做特征组合的缺点。</p><p><img loading=lazy src="https://math.jianshu.com/math?formula=f%28x%29%3D%5Cfrac%7B1%7D%7B1%2Bexp%28-wx%29%7D" alt="f(x)=\frac{1}{1+exp(-wx)}">
<img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.749i3i2h7gc0.webp alt=gs></p><p>通过梯度下降法可以优化参数</p><blockquote><p>可以称之上是 CTR 预估模型的开山鼻祖，也是工业界使用最为广泛的 CTR 预估模型</p></blockquote><p>但是在CTR领域，单纯的LR虽然可以快速处理<strong>海量高维离散特征</strong>，但是由于线性模型的局限性，其在特征组合方面仍有不足，所以后续才发展出了FM来引入特征交叉。在此之前，业界也有使用GBDT来作为特征组合的工具，其结果输出给LR。</p><h4 id=lr-优缺点>LR 优缺点<a hidden class=anchor aria-hidden=true href=#lr-优缺点>#</a></h4><blockquote><p><strong>优点：<strong>由于 LR 模型简单，训练时便于并行化，在预测时只需要对特征进行线性加权，所以</strong>性能比较好</strong>，往往适合处理<strong>海量 id 类特征</strong>，用 id 类特征有一个很重要的好处，就是<strong>防止信息损失</strong>（相对于范化的 CTR 特征），对于头部资源会有更细致的描述。</p><p><strong>缺点：<strong>LR 的缺点也很明显，首先对连续特征的处理需要先进行</strong>离散化</strong>，如上文所说，人工分桶的方式会引入多种问题。另外 LR 需要进行<strong>人工特征组合</strong>，这就需要开发者有非常丰富的领域经验，才能不走弯路。这样的模型迁移起来比较困难，换一个领域又需要重新进行大量的特征工程。</p></blockquote><h4 id=gbdtlr>GBDT+LR<a hidden class=anchor aria-hidden=true href=#gbdtlr>#</a></h4><p>首先，GBDT是一堆树的组合，假设有k棵树<img loading=lazy src="https://math.jianshu.com/math?formula=%28T_1%2CT_2...T_k%29" alt=(T_1,T_2&amp;hellip;T_k)>
。
对于第i棵树<img loading=lazy src="https://math.jianshu.com/math?formula=T_i" alt=T_i>
，其存在<img loading=lazy src="https://math.jianshu.com/math?formula=N_i" alt=N_i>
个叶子节点。而从根节点到叶子节点，可以认为是一条路径，这条路径是一些特征的组合，例如从根节点到某一个叶子节点的路径可能是“<img loading=lazy src="https://math.jianshu.com/math?formula=x_1%3Ca%2Cx_2%3Eb%2Cx_3%3Cc" alt=x_1&amp;lt;a,x_2&amp;gt;b,x_3&amp;lt;c>
”这就是一组特征组合。到达这个叶子节点的样本都拥有这样的组合特征，而这个组合特征使得这个样本得到了GBDT的预测结果。
所以对于GBDT子树<img loading=lazy src="https://math.jianshu.com/math?formula=T_i" alt=T_i>
，会返回一个<img loading=lazy src="https://math.jianshu.com/math?formula=N_i" alt=N_i>
维的one-hot向量
对于整个GBDT，会返回一个<img loading=lazy src="https://math.jianshu.com/math?formula=%5Csum_%7Bi%3D1%7D%5E%7Bk%7DN_i" alt="\sum_{i=1}^{k}N_i">
维的向量<img loading=lazy src="https://math.jianshu.com/math?formula=X_%7Bgbdt%7D" alt=X_{gbdt}>
，这个向量由0-1组成。</p><p>然后，这个<img loading=lazy src="https://math.jianshu.com/math?formula=X_%7Bgbdt%7D" alt=X_{gbdt}>
,会作为输入，送进LR模型，最终输出结果</p><p><img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220610/image.kq9wdk6lmdc.webp alt=img></p><p>模型大致如图所示。上图中由两棵子树，分别有3和2个叶子节点。对于一个样本x，最终可以落入第一棵树的某一个叶子和第二棵树的某一个叶子，得到两个独热编码的结果例如
[0,0,1],[1,0]组合得[0,0,1,1,0]输入到LR模型最后输出结果。</p><p>由于LR善于处理离散特征，GBDT善于处理连续特征。所以也可以交由GBDT处理连续特征，输出结果拼接上离散特征一起输入LR。</p><h4 id=讨论>讨论<a hidden class=anchor aria-hidden=true href=#讨论>#</a></h4><p>至于GBDT为何不善于处理高维离散特征？</p><blockquote><p><a href=https://cloud.tencent.com/developer/article/1005416>https://cloud.tencent.com/developer/article/1005416</a></p><p>缺点：对于海量的 id 类特征，GBDT 由于树的深度和棵树限制（防止过拟合），不能有效的存储；另外海量特征在也会存在性能瓶颈，经笔者测试，当 GBDT 的 one hot 特征大于 10 万维时，就必须做分布式的训练才能保证不爆内存。所以 GBDT 通常配合少量的反馈 CTR 特征来表达，这样虽然具有一定的范化能力，但是同时会有信息损失，对于头部资源不能有效的表达。</p></blockquote><blockquote><p><a href=https://www.zhihu.com/question/35821566>https://www.zhihu.com/question/35821566</a></p><p>后来思考后发现原因是因为现在的模型普遍都会带着正则项，而 lr 等线性模型的正则项是对权重的惩罚，也就是 W1一旦过大，惩罚就会很大，进一步压缩 W1的值，使他不至于过大，而树模型则不一样，树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于上面这种 case，树只需要一个节点就可以完美分割9990和10个样本，惩罚项极其之小.
<strong>这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：带正则化的线性模型比较不容易对稀疏特征过拟合。</strong></p></blockquote><p>GBDT当树深度>2时，其实组合的是多元特征了，而且由于子树规模的限制，导致其特征组合的能力并不是很强，所以才有了后续FM，FFM的发展x</p><h4 id=gbdt--lr-改进>GBDT + LR 改进<a hidden class=anchor aria-hidden=true href=#gbdt--lr-改进>#</a></h4><p>Facebook 的方案在实际使用中，发现并不可行，因为广告系统往往存在上亿维的 id 类特征(用户 guid10 亿维，广告 aid 上百万维)，而 GBDT 由于树的深度和棵树的限制，无法存储这么多 id 类特征，导致信息的损失。有如下改进方案供读者参考：</p><p>**方案一：**GBDT 训练除 id 类特征以外的所有特征，其他 id 类特征在 LR 阶段再加入。这样的好处很明显，既利用了 GBDT 对连续特征的自动离散化和特征组合，同时 LR 又有效利用了 id 类离散特征，防止信息损失。</p><p><img loading=lazy src=https://blog-10039692.file.myqcloud.com/1499309378679_1105_1499309380228.png alt=img></p><p>**方案二：**GBDT 分别训练 id 类树和非 id 类树，并把组合特征传入 LR 进行二次训练。对于 id 类树可以有效保留头部资源的信息不受损失；对于非 id 类树，长尾资源可以利用其范化信息（反馈 CTR 等）。但这样做有一个缺点是，介于头部资源和长尾资源中间的一部分资源，其有效信息即包含在范化信息(反馈 CTR) 中，又包含在 id 类特征中，而 GBDT 的非 id 类树只存的下头部的资源信息，所以还是会有部分信息损失。</p><p><img loading=lazy src=https://blog-10039692.file.myqcloud.com/1499309348373_993_1499309349919.jpg alt=img></p><p>优缺点:</p><blockquote><p><strong>优点：<strong>GBDT 可以</strong>自动进行特征组合</strong>和<strong>离散化</strong>，LR 可以有效利用<strong>海量 id 类离散特征</strong>，<strong>保持信息的完整性</strong>。</p><p><strong>缺点：<strong>LR 预测的时候需要等待 GBDT 的输出，一方面 GBDT</strong>在线预测慢</strong>于单 LR，另一方面 GBDT 目前<strong>不支持在线算法</strong>，只能以离线方式进行更新。</p></blockquote></div><footer class=post-footer><ul class=post-tags><li><a href=https://reid00.github.io/en/tags/gbdt/>GBDT</a></li><li><a href=https://reid00.github.io/en/tags/lr/>LR</a></li><li><a href=https://reid00.github.io/en/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/>逻辑回归</a></li></ul><nav class=paginav><a class=prev href=https://reid00.github.io/en/posts/ml/self-attention/><span class=title>« Prev</span><br><span>Self Attention</span>
</a><a class=next href=https://reid00.github.io/en/posts/ml/kg%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/><span class=title>Next »</span><br><span>KG表示学习</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main></body></html>