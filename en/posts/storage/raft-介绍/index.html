<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Raft 介绍 | Reid's Blog</title>
<meta name=keywords content="Raft,MIT6.824,Consensus,共识算法"><meta name=description content="Raft 介绍"><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/en/posts/storage/raft-%E4%BB%8B%E7%BB%8D/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://reid00.github.io/en/posts/storage/raft-%E4%BB%8B%E7%BB%8D/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><head><meta name=referrer content="no-referrer"><script async src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script><script>var doNotTrack=!1,dnt;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK")}</script><meta property="og:title" content="Raft 介绍"><meta property="og:description" content="Raft 介绍"><meta property="og:type" content="article"><meta property="og:url" content="https://reid00.github.io/en/posts/storage/raft-%E4%BB%8B%E7%BB%8D/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-16T19:34:52+08:00"><meta property="article:modified_time" content="2023-03-16T19:34:52+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="Raft 介绍"><meta name=twitter:description content="Raft 介绍"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://reid00.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"存储, 分布式相关的文章","item":"https://reid00.github.io/en/posts/storage/"},{"@type":"ListItem","position":3,"name":"Raft 介绍","item":"https://reid00.github.io/en/posts/storage/raft-%E4%BB%8B%E7%BB%8D/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Raft 介绍","name":"Raft 介绍","description":"Raft 介绍","keywords":["Raft","MIT6.824","Consensus","共识算法"],"articleBody":"1. Raft 算法简介 1.1 Raft 背景 在分布式系统中，一致性算法至关重要。在所有一致性算法中，Paxos 最负盛名，它由莱斯利·兰伯特（Leslie Lamport）于 1990 年提出，是一种基于消息传递的一致性算法，被认为是类似算法中最有效的。\nPaxos 算法虽然很有效，但复杂的原理使它实现起来非常困难，截止目前，实现 Paxos 算法的开源软件很少，比较出名的有 Chubby、LibPaxos。此外，Zookeeper 采用的 ZAB（Zookeeper Atomic Broadcast）协议也是基于 Paxos 算法实现的，不过 ZAB 对 Paxos 进行了很多改进与优化，两者的设计目标也存在差异——ZAB 协议主要用于构建一个高可用的分布式数据主备系统，而 Paxos 算法则是用于构建一个分布式的一致性状态机系统。\n由于 Paxos 算法过于复杂、实现困难，极大地制约了其应用，而分布式系统领域又亟需一种高效而易于实现的分布式一致性算法，在此背景下，Raft 算法应运而生。\nRaft 算法在斯坦福 Diego Ongaro 和 John Ousterhout 于 2013 年发表的《In Search of an Understandable Consensus Algorithm》中提出。相较于 Paxos，Raft 通过逻辑分离使其更容易理解和实现，目前，已经有十多种语言的 Raft 算法实现框架，较为出名的有 etcd、Consul 。\n本文基于论文In Search of an Understandable Consensus Algorithm对raft协议进行分析，当然，还是建议读者直接看论文。\n相关链接:\n论文 官网 动画展示 分布式共识算法核心理论基础 在正式谈raft之前，还需要简单介绍下分布式共识算法所基于的理论工具。分布式共识协议在复制状态机的背景下产生的。在该方法中，一组服务器上的状态机计算相同的副本，即便某台机器宕机依然会继续运行。复制状态机是基于日志实现的。在这里有必要唠叨两句日志的特性。日志可以看做一个简单的存储抽象，append only，按照时间完全有序，注意这里面的日志并不是log4j或是syslog打出来的业务日志，那个我们称之为应用日志，这里的日志是用于程序访问的存储结构。有了上面的限制，使用日志就能够保证这样一件事。如图所示 我有一个日志，里面存储的是一系列的对数据的操作，此时系统外部有一系列输入数据，输入到这个日志中，经过日志中一系列command操作，由于日志的确定性和有序性，保证最后得到的输出序列也应该是确定的。扩展到分布式的场景，此时每台机器上所有了这么一个日志，此时我需要做的事情就是保证这几份日志是完全一致的。详细步骤就引出了论文中的那张经典的复制状态机的示意图 如图所示，server中的共识模块负责接收由client发送过来的请求，将请求中对应的操作记录到自己的日志中，同时通知给其他机器，让他们也进行同样的操作最终保证所有的机器都在日志中写入了这条操作。然后返回给客户端写入成功。复制状态机用于解决分布式中系统中的各种容错问题，例如master的高可用，例如Chubby以及ZK都是复制状态机，\n分布式一致性算法，通常满足以下性质： 在非拜占庭错误下，保证安全性(不会返回不正确的结果) 大多数机器运行，系统就可以正常运行，发生故障的机器在恢复正常后可以重新正常的加入到集群中 不依赖时序来保证日志的一致性 通常情况下，大多数机器就可以做出响应了，少数慢节点并不会拉低整个系统的性能 1.2 Raft 基本概念 首先我将整体串一遍raft，然后抽提出里面的相关概念进行说明。\n一个raft协议通常包含若干个节点，通常5个(2n+1),它最多允许其中的n个节点挂掉。在任何时刻，任何节点都会处在下列三种状态之一，Leader，Follower以及Candidate。在系统正常运行的过程中，系统中会有一个Leader，其他节点都处于Follower状态，Leader负责处理所有来自客户端的请求，Follower如果收到请求会将请求路由到Leader，Follower只是被动的接收leader和candidate的请求，它自己不会对外发出请求。而candidate是用于做leader选举的状态。正常情况下leader会向follower汇报心跳，证明自己是当前系统的leader，这样所有follower就会老老实实负责同步leader的日志内容变更。当一段时间(随机时间)follower收不到leader的心跳信息时，会认为此时系统处于无leader状态，那么自己会转换到candidate状态并发起leader选举。\nraft将整个时间分为若干个长度不一的片段，每一个段叫做一个任期(term)，一次新的选主操作会触发一次term的更新，这里term就可以理解为逻辑时钟的概念。raft规定，一个term最多只有一个leader，可能没有，这是因为可能多个follower同时发现没有leader同时发起选主，瓜分选票。节点间在通信过程中会交换term，这样做的目的是为了唯一确认当前应该是谁在当政。如果某个candidate或leader发现自己的term小于当前的就会自觉地退到follower状态。同样的如果某个节点收到包含过期term的请求，则会直接拒绝该请求。\nraft节点间使用RPC进行通信，基本的有关一致性算法的有两种基本RPC类型，分别为请求投票(RequestVotes)以及追加日志(AppendEntries),在日志压缩方面还有另外一种RPC类型(InstallSnapshot),后面会详细说明。其中日志中由若干个条目组成，每个条目都有一个Index标识。\n至此raft的所有概念都出来了，我简单列举一下：\n1 2 3 4 5 6 7 8 Leader:节点状态一种，用于处理所有来自client请求，并将自己的日志追加行为广播到所有的follower上 Follower:节点状态一种，用于接收leader心跳，将leader的日志变更同步到自己的状态机日志中，在选举时给candidate投票 Candidate:节点状态一种,当follower发现没有leader时发起选主请求，极有可能成为下一任leader term：用于标记当前leader/请求的有效性，一种逻辑时钟 Index：用于表示复制状态机中日志的条目 RequestVotes:candidate要求选主发送给Follower的RPC请求 AppendEntries:leader给follower发送的添加日志条目(心跳)的请求 InstallSnapshot:生成日志快照的RPC请求 1.3 Raft协议核心特性 文章中列举了五条raft的核心特性，也可以说这是raft设计的原则\n选举安全(Election Safety):在一个term中最多只能存在一个leader leader日志追加(Leader Append-Only):leader不能覆盖或删除日志中的内容，只能新增日志 日志匹配(Log Matching):如果两个日志有相同的index和任期，那么在这个任期前的所有日志条目全部相同 Leader强制完成(Leader Completeness):如果再某个任期内提交了某条日志条目，那么这个任期前面的日志也是确认被提交的 状态机确定性(State Machine Safety):如果一台服务器将某一个index的日志条目应用到自己的状态机上，那么其他服务器不可能在同一个index上应用不同的日志条目 1.4 Raft 角色 根据官方文档解释，一个 Raft 集群包含若干节点，Raft 把这些节点分为三种状态：Leader、 Follower、Candidate，每种状态负责的任务也是不一样的。正常情况下，集群中的节点只存在 Leader 与 Follower 两种状态。\nLeader（领导者）：负责日志的同步管理，处理来自客户端的请求，与Follower保持heartBeat的联系； Follower（追随者）：响应 Leader 的日志同步请求，响应Candidate的邀票请求，以及把客户端请求到Follower的事务转发（重定向）给Leader； Candidate（候选者）：负责选举投票，集群刚启动或者Leader宕机时，状态为Follower的节点将转为Candidate并发起选举，选举胜出（获得超过半数节点的投票）后，从Candidate转为Leader状态。 1.5 Raft 三个子问题 通常，Raft 集群中只有一个 Leader，其它节点都是 Follower。Follower 都是被动的，不会发送任何请求，只是简单地响应来自 Leader 或者 Candidate 的请求。Leader 负责处理所有的客户端请求（如果一个客户端和 Follower 联系，那么 Follower 会把请求重定向给 Leader）。\n为简化逻辑和实现，Raft 将一致性问题分解成了三个相对独立的子问题。\n选举（Leader Election）：当 Leader 宕机或者集群初创时，一个新的 Leader 需要被选举出来； 日志复制（Log Replication）：Leader 接收来自客户端的请求并将其以日志条目的形式复制到集群中的其它节点，并且强制要求其它节点的日志和自己保持一致； 安全性（Safety）：如果有任何的服务器节点已经应用了一个确定的日志条目到它的状态机中，那么其它服务器节点不能在同一个日志索引位置应用一个不同的指令。 2. Raft 算法之 Leader Election 原理 根据 Raft 协议，一个应用 Raft 协议的集群在刚启动时，所有节点的状态都是 Follower。由于没有 Leader，Followers 无法与 Leader 保持心跳（Heart Beat），因此，Followers 会认为 Leader 已经下线，进而转为 Candidate 状态。然后，Candidate 将向集群中其它节点请求投票，同意自己升级为 Leader。如果 Candidate 收到超过半数节点的投票（N/2 + 1），它将获胜成为 Leader。\n第一阶段：所有节点都是 Follower。 上面提到，一个应用 Raft 协议的集群在刚启动（或 Leader 宕机）时，所有节点的状态都是 Follower，初始 Term（任期）为 0。同时启动选举定时器，每个节点的选举定时器超时时间都在 100~500 毫秒之间且并不一致（避免同时发起选举）。 第二阶段：Follower 转为 Candidate 并发起投票。 没有 Leader，Followers 无法与 Leader 保持心跳（Heart Beat），节点启动后在一个选举定时器周期内未收到心跳和投票请求，则状态转为候选者 Candidate 状态，且 Term 自增，并向集群中所有节点发送投票请求并且重置选举定时器。\n注意，由于每个节点的选举定时器超时时间都在 100-500 毫秒之间，且彼此不一样，以避免所有 Follower 同时转为 Candidate 并同时发起投票请求。换言之，最先转为 Candidate 并发起投票请求的节点将具有成为 Leader 的“先发优势”。 第三阶段：投票策略。 节点收到投票请求后会根据以下情况决定是否接受投票请求：\n请求节点的 Term 大于自己的 Term，且自己尚未投票给其它节点，则接受请求，把票投给它； 请求节点的 Term 小于自己的 Term，且自己尚未投票，则拒绝请求，将票投给自己。 第四阶段：Candidate 转为 Leader。 一轮选举过后，正常情况下，会有一个 Candidate 收到超过半数节点（N/2 + 1）的投票，它将胜出并升级为 Leader。然后定时发送心跳给其它的节点，其它节点会转为 Follower 并与 Leader 保持同步，到此，本轮选举结束。\n注意：有可能一轮选举中，没有 Candidate 收到超过半数节点投票，那么将进行下一轮选举。 3. Raft 算法之 Log Replication 原理 在一个 Raft 集群中，只有 Leader 节点能够处理客户端的请求（如果客户端的请求发到了 Follower，Follower 将会把请求重定向到 Leader），客户端的每一个请求都包含一条被复制状态机执行的指令。Leader 把这条指令作为一条新的日志条目（Entry）附加到日志中去，然后并行得将附加条目发送给 Followers，让它们复制这条日志条目。\n当这条日志条目被 Followers 安全复制，Leader 会将这条日志条目应用到它的状态机中，然后把执行的结果返回给客户端。如果 Follower 崩溃或者运行缓慢，再或者网络丢包，Leader 会不断得重复尝试附加日志条目（尽管已经回复了客户端）直到所有的 Follower 都最终存储了所有的日志条目，确保强一致性。\n第一阶段：客户端请求提交到 Leader。 如下图所示，Leader 收到客户端的请求，比如存储数据 5。Leader 在收到请求后，会将它作为日志条目（Entry）写入本地日志中。需要注意的是，此时该 Entry 的状态是未提交（Uncommitted），Leader 并不会更新本地数据，因此它是不可读的。 第二阶段：Leader 将 Entry 发送到其它 Follower Leader 与 Floolwers 之间保持着心跳联系，随心跳 Leader 将追加的 Entry（AppendEntries）并行地发送给其它的 Follower，并让它们复制这条日志条目，这一过程称为复制（Replicate）。\n有几点需要注意：\n为什么 Leader 向 Follower 发送的 Entry 是 AppendEntries 呢？ 因为 Leader 与 Follower 的心跳是周期性的，而一个周期间 Leader 可能接收到多条客户端的请求，因此，随心跳向 Followers 发送的大概率是多个 Entry，即 AppendEntries。当然，在本例中，我们假设只有一条请求，自然也就是一个Entry了。\nLeader 向 Followers 发送的不仅仅是追加的 Entry（AppendEntries）。 在发送追加日志条目的时候，Leader 会把新的日志条目紧接着之前条目的索引位置（prevLogIndex）， Leader 任期号（Term）也包含在其中。如果 Follower 在它的日志中找不到包含相同索引位置和任期号的条目，那么它就会拒绝接收新的日志条目，因为出现这种情况说明 Follower 和 Leader 不一致。\n如何解决 Leader 与 Follower 不一致的问题？ 在正常情况下，Leader 和 Follower 的日志保持一致，所以追加日志的一致性检查从来不会失败。然而，Leader 和 Follower 一系列崩溃的情况会使它们的日志处于不一致状态。Follower可能会丢失一些在新的 Leader 中有的日志条目，它也可能拥有一些 Leader 没有的日志条目，或者两者都发生。丢失或者多出日志条目可能会持续多个任期。\n要使 Follower 的日志与 Leader 恢复一致，Leader 必须找到最后两者达成一致的地方（说白了就是回溯，找到两者最近的一致点），然后删除从那个点之后的所有日志条目，发送自己的日志给 Follower。所有的这些操作都在进行附加日志的一致性检查时完成。\nLeader 为每一个 Follower 维护一个 nextIndex，它表示下一个需要发送给 Follower 的日志条目的索引地址。当一个 Leader 刚获得权力的时候，它初始化所有的 nextIndex 值，为自己的最后一条日志的 index 加 1。如果一个 Follower 的日志和 Leader 不一致，那么在下一次附加日志时一致性检查就会失败。在被 Follower 拒绝之后，Leader 就会减小该 Follower 对应的 nextIndex 值并进行重试。最终 nextIndex 会在某个位置使得 Leader 和 Follower 的日志达成一致。当这种情况发生，附加日志就会成功，这时就会把 Follower 冲突的日志条目全部删除并且加上 Leader 的日志。一旦附加日志成功，那么 Follower 的日志就会和 Leader 保持一致，并且在接下来的任期继续保持一致。 第三阶段：Leader 等待 Followers 回应。 Followers 接收到 Leader 发来的复制请求后，有两种可能的回应：\n写入本地日志中，返回 Success； 一致性检查失败，拒绝写入，返回 False，原因和解决办法上面已做了详细说明。 需要注意的是，此时该 Entry 的状态也是未提交（Uncommitted）。完成上述步骤后，Followers 会向 Leader 发出 Success 的回应，当 Leader 收到大多数 Followers 的回应后，会将第一阶段写入的 Entry 标记为提交状态（Committed），并把这条日志条目应用到它的状态机中。 第四阶段：Leader 回应客户端。 完成前三个阶段后，Leader会向客户端回应 OK，表示写操作成功。 第五阶段，Leader 通知 Followers Entry 已提交 Leader 回应客户端后，将随着下一个心跳通知 Followers，Followers 收到通知后也会将 Entry 标记为提交状态。至此，Raft 集群超过半数节点已经达到一致状态，可以确保强一致性。\n需要注意的是，由于网络、性能、故障等各种原因导致“反应慢”、“不一致”等问题的节点，最终也会与 Leader 达成一致。 4. Raft 算法之安全性 前面描述了 Raft 算法是如何选举 Leader 和复制日志的。然而，到目前为止描述的机制并不能充分地保证每一个状态机会按照相同的顺序执行相同的指令。例如，一个 Follower 可能处于不可用状态，同时 Leader 已经提交了若干的日志条目；然后这个 Follower 恢复（尚未与 Leader 达成一致）而 Leader 故障；如果该 Follower 被选举为 Leader 并且覆盖这些日志条目，就会出现问题，即不同的状态机执行不同的指令序列。\n鉴于此，在 Leader 选举的时候需增加一些限制来完善 Raft 算法。这些限制可保证任何的 Leader 对于给定的任期号（Term），都拥有之前任期的所有被提交的日志条目（所谓 Leader 的完整特性）。关于这一选举时的限制，下文将详细说明。\n4.1 选举限制 在所有基于 Leader 机制的一致性算法中，Leader 都必须存储所有已经提交的日志条目。为了保障这一点，Raft 使用了一种简单而有效的方法，以保证所有之前的任期号中已经提交的日志条目在选举的时候都会出现在新的 Leader 中。换言之，日志条目的传送是单向的，只从 Leader 传给 Follower，并且 Leader 从不会覆盖自身本地日志中已经存在的条目。\nRaft 使用投票的方式来阻止一个 Candidate 赢得选举，除非这个 Candidate 包含了所有已经提交的日志条目。Candidate 为了赢得选举必须联系集群中的大部分节点。这意味着每一个已经提交的日志条目肯定存在于至少一个服务器节点上。如果 Candidate 的日志至少和大多数的服务器节点一样新（这个新的定义会在下面讨论），那么它一定持有了所有已经提交的日志条目（多数派的思想）。投票请求的限制中请求中包含了 Candidate 的日志信息，然后投票人会拒绝那些日志没有自己新的投票请求。\nRaft 通过比较两份日志中最后一条日志条目的索引值和任期号，确定谁的日志比较新。如果两份日志最后条目的任期号不同，那么任期号大的日志更加新。如果两份日志最后的条目任期号相同，那么日志比较长的那个就更加新。\n总结 - 选举时: 保证新的 Leader 拥有所有已经提交的日志\n每个 Follower 节点在投票时会检查 Candidate 的日志索引，并拒绝为日志不完整的 Candidate 投赞成票 半数以上的 Follower 节点都投了赞成票，意味着 Candidate 中包含了所有可能已经被提交的日志 4.2 提交之前任期内的日志条目 如同 4.1 节介绍的那样，Leader 知道一条当前任期内的日志记录是可以被提交的，只要它被复制到了大多数的 Follower 上（多数派的思想）。如果一个 Leader 在提交日志条目之前崩溃了，继任的 Leader 会继续尝试复制这条日志记录。然而，一个 Leader 并不能断定被保存到大多数 Follower 上的一个之前任期里的日志条目 就一定已经提交了。这很明显，从日志复制的过程可以看出。\n鉴于上述情况，Raft 算法不会通过计算副本数目的方式去提交一个之前任期内的日志条目。只有 Leader 当前任期里的日志条目通过计算副本数目可以被提交；一旦当前任期的日志条目以这种方式被提交，那么由于日志匹配特性，之前的日志条目也都会被间接的提交。在某些情况下，Leader 可以安全地知道一个老的日志条目是否已经被提交（只需判断该条目是否存储到所有节点上），但是 Raft 为了简化问题使用了一种更加保守的方法。\n当 Leader 复制之前任期里的日志时，Raft 会为所有日志保留原始的任期号，这在提交规则上产生了额外的复杂性。但是，这种策略更加容易辨别出日志，即使随着时间和日志的变化，日志仍维护着同一个任期编号。此外，该策略使得新 Leader 只需要发送较少日志条目。\n总结 - 提交日志时: Leader 只主动提交自己任期内产生的日志\n如果记录是当前 Leader 所创建的，那么当这条记录被复制到大多数节点上时，Leader 就可以提交这条记录以及之前的记录 如果记录是之前 Leader 所创建的，则只有当前 Leader 创建的记录被提交后，才能提交这些由之前 Leader 创建的日志 5. 集群成员变更 到目前为止，我们都假设集群的配置（加入到一致性算法的服务器集合）是固定不变的。但是在实践中，偶尔是会改变集群的配置的，例如替换那些宕机的机器或者改变复制级别。尽管可以通过暂停整个集群，更新所有配置，然后重启整个集群的方式来实现，但是在更改的时候集群会不可用。另外，如果存在手工操作步骤，那么就会有操作失误的风险。为了避免这样的问题，我们决定自动化配置改变并且将其纳入到 Raft 一致性算法中来。 为了让配置修改机制能够安全，那么在转换的过程中不能够存在任何时间点使得两个领导人同时被选举成功在同一个任期里。不幸的是，任何服务器直接从旧的配置直接转换到新的配置的方案都是不安全的。一次性原子地转换所有服务器是不可能的，所以在转换期间整个集群存在划分成两个独立的大多数群体的可能性（见图）。 直接从一种配置转到新的配置是十分不安全的，因为各个机器可能在任何的时候进行转换。在这个例子中，集群配额从 3 台机器变成了 5 台。不幸的是，存在这样的一个时间点，两个不同的领导人在同一个任期里都可以被选举成功。一个是通过旧的配置，一个通过新的配置。\n为了保证安全性，配置更改必须使用两阶段方法。目前有很多种两阶段的实现。例如，有些系统在第一阶段停掉旧的配置所以集群就不能处理客户端请求；然后在第二阶段在启用新的配置。在 Raft 中，集群先切换到一个过渡的配置，我们称之为共同一致；一旦共同一致已经被提交了，那么系统就切换到新的配置上。共同一致是老配置和新配置的结合：\n日志条目被复制给集群中新、老配置的所有服务器。 新、旧配置的服务器都可以成为领导人。 达成一致（针对选举和提交）需要分别在两种配置上获得大多数的支持。 共同一致允许独立的服务器在不影响安全性的前提下，在不同的时间进行配置转换过程。此外，共同一致可以让集群在配置转换的过程中依然响应客户端的请求。 集群配置在复制日志中以特殊的日志条目来存储和通信；图 11 展示了配置转换的过程。当一个领导人接收到一个改变配置从 C-old 到 C-new 的请求，他会为了共同一致存储配置（图中的 C-old,new），以前面描述的日志条目和副本的形式。一旦一个服务器将新的配置日志条目增加到它的日志中，他就会用这个配置来做出未来所有的决定（服务器总是使用最新的配置，无论他是否已经被提交）。这意味着领导人要使用 C-old,new 的规则来决定日志条目 C-old,new 什么时候需要被提交。如果领导人崩溃了，被选出来的新领导人可能是使用 C-old 配置也可能是 C-old,new 配置，这取决于赢得选举的候选人是否已经接收到了 C-old,new 配置。在任何情况下， C-new 配置在这一时期都不会单方面的做出决定。\n一旦 C-old,new 被提交，那么无论是 C-old 还是 C-new，在没有经过他人批准的情况下都不可能做出决定，并且领导人完全特性保证了只有拥有 C-old,new 日志条目的服务器才有可能被选举为领导人。这个时候，领导人创建一条关于 C-new 配置的日志条目并复制给集群就是安全的了。再者，每个服务器在见到新的配置的时候就会立即生效。当新的配置在 C-new 的规则下被提交，旧的配置就变得无关紧要，同时不使用新的配置的服务器就可以被关闭了。如图 11，C-old 和 C-new 没有任何机会同时做出单方面的决定；这保证了安全性。 图 11：一个配置切换的时间线。虚线表示已经被创建但是还没有被提交的配置日志条目，实线表示最后被提交的配置日志条目。领导人首先创建了 C-old,new 的配置条目在自己的日志中，并提交到 C-old,new 中（C-old 的大多数和 C-new 的大多数）。然后他创建 C-new 条目并提交到 C-new 中的大多数。这样就不存在 C-new 和 C-old 可以同时做出决定的时间点。\n在关于重新配置还有三个问题需要提出。 第一个问题是，新的服务器可能初始化没有存储任何的日志条目。当这些服务器以这种状态加入到集群中，那么他们需要一段时间来更新追赶，这时还不能提交新的日志条目。为了避免这种可用性的间隔时间，Raft 在配置更新之前使用了一种额外的阶段，在这个阶段，新的服务器以没有投票权身份加入到集群中来（领导人复制日志给他们，但是不考虑他们是大多数）。一旦新的服务器追赶上了集群中的其他机器，重新配置可以像上面描述的一样处理。\n第二个问题是，集群的领导人可能不是新配置的一员。在这种情况下，领导人就会在提交了 C-new 日志之后退位（回到跟随者状态）。这意味着有这样的一段时间，领导人管理着集群，但是不包括他自己；他复制日志但是不把他自己算作是大多数之一。当 C-new 被提交时，会发生领导人过渡，因为这时是最早新的配置可以独立工作的时间点（将总是能够在 C-new 配置下选出新的领导人）。在此之前，可能只能从 C-old 中选出领导人。\n第三个问题是，移除不在 C-new 中的服务器可能会扰乱集群。这些服务器将不会再接收到心跳，所以当选举超时，他们就会进行新的选举过程。他们会发送拥有新的任期号的请求投票 RPCs，这样会导致当前的领导人回退成跟随者状态。新的领导人最终会被选出来，但是被移除的服务器将会再次超时，然后这个过程会再次重复，导致整体可用性大幅降低。\n为了避免这个问题，当服务器确认当前领导人存在时，服务器会忽略请求投票 RPCs。特别的，当服务器在当前最小选举超时时间内收到一个请求投票 RPC，他不会更新当前的任期号或者投出选票。这不会影响正常的选举，每个服务器在开始一次选举之前，至少等待一个最小选举超时时间。然而，这有利于避免被移除的服务器扰乱：如果领导人能够发送心跳给集群，那么他就不会被更大的任期号废黜。\n6. 日志压缩 Raft 的日志在正常操作中不断的增长，但是在实际的系统中，日志不能无限制的增长。随着日志不断增长，他会占用越来越多的空间，花费越来越多的时间来重置。如果没有一定的机制去清除日志里积累的陈旧的信息，那么会带来可用性问题。\n快照是最简单的压缩方法。在快照系统中，整个系统的状态都以快照的形式写入到稳定的持久化存储中，然后到那个时间点之前的日志全部丢弃。快照技术被使用在 Chubby 和 ZooKeeper 中，接下来的章节会介绍 Raft 中的快照技术。\n增量压缩的方法，例如日志清理或者日志结构合并树，都是可行的。这些方法每次只对一小部分数据进行操作，这样就分散了压缩的负载压力。首先，他们先选择一个已经积累的大量已经被删除或者被覆盖对象的区域，然后重写那个区域还活跃的对象，之后释放那个区域。和简单操作整个数据集合的快照相比，需要增加复杂的机制来实现。状态机可以实现 LSM tree 使用和快照相同的接口，但是日志清除方法就需要修改 Raft 了。 图 12：一个服务器用新的快照替换了从 1 到 5 的条目，快照值存储了当前的状态。快照中包含了最后的索引位置和任期号。\n图 12 展示了 Raft 中快照的基础思想。每个服务器独立的创建快照，只包括已经被提交的日志。主要的工作包括将状态机的状态写入到快照中。Raft 也包含一些少量的元数据到快照中：最后被包含索引指的是被快照取代的最后的条目在日志中的索引值（状态机最后应用的日志），最后被包含的任期指的是该条目的任期号。保留这些数据是为了支持快照后紧接着的第一个条目的附加日志请求时的一致性检查，因为这个条目需要前一日志条目的索引值和任期号。为了支持集群成员更新（第 5 节），快照中也将最后的一次配置作为最后一个条目存下来。一旦服务器完成一次快照，他就可以删除最后索引位置之前的所有日志和快照了。\n尽管通常服务器都是独立的创建快照，但是领导人必须偶尔的发送快照给一些落后的跟随者。这通常发生在当领导人已经丢弃了下一条需要发送给跟随者的日志条目的时候。幸运的是这种情况不是常规操作：一个与领导人保持同步的跟随者通常都会有这个条目。然而一个运行非常缓慢的跟随者或者新加入集群的服务器(第5节)将不会有这个条目。这时让这个跟随者更新到最新的状态的方式就是通过网络把快照发送给他们。\n安装快照 RPC： 由领导人调用以将快照的分块发送给跟随者。领导者总是按顺序发送分块。\n参数 解释 term 领导人的任期号 leaderId 领导人的 Id，以便于跟随者重定向请求 lastIncludedIndex 快照中包含的最后日志条目的索引值 lastIncludedTerm 快照中包含的最后日志条目的任期号 offset 分块在快照中的字节偏移量 data[] 从偏移量开始的快照分块的原始字节 done 如果这是最后一个分块则为 true 结果 解释 term 当前任期号（currentTerm），便于领导人更新自己 接收者实现：\n如果term \u003c currentTerm就立即回复 如果是第一个分块（offset 为 0）就创建一个新的快照 在指定偏移量写入数据 如果 done 是 false，则继续等待更多的数据 保存快照文件，丢弃具有较小索引的任何现有或部分快照 如果现存的日志条目与快照中最后包含的日志条目具有相同的索引值和任期号，则保留其后的日志条目并进行回复 丢弃整个日志 使用快照重置状态机（并加载快照的集群配置） 在这种情况下领导人使用一种叫做安装快照的新的 RPC 来发送快照给太落后的跟随者；见图 13。当跟随者通过这种 RPC 接收到快照时，他必须自己决定对于已经存在的日志该如何处理。通常快照会包含没有在接收者日志中存在的信息。在这种情况下，跟随者丢弃其整个日志；它全部被快照取代，并且可能包含与快照冲突的未提交条目。如果接收到的快照是自己日志的前面部分（由于网络重传或者错误），那么被快照包含的条目将会被全部删除，但是快照后面的条目仍然有效，必须保留。\n这种快照的方式背离了 Raft 的强领导人原则，因为跟随者可以在不知道领导人情况下创建快照。但是我们认为这种背离是值得的。领导人的存在，是为了解决在达成一致性的时候的冲突，但是在创建快照的时候，一致性已经达成，这时不存在冲突了，所以没有领导人也是可以的。数据依然是从领导人传给跟随者，只是跟随者可以重新组织他们的数据了。\n我们考虑过一种替代的基于领导人的快照方案，即只有领导人创建快照，然后发送给所有的跟随者。但是这样做有两个缺点。第一，发送快照会浪费网络带宽并且延缓了快照处理的时间。每个跟随者都已经拥有了所有产生快照需要的信息，而且很显然，自己从本地的状态中创建快照比通过网络接收别人发来的要经济。第二，领导人的实现会更加复杂。例如，领导人需要发送快照的同时并行的将新的日志条目发送给跟随者，这样才不会阻塞新的客户端请求。\n还有两个问题影响了快照的性能。首先，服务器必须决定什么时候应该创建快照。如果快照创建的过于频繁，那么就会浪费大量的磁盘带宽和其他资源；如果创建快照频率太低，他就要承受耗尽存储容量的风险，同时也增加了从日志重建的时间。一个简单的策略就是当日志大小达到一个固定大小的时候就创建一次快照。如果这个阈值设置的显著大于期望的快照的大小，那么快照对磁盘压力的影响就会很小了。\n第二个影响性能的问题就是写入快照需要花费显著的一段时间，并且我们还不希望影响到正常操作。解决方案是通过写时复制的技术，这样新的更新就可以被接收而不影响到快照。例如，具有函数式数据结构的状态机天然支持这样的功能。另外，操作系统的写时复制技术的支持（如 Linux 上的 fork）可以被用来创建完整的状态机的内存快照（我们的实现就是这样的）。\n","wordCount":"743","inLanguage":"en","datePublished":"2023-03-16T19:34:52+08:00","dateModified":"2023-03-16T19:34:52+08:00","author":[{"@type":"Person","name":"Reid"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://reid00.github.io/en/posts/storage/raft-%E4%BB%8B%E7%BB%8D/"},"publisher":{"@type":"Organization","name":"Reid's Blog","logo":{"@type":"ImageObject","url":"https://reid00.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://reid00.github.io/en/ accesskey=h title="Reid's Blog (Alt + H)">Reid's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://reid00.github.io/en/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://reid00.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://reid00.github.io/en/categories/ title=Categorys><span>Categorys</span></a></li><li><a href=https://reid00.github.io/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://reid00.github.io/en/>Home</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/storage/>存储, 分布式相关的文章</a></div><h1 class=post-title>Raft 介绍</h1><div class=post-description>Raft 介绍</div><div class=post-meta><span title='2023-03-16 19:34:52 +0800 +0800'>2023-03-16 19:34</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;743 words&nbsp;·&nbsp;Reid</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-raft-%e7%ae%97%e6%b3%95%e7%ae%80%e4%bb%8b aria-label="1. Raft 算法简介">1. Raft 算法简介</a><ul><li><a href=#11-raft-%e8%83%8c%e6%99%af aria-label="1.1 Raft 背景">1.1 Raft 背景</a><ul><li><a href=#%e5%88%86%e5%b8%83%e5%bc%8f%e5%85%b1%e8%af%86%e7%ae%97%e6%b3%95%e6%a0%b8%e5%bf%83%e7%90%86%e8%ae%ba%e5%9f%ba%e7%a1%80 aria-label=分布式共识算法核心理论基础>分布式共识算法核心理论基础</a></li><li><a href=#%e5%88%86%e5%b8%83%e5%bc%8f%e4%b8%80%e8%87%b4%e6%80%a7%e7%ae%97%e6%b3%95%e9%80%9a%e5%b8%b8%e6%bb%a1%e8%b6%b3%e4%bb%a5%e4%b8%8b%e6%80%a7%e8%b4%a8 aria-label=分布式一致性算法，通常满足以下性质：>分布式一致性算法，通常满足以下性质：</a></li></ul></li><li><a href=#12-raft-%e5%9f%ba%e6%9c%ac%e6%a6%82%e5%bf%b5 aria-label="1.2 Raft 基本概念">1.2 Raft 基本概念</a></li><li><a href=#13-raft%e5%8d%8f%e8%ae%ae%e6%a0%b8%e5%bf%83%e7%89%b9%e6%80%a7 aria-label="1.3 Raft协议核心特性">1.3 Raft协议核心特性</a></li><li><a href=#14-raft-%e8%a7%92%e8%89%b2 aria-label="1.4 Raft 角色">1.4 Raft 角色</a></li><li><a href=#15-raft-%e4%b8%89%e4%b8%aa%e5%ad%90%e9%97%ae%e9%a2%98 aria-label="1.5 Raft 三个子问题">1.5 Raft 三个子问题</a></li></ul></li><li><a href=#2-raft-%e7%ae%97%e6%b3%95%e4%b9%8b-leader-election-%e5%8e%9f%e7%90%86 aria-label="2. Raft 算法之 Leader Election 原理">2. Raft 算法之 Leader Election 原理</a><ul><li><a href=#%e7%ac%ac%e4%b8%80%e9%98%b6%e6%ae%b5%e6%89%80%e6%9c%89%e8%8a%82%e7%82%b9%e9%83%bd%e6%98%af-follower aria-label="第一阶段：所有节点都是 Follower。">第一阶段：所有节点都是 Follower。</a></li><li><a href=#%e7%ac%ac%e4%ba%8c%e9%98%b6%e6%ae%b5follower-%e8%bd%ac%e4%b8%ba-candidate-%e5%b9%b6%e5%8f%91%e8%b5%b7%e6%8a%95%e7%a5%a8 aria-label="第二阶段：Follower 转为 Candidate 并发起投票。">第二阶段：Follower 转为 Candidate 并发起投票。</a></li><li><a href=#%e7%ac%ac%e4%b8%89%e9%98%b6%e6%ae%b5%e6%8a%95%e7%a5%a8%e7%ad%96%e7%95%a5 aria-label=第三阶段：投票策略。>第三阶段：投票策略。</a></li><li><a href=#%e7%ac%ac%e5%9b%9b%e9%98%b6%e6%ae%b5candidate-%e8%bd%ac%e4%b8%ba-leader aria-label="第四阶段：Candidate 转为 Leader。">第四阶段：Candidate 转为 Leader。</a></li></ul></li><li><a href=#3-raft-%e7%ae%97%e6%b3%95%e4%b9%8b-log-replication-%e5%8e%9f%e7%90%86 aria-label="3. Raft 算法之 Log Replication 原理">3. Raft 算法之 Log Replication 原理</a><ul><li><a href=#%e7%ac%ac%e4%b8%80%e9%98%b6%e6%ae%b5%e5%ae%a2%e6%88%b7%e7%ab%af%e8%af%b7%e6%b1%82%e6%8f%90%e4%ba%a4%e5%88%b0-leader aria-label="第一阶段：客户端请求提交到 Leader。">第一阶段：客户端请求提交到 Leader。</a></li><li><a href=#%e7%ac%ac%e4%ba%8c%e9%98%b6%e6%ae%b5leader-%e5%b0%86-entry-%e5%8f%91%e9%80%81%e5%88%b0%e5%85%b6%e5%ae%83-follower aria-label="第二阶段：Leader 将 Entry 发送到其它 Follower">第二阶段：Leader 将 Entry 发送到其它 Follower</a></li><li><a href=#%e7%ac%ac%e4%b8%89%e9%98%b6%e6%ae%b5leader-%e7%ad%89%e5%be%85-followers-%e5%9b%9e%e5%ba%94 aria-label="第三阶段：Leader 等待 Followers 回应。">第三阶段：Leader 等待 Followers 回应。</a></li><li><a href=#%e7%ac%ac%e5%9b%9b%e9%98%b6%e6%ae%b5leader-%e5%9b%9e%e5%ba%94%e5%ae%a2%e6%88%b7%e7%ab%af aria-label="第四阶段：Leader 回应客户端。">第四阶段：Leader 回应客户端。</a></li><li><a href=#%e7%ac%ac%e4%ba%94%e9%98%b6%e6%ae%b5leader-%e9%80%9a%e7%9f%a5-followers-entry-%e5%b7%b2%e6%8f%90%e4%ba%a4 aria-label="第五阶段，Leader 通知 Followers Entry 已提交">第五阶段，Leader 通知 Followers Entry 已提交</a></li></ul></li><li><a href=#4-raft-%e7%ae%97%e6%b3%95%e4%b9%8b%e5%ae%89%e5%85%a8%e6%80%a7 aria-label="4. Raft 算法之安全性">4. Raft 算法之安全性</a><ul><li><a href=#41-%e9%80%89%e4%b8%be%e9%99%90%e5%88%b6 aria-label="4.1 选举限制">4.1 选举限制</a></li><li><a href=#42-%e6%8f%90%e4%ba%a4%e4%b9%8b%e5%89%8d%e4%bb%bb%e6%9c%9f%e5%86%85%e7%9a%84%e6%97%a5%e5%bf%97%e6%9d%a1%e7%9b%ae aria-label="4.2 提交之前任期内的日志条目">4.2 提交之前任期内的日志条目</a></li></ul></li><li><a href=#5-%e9%9b%86%e7%be%a4%e6%88%90%e5%91%98%e5%8f%98%e6%9b%b4 aria-label="5. 集群成员变更">5. 集群成员变更</a></li><li><a href=#6-%e6%97%a5%e5%bf%97%e5%8e%8b%e7%bc%a9 aria-label="6. 日志压缩">6. 日志压缩</a><ul><li><a href=#%e5%ae%89%e8%a3%85%e5%bf%ab%e7%85%a7-rpc aria-label="安装快照 RPC：">安装快照 RPC：</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=1-raft-算法简介>1. Raft 算法简介<a hidden class=anchor aria-hidden=true href=#1-raft-算法简介>#</a></h1><h2 id=11-raft-背景>1.1 Raft 背景<a hidden class=anchor aria-hidden=true href=#11-raft-背景>#</a></h2><p>在分布式系统中，一致性算法至关重要。在所有一致性算法中，Paxos 最负盛名，它由莱斯利·兰伯特（Leslie Lamport）于 1990 年提出，是一种基于消息传递的一致性算法，被认为是类似算法中最有效的。</p><p>Paxos 算法虽然很有效，但复杂的原理使它实现起来非常困难，截止目前，实现 Paxos 算法的开源软件很少，比较出名的有 Chubby、LibPaxos。此外，Zookeeper 采用的 ZAB（Zookeeper Atomic Broadcast）协议也是基于 Paxos 算法实现的，不过 ZAB 对 Paxos 进行了很多改进与优化，两者的设计目标也存在差异——ZAB 协议主要用于构建一个高可用的分布式数据主备系统，而 Paxos 算法则是用于构建一个分布式的一致性状态机系统。</p><p>由于 Paxos 算法过于复杂、实现困难，极大地制约了其应用，而分布式系统领域又亟需一种高效而易于实现的分布式一致性算法，在此背景下，Raft 算法应运而生。</p><p>Raft 算法在斯坦福 Diego Ongaro 和 John Ousterhout 于 2013 年发表的《In Search of an Understandable Consensus Algorithm》中提出。相较于 Paxos，Raft 通过逻辑分离使其更容易理解和实现，目前，已经有十多种语言的 Raft 算法实现框架，较为出名的有 etcd、Consul 。</p><p>本文基于论文<a href=https://web.stanford.edu/~ouster/cgi-bin/papers/raft-atc14>In Search of an Understandable Consensus Algorithm</a>对raft协议进行分析，当然，还是建议读者直接看论文。</p><p>相关链接:</p><ul><li><a href=https://raft.github.io/raft.pdf>论文</a></li><li><a href=https://raft.github.io/>官网</a></li><li><a href=http://thesecretlivesofdata.com/raft/#overview>动画展示</a></li></ul><h3 id=分布式共识算法核心理论基础>分布式共识算法核心理论基础<a hidden class=anchor aria-hidden=true href=#分布式共识算法核心理论基础>#</a></h3><p>在正式谈raft之前，还需要简单介绍下分布式共识算法所基于的理论工具。分布式共识协议在复制状态机的背景下产生的。在该方法中，一组服务器上的状态机计算相同的副本，即便某台机器宕机依然会继续运行。复制状态机是基于日志实现的。在这里有必要唠叨两句日志的特性。日志可以看做一个简单的存储抽象，append only，按照时间完全有序，注意这里面的日志并不是log4j或是syslog打出来的业务日志，那个我们称之为应用日志，这里的日志是用于程序访问的存储结构。有了上面的限制，使用日志就能够保证这样一件事。如图所示
<img loading=lazy src=https://github.com/Reid00/image-host/raw/main/20221103/image.166fh20hq0tc.webp alt=img>
我有一个日志，里面存储的是一系列的对数据的操作，此时系统外部有一系列输入数据，输入到这个日志中，经过日志中一系列command操作，由于日志的确定性和有序性，保证最后得到的输出序列也应该是确定的。扩展到分布式的场景，此时每台机器上所有了这么一个日志，此时我需要做的事情就是保证这几份日志是完全一致的。详细步骤就引出了论文中的那张经典的复制状态机的示意图
<img loading=lazy src=https://github.com/Reid00/image-host/raw/main/20221103/image.3ykqxg1z71a0.webp alt=img></p><p>如图所示，server中的共识模块负责接收由client发送过来的请求，将请求中对应的操作记录到自己的日志中，同时通知给其他机器，让他们也进行同样的操作最终保证所有的机器都在日志中写入了这条操作。然后返回给客户端写入成功。复制状态机用于解决分布式中系统中的各种容错问题，例如master的高可用，例如Chubby以及ZK都是复制状态机，</p><h3 id=分布式一致性算法通常满足以下性质>分布式一致性算法，通常满足以下性质：<a hidden class=anchor aria-hidden=true href=#分布式一致性算法通常满足以下性质>#</a></h3><ul><li>在非拜占庭错误下，保证安全性(不会返回不正确的结果)</li><li>大多数机器运行，系统就可以正常运行，发生故障的机器在恢复正常后可以重新正常的加入到集群中</li><li>不依赖时序来保证日志的一致性</li><li>通常情况下，大多数机器就可以做出响应了，少数慢节点并不会拉低整个系统的性能</li></ul><h2 id=12-raft-基本概念>1.2 Raft 基本概念<a hidden class=anchor aria-hidden=true href=#12-raft-基本概念>#</a></h2><p>首先我将整体串一遍raft，然后抽提出里面的相关概念进行说明。</p><p>一个raft协议通常包含若干个节点，通常5个(2n+1),它最多允许其中的n个节点挂掉。在任何时刻，任何节点都会处在下列三种状态之一，Leader，Follower以及Candidate。在系统正常运行的过程中，系统中会有一个Leader，其他节点都处于Follower状态，Leader负责处理所有来自客户端的请求，Follower如果收到请求会将请求路由到Leader，Follower只是被动的接收leader和candidate的请求，它自己不会对外发出请求。而candidate是用于做leader选举的状态。正常情况下leader会向follower汇报心跳，证明自己是当前系统的leader，这样所有follower就会老老实实负责同步leader的日志内容变更。当一段时间(随机时间)follower收不到leader的心跳信息时，会认为此时系统处于无leader状态，那么自己会转换到candidate状态并发起leader选举。</p><p>raft将整个时间分为若干个长度不一的片段，每一个段叫做一个任期(term)，一次新的选主操作会触发一次term的更新，这里term就可以理解为逻辑时钟的概念。raft规定，一个term最多只有一个leader，可能没有，这是因为可能多个follower同时发现没有leader同时发起选主，瓜分选票。节点间在通信过程中会交换term，这样做的目的是为了唯一确认当前应该是谁在当政。如果某个candidate或leader发现自己的term小于当前的就会自觉地退到follower状态。同样的如果某个节点收到包含过期term的请求，则会直接拒绝该请求。</p><p>raft节点间使用RPC进行通信，基本的有关一致性算法的有两种基本RPC类型，分别为请求投票(RequestVotes)以及追加日志(AppendEntries),在日志压缩方面还有另外一种RPC类型(InstallSnapshot),后面会详细说明。其中日志中由若干个条目组成，每个条目都有一个Index标识。</p><p>至此raft的所有概念都出来了，我简单列举一下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Leader:节点状态一种，用于处理所有来自client请求，并将自己的日志追加行为广播到所有的follower上
</span></span><span class=line><span class=cl>Follower:节点状态一种，用于接收leader心跳，将leader的日志变更同步到自己的状态机日志中，在选举时给candidate投票
</span></span><span class=line><span class=cl>Candidate:节点状态一种,当follower发现没有leader时发起选主请求，极有可能成为下一任leader
</span></span><span class=line><span class=cl>term：用于标记当前leader/请求的有效性，一种逻辑时钟
</span></span><span class=line><span class=cl>Index：用于表示复制状态机中日志的条目
</span></span><span class=line><span class=cl>RequestVotes:candidate要求选主发送给Follower的RPC请求
</span></span><span class=line><span class=cl>AppendEntries:leader给follower发送的添加日志条目(心跳)的请求
</span></span><span class=line><span class=cl>InstallSnapshot:生成日志快照的RPC请求
</span></span></code></pre></td></tr></table></div></div><h2 id=13-raft协议核心特性>1.3 Raft协议核心特性<a hidden class=anchor aria-hidden=true href=#13-raft协议核心特性>#</a></h2><p>文章中列举了五条raft的核心特性，也可以说这是raft设计的原则</p><ul><li>选举安全(Election Safety):在一个term中最多只能存在一个leader</li><li>leader日志追加(Leader Append-Only):leader不能覆盖或删除日志中的内容，只能新增日志</li><li>日志匹配(Log Matching):如果两个日志有相同的index和任期，那么在这个任期前的所有日志条目全部相同</li><li>Leader强制完成(Leader Completeness):如果再某个任期内提交了某条日志条目，那么这个任期前面的日志也是确认被提交的</li><li>状态机确定性(State Machine Safety):如果一台服务器将某一个index的日志条目应用到自己的状态机上，那么其他服务器不可能在同一个index上应用不同的日志条目</li></ul><h2 id=14-raft-角色>1.4 Raft 角色<a hidden class=anchor aria-hidden=true href=#14-raft-角色>#</a></h2><p>根据官方文档解释，一个 Raft 集群包含若干节点，Raft 把这些节点分为三种状态：Leader、 Follower、Candidate，每种状态负责的任务也是不一样的。正常情况下，集群中的节点只存在 Leader 与 Follower 两种状态。</p><ul><li><code>Leader（领导者）</code>：负责日志的同步管理，处理来自客户端的请求，与Follower保持heartBeat的联系；</li><li><code>Follower（追随者）</code>：响应 Leader 的日志同步请求，响应Candidate的邀票请求，以及把客户端请求到Follower的事务转发（重定向）给Leader；</li><li><code>Candidate（候选者）</code>：负责选举投票，集群刚启动或者Leader宕机时，状态为Follower的节点将转为Candidate并发起选举，选举胜出（获得超过半数节点的投票）后，从Candidate转为Leader状态。</li></ul><h2 id=15-raft-三个子问题>1.5 Raft 三个子问题<a hidden class=anchor aria-hidden=true href=#15-raft-三个子问题>#</a></h2><p>通常，Raft 集群中只有一个 Leader，其它节点都是 Follower。Follower 都是被动的，不会发送任何请求，只是简单地响应来自 Leader 或者 Candidate 的请求。Leader 负责处理所有的客户端请求（如果一个客户端和 Follower 联系，那么 Follower 会把请求重定向给 Leader）。</p><p>为简化逻辑和实现，Raft 将一致性问题分解成了三个相对独立的子问题。</p><ul><li><code>选举（Leader Election）</code>：当 Leader 宕机或者集群初创时，一个新的 Leader 需要被选举出来；</li><li><code>日志复制（Log Replication）</code>：Leader 接收来自客户端的请求并将其以日志条目的形式复制到集群中的其它节点，并且强制要求其它节点的日志和自己保持一致；</li><li><code>安全性（Safety）</code>：如果有任何的服务器节点已经应用了一个确定的日志条目到它的状态机中，那么其它服务器节点不能在同一个日志索引位置应用一个不同的指令。</li></ul><h1 id=2-raft-算法之-leader-election-原理>2. Raft 算法之 Leader Election 原理<a hidden class=anchor aria-hidden=true href=#2-raft-算法之-leader-election-原理>#</a></h1><p>根据 Raft 协议，一个应用 Raft 协议的集群在刚启动时，所有节点的状态都是 Follower。由于没有 Leader，Followers 无法与 Leader 保持心跳（Heart Beat），因此，Followers 会认为 Leader 已经下线，进而转为 Candidate 状态。然后，Candidate 将向集群中其它节点请求投票，同意自己升级为 Leader。如果 Candidate 收到超过半数节点的投票（N/2 + 1），它将获胜成为 Leader。</p><h2 id=第一阶段所有节点都是-follower>第一阶段：所有节点都是 Follower。<a hidden class=anchor aria-hidden=true href=#第一阶段所有节点都是-follower>#</a></h2><p>上面提到，一个应用 Raft 协议的集群在刚启动（或 Leader 宕机）时，所有节点的状态都是 Follower，初始 Term（任期）为 0。同时启动选举定时器，每个节点的选举定时器超时时间都在 100~500 毫秒之间且并不一致（避免同时发起选举）。
<img loading=lazy src=https://github.com/Reid00/image-host/raw/main/20221103/image.627v9m3yzds0.webp alt=img></p><h2 id=第二阶段follower-转为-candidate-并发起投票>第二阶段：Follower 转为 Candidate 并发起投票。<a hidden class=anchor aria-hidden=true href=#第二阶段follower-转为-candidate-并发起投票>#</a></h2><p>没有 Leader，Followers 无法与 Leader 保持心跳（Heart Beat），节点启动后在一个选举定时器周期内未收到心跳和投票请求，则状态转为候选者 Candidate 状态，且 Term 自增，并向集群中所有节点发送投票请求并且重置选举定时器。</p><p>注意，由于每个节点的选举定时器超时时间都在 100-500 毫秒之间，且彼此不一样，以避免所有 Follower 同时转为 Candidate 并同时发起投票请求。换言之，最先转为 Candidate 并发起投票请求的节点将具有成为 Leader 的“先发优势”。
<img loading=lazy src=https://github.com/Reid00/image-host/raw/main/20221103/image.745nuwesy040.webp alt=img></p><h2 id=第三阶段投票策略>第三阶段：投票策略。<a hidden class=anchor aria-hidden=true href=#第三阶段投票策略>#</a></h2><p>节点收到投票请求后会根据以下情况决定是否接受投票请求：</p><ol><li>请求节点的 Term 大于自己的 Term，且自己尚未投票给其它节点，则接受请求，把票投给它；</li><li>请求节点的 Term 小于自己的 Term，且自己尚未投票，则拒绝请求，将票投给自己。
<img loading=lazy src=https://github.com/Reid00/image-host/raw/main/20221103/image.6txsudcjl6g0.webp alt=img></li></ol><h2 id=第四阶段candidate-转为-leader>第四阶段：Candidate 转为 Leader。<a hidden class=anchor aria-hidden=true href=#第四阶段candidate-转为-leader>#</a></h2><p>一轮选举过后，正常情况下，会有一个 Candidate 收到超过半数节点（N/2 + 1）的投票，它将胜出并升级为 Leader。然后定时发送心跳给其它的节点，其它节点会转为 Follower 并与 Leader 保持同步，到此，本轮选举结束。</p><p>注意：有可能一轮选举中，没有 Candidate 收到超过半数节点投票，那么将进行下一轮选举。
<img loading=lazy src=https://github.com/Reid00/image-host/raw/main/20221103/image.3vfbfh449dm0.webp alt=img></p><h1 id=3-raft-算法之-log-replication-原理>3. Raft 算法之 Log Replication 原理<a hidden class=anchor aria-hidden=true href=#3-raft-算法之-log-replication-原理>#</a></h1><p>在一个 Raft 集群中，只有 Leader 节点能够处理客户端的请求（如果客户端的请求发到了 Follower，Follower 将会把请求重定向到 Leader），客户端的每一个请求都包含一条被复制状态机执行的指令。Leader 把这条指令作为一条新的日志条目（Entry）附加到日志中去，然后并行得将附加条目发送给 Followers，让它们复制这条日志条目。</p><p>当这条日志条目被 Followers 安全复制，Leader 会将这条日志条目应用到它的状态机中，然后把执行的结果返回给客户端。如果 Follower 崩溃或者运行缓慢，再或者网络丢包，Leader 会不断得重复尝试附加日志条目（尽管已经回复了客户端）直到所有的 Follower 都最终存储了所有的日志条目，确保强一致性。</p><h2 id=第一阶段客户端请求提交到-leader>第一阶段：客户端请求提交到 Leader。<a hidden class=anchor aria-hidden=true href=#第一阶段客户端请求提交到-leader>#</a></h2><p>如下图所示，Leader 收到客户端的请求，比如存储数据 5。Leader 在收到请求后，会将它作为日志条目（Entry）写入本地日志中。需要注意的是，此时该 Entry 的状态是未提交（Uncommitted），Leader 并不会更新本地数据，因此它是不可读的。
<img loading=lazy src=https://github.com/Reid00/image-host/raw/main/20221103/image.5ogzl0uxuso0.webp alt=img></p><h2 id=第二阶段leader-将-entry-发送到其它-follower>第二阶段：Leader 将 Entry 发送到其它 Follower<a hidden class=anchor aria-hidden=true href=#第二阶段leader-将-entry-发送到其它-follower>#</a></h2><p>Leader 与 Floolwers 之间保持着心跳联系，随心跳 Leader 将追加的 Entry（AppendEntries）并行地发送给其它的 Follower，并让它们复制这条日志条目，这一过程称为复制（Replicate）。</p><p>有几点需要注意：</p><ol><li>为什么 Leader 向 Follower 发送的 Entry 是 AppendEntries 呢？</li></ol><p>因为 Leader 与 Follower 的心跳是周期性的，而一个周期间 Leader 可能接收到多条客户端的请求，因此，随心跳向 Followers 发送的大概率是多个 Entry，即 AppendEntries。当然，在本例中，我们假设只有一条请求，自然也就是一个Entry了。</p><ol start=2><li><p>Leader 向 Followers 发送的不仅仅是追加的 Entry（AppendEntries）。
在发送追加日志条目的时候，Leader 会把新的日志条目紧接着之前条目的索引位置（prevLogIndex）， Leader 任期号（Term）也包含在其中。如果 Follower 在它的日志中找不到包含相同索引位置和任期号的条目，那么它就会拒绝接收新的日志条目，因为出现这种情况说明 Follower 和 Leader 不一致。</p></li><li><p>如何解决 Leader 与 Follower 不一致的问题？
在正常情况下，Leader 和 Follower 的日志保持一致，所以追加日志的一致性检查从来不会失败。然而，Leader 和 Follower 一系列崩溃的情况会使它们的日志处于不一致状态。Follower可能会丢失一些在新的 Leader 中有的日志条目，它也可能拥有一些 Leader 没有的日志条目，或者两者都发生。丢失或者多出日志条目可能会持续多个任期。</p></li></ol><p>要使 Follower 的日志与 Leader 恢复一致，Leader 必须找到最后两者达成一致的地方（说白了就是回溯，找到两者最近的一致点），然后删除从那个点之后的所有日志条目，发送自己的日志给 Follower。所有的这些操作都在进行附加日志的一致性检查时完成。</p><p>Leader 为每一个 Follower 维护一个 nextIndex，它表示下一个需要发送给 Follower 的日志条目的索引地址。当一个 Leader 刚获得权力的时候，它初始化所有的 nextIndex 值，为自己的最后一条日志的 index 加 1。如果一个 Follower 的日志和 Leader 不一致，那么在下一次附加日志时一致性检查就会失败。在被 Follower 拒绝之后，Leader 就会减小该 Follower 对应的 nextIndex 值并进行重试。最终 nextIndex 会在某个位置使得 Leader 和 Follower 的日志达成一致。当这种情况发生，附加日志就会成功，这时就会把 Follower 冲突的日志条目全部删除并且加上 Leader 的日志。一旦附加日志成功，那么 Follower 的日志就会和 Leader 保持一致，并且在接下来的任期继续保持一致。
<img loading=lazy src=https://github.com/Reid00/image-host/raw/main/20221103/image.4bdnot6ocfm0.webp alt=img></p><h2 id=第三阶段leader-等待-followers-回应>第三阶段：Leader 等待 Followers 回应。<a hidden class=anchor aria-hidden=true href=#第三阶段leader-等待-followers-回应>#</a></h2><p>Followers 接收到 Leader 发来的复制请求后，有两种可能的回应：</p><ol><li>写入本地日志中，返回 Success；</li><li>一致性检查失败，拒绝写入，返回 False，原因和解决办法上面已做了详细说明。</li></ol><p>需要注意的是，此时该 Entry 的状态也是未提交（Uncommitted）。完成上述步骤后，Followers 会向 Leader 发出 Success 的回应，当 Leader 收到大多数 Followers 的回应后，会将第一阶段写入的 Entry 标记为提交状态（Committed），并把这条日志条目应用到它的状态机中。
<img loading=lazy src=https://github.com/Reid00/image-host/raw/main/20221103/image.725e1mym9xg0.webp alt=img></p><h2 id=第四阶段leader-回应客户端>第四阶段：Leader 回应客户端。<a hidden class=anchor aria-hidden=true href=#第四阶段leader-回应客户端>#</a></h2><p>完成前三个阶段后，Leader会向客户端回应 OK，表示写操作成功。
<img loading=lazy src=https://github.com/Reid00/image-host/raw/main/20221103/image.545w5lr447g0.webp alt=img></p><h2 id=第五阶段leader-通知-followers-entry-已提交>第五阶段，Leader 通知 Followers Entry 已提交<a hidden class=anchor aria-hidden=true href=#第五阶段leader-通知-followers-entry-已提交>#</a></h2><p>Leader 回应客户端后，将随着下一个心跳通知 Followers，Followers 收到通知后也会将 Entry 标记为提交状态。至此，Raft 集群超过半数节点已经达到一致状态，可以确保强一致性。</p><p>需要注意的是，由于网络、性能、故障等各种原因导致“反应慢”、“不一致”等问题的节点，最终也会与 Leader 达成一致。
<img loading=lazy src=https://github.com/Reid00/image-host/raw/main/20221103/image.v4c8gf50j40.webp alt=img></p><h1 id=4-raft-算法之安全性>4. Raft 算法之安全性<a hidden class=anchor aria-hidden=true href=#4-raft-算法之安全性>#</a></h1><p>前面描述了 Raft 算法是如何选举 Leader 和复制日志的。然而，到目前为止描述的机制并不能充分地保证每一个状态机会按照相同的顺序执行相同的指令。例如，一个 Follower 可能处于不可用状态，同时 Leader 已经提交了若干的日志条目；然后这个 Follower 恢复（尚未与 Leader 达成一致）而 Leader 故障；如果该 Follower 被选举为 Leader 并且覆盖这些日志条目，就会出现问题，即不同的状态机执行不同的指令序列。</p><p>鉴于此，在 Leader 选举的时候需增加一些限制来完善 Raft 算法。这些限制可保证任何的 Leader 对于给定的任期号（Term），都拥有之前任期的所有被提交的日志条目（所谓 Leader 的完整特性）。关于这一选举时的限制，下文将详细说明。</p><h2 id=41-选举限制>4.1 选举限制<a hidden class=anchor aria-hidden=true href=#41-选举限制>#</a></h2><p>在所有基于 Leader 机制的一致性算法中，Leader 都必须存储所有已经提交的日志条目。为了保障这一点，Raft 使用了一种简单而有效的方法，以保证所有之前的任期号中已经提交的日志条目在选举的时候都会出现在新的 Leader 中。换言之，日志条目的传送是单向的，只从 Leader 传给 Follower，并且 Leader 从不会覆盖自身本地日志中已经存在的条目。</p><p>Raft 使用投票的方式来阻止一个 Candidate 赢得选举，除非这个 Candidate 包含了所有已经提交的日志条目。Candidate 为了赢得选举必须联系集群中的大部分节点。这意味着每一个已经提交的日志条目肯定存在于至少一个服务器节点上。如果 Candidate 的日志至少和大多数的服务器节点一样新（这个新的定义会在下面讨论），那么它一定持有了所有已经提交的日志条目（多数派的思想）。投票请求的限制中请求中包含了 Candidate 的日志信息，然后投票人会拒绝那些日志没有自己新的投票请求。</p><p>Raft 通过比较两份日志中最后一条日志条目的索引值和任期号，确定谁的日志比较新。如果两份日志最后条目的任期号不同，那么任期号大的日志更加新。如果两份日志最后的条目任期号相同，那么日志比较长的那个就更加新。</p><p>总结 - 选举时:
<strong>保证新的 Leader 拥有所有已经提交的日志</strong></p><ul><li>每个 Follower 节点在投票时会检查 Candidate 的日志索引，并拒绝为日志不完整的 Candidate 投赞成票</li><li>半数以上的 Follower 节点都投了赞成票，意味着 Candidate 中包含了所有可能已经被提交的日志</li></ul><h2 id=42-提交之前任期内的日志条目>4.2 提交之前任期内的日志条目<a hidden class=anchor aria-hidden=true href=#42-提交之前任期内的日志条目>#</a></h2><p>如同 4.1 节介绍的那样，Leader 知道一条当前任期内的日志记录是可以被提交的，只要它被复制到了大多数的 Follower 上（多数派的思想）。如果一个 Leader 在提交日志条目之前崩溃了，继任的 Leader 会继续尝试复制这条日志记录。然而，一个 Leader 并不能断定被保存到大多数 Follower 上的一个之前任期里的日志条目 就一定已经提交了。这很明显，从日志复制的过程可以看出。</p><p>鉴于上述情况，Raft 算法不会通过计算副本数目的方式去提交一个之前任期内的日志条目。只有 Leader 当前任期里的日志条目通过计算副本数目可以被提交；一旦当前任期的日志条目以这种方式被提交，那么由于日志匹配特性，之前的日志条目也都会被间接的提交。在某些情况下，Leader 可以安全地知道一个老的日志条目是否已经被提交（只需判断该条目是否存储到所有节点上），但是 Raft 为了简化问题使用了一种更加保守的方法。</p><p>当 Leader 复制之前任期里的日志时，Raft 会为所有日志保留原始的任期号，这在提交规则上产生了额外的复杂性。但是，这种策略更加容易辨别出日志，即使随着时间和日志的变化，日志仍维护着同一个任期编号。此外，该策略使得新 Leader 只需要发送较少日志条目。</p><p>总结 - 提交日志时:
<strong>Leader 只主动提交自己任期内产生的日志</strong></p><ul><li>如果记录是当前 Leader 所创建的，那么当这条记录被复制到大多数节点上时，Leader 就可以提交这条记录以及之前的记录</li><li>如果记录是之前 Leader 所创建的，则只有当前 Leader 创建的记录被提交后，才能提交这些由之前 Leader 创建的日志</li></ul><h1 id=5-集群成员变更>5. 集群成员变更<a hidden class=anchor aria-hidden=true href=#5-集群成员变更>#</a></h1><p>到目前为止，我们都假设集群的配置（加入到一致性算法的服务器集合）是固定不变的。但是在实践中，偶尔是会改变集群的配置的，例如替换那些宕机的机器或者改变复制级别。尽管可以通过暂停整个集群，更新所有配置，然后重启整个集群的方式来实现，但是在更改的时候集群会不可用。另外，如果存在手工操作步骤，那么就会有操作失误的风险。为了避免这样的问题，我们决定自动化配置改变并且将其纳入到 Raft 一致性算法中来。 为了让配置修改机制能够安全，那么在转换的过程中不能够存在任何时间点使得两个领导人同时被选举成功在同一个任期里。不幸的是，任何服务器直接从旧的配置直接转换到新的配置的方案都是不安全的。一次性原子地转换所有服务器是不可能的，所以在转换期间整个集群存在划分成两个独立的大多数群体的可能性（见图）。
<img loading=lazy src=https://github.com/Reid00/image-host/raw/main/20221103/image.1hzef0ak1rs0.webp alt=img10></p><blockquote><p>直接从一种配置转到新的配置是十分不安全的，因为各个机器可能在任何的时候进行转换。在这个例子中，集群配额从 3 台机器变成了 5 台。不幸的是，存在这样的一个时间点，两个不同的领导人在同一个任期里都可以被选举成功。一个是通过旧的配置，一个通过新的配置。</p></blockquote><p>为了保证安全性，配置更改必须使用两阶段方法。目前有很多种两阶段的实现。例如，有些系统在第一阶段停掉旧的配置所以集群就不能处理客户端请求；然后在第二阶段在启用新的配置。在 Raft 中，集群先切换到一个过渡的配置，我们称之为共同一致；一旦共同一致已经被提交了，那么系统就切换到新的配置上。共同一致是老配置和新配置的结合：</p><ul><li>日志条目被复制给集群中新、老配置的所有服务器。</li><li>新、旧配置的服务器都可以成为领导人。</li><li>达成一致（针对选举和提交）需要分别在两种配置上获得大多数的支持。</li><li>共同一致允许独立的服务器在不影响安全性的前提下，在不同的时间进行配置转换过程。此外，共同一致可以让集群在配置转换的过程中依然响应客户端的请求。</li></ul><p>集群配置在复制日志中以特殊的日志条目来存储和通信；图 11 展示了配置转换的过程。当一个领导人接收到一个改变配置从 C-old 到 C-new 的请求，他会为了共同一致存储配置（图中的 C-old,new），以前面描述的日志条目和副本的形式。一旦一个服务器将新的配置日志条目增加到它的日志中，他就会用这个配置来做出未来所有的决定（服务器总是使用最新的配置，无论他是否已经被提交）。这意味着领导人要使用 C-old,new 的规则来决定日志条目 C-old,new 什么时候需要被提交。如果领导人崩溃了，被选出来的新领导人可能是使用 C-old 配置也可能是 C-old,new 配置，这取决于赢得选举的候选人是否已经接收到了 C-old,new 配置。在任何情况下， C-new 配置在这一时期都不会单方面的做出决定。</p><p>一旦 C-old,new 被提交，那么无论是 C-old 还是 C-new，在没有经过他人批准的情况下都不可能做出决定，并且领导人完全特性保证了只有拥有 C-old,new 日志条目的服务器才有可能被选举为领导人。这个时候，领导人创建一条关于 C-new 配置的日志条目并复制给集群就是安全的了。再者，每个服务器在见到新的配置的时候就会立即生效。当新的配置在 C-new 的规则下被提交，旧的配置就变得无关紧要，同时不使用新的配置的服务器就可以被关闭了。如图 11，C-old 和 C-new 没有任何机会同时做出单方面的决定；这保证了安全性。
<img loading=lazy src=https://github.com/Reid00/image-host/raw/main/20221103/image.4hzmnos2kgw.webp alt=img11></p><blockquote><p>图 11：一个配置切换的时间线。虚线表示已经被创建但是还没有被提交的配置日志条目，实线表示最后被提交的配置日志条目。领导人首先创建了 C-old,new 的配置条目在自己的日志中，并提交到 C-old,new 中（C-old 的大多数和 C-new 的大多数）。然后他创建 C-new 条目并提交到 C-new 中的大多数。这样就不存在 C-new 和 C-old 可以同时做出决定的时间点。</p></blockquote><p>在关于重新配置还有三个问题需要提出。
第一个问题是，新的服务器可能初始化没有存储任何的日志条目。当这些服务器以这种状态加入到集群中，那么他们需要一段时间来更新追赶，这时还不能提交新的日志条目。为了避免这种可用性的间隔时间，Raft 在配置更新之前使用了一种额外的阶段，在这个阶段，新的服务器以没有投票权身份加入到集群中来（领导人复制日志给他们，但是不考虑他们是大多数）。一旦新的服务器追赶上了集群中的其他机器，重新配置可以像上面描述的一样处理。</p><p>第二个问题是，集群的领导人可能不是新配置的一员。在这种情况下，领导人就会在提交了 C-new 日志之后退位（回到跟随者状态）。这意味着有这样的一段时间，领导人管理着集群，但是不包括他自己；他复制日志但是不把他自己算作是大多数之一。当 C-new 被提交时，会发生领导人过渡，因为这时是最早新的配置可以独立工作的时间点（将总是能够在 C-new 配置下选出新的领导人）。在此之前，可能只能从 C-old 中选出领导人。</p><p>第三个问题是，移除不在 C-new 中的服务器可能会扰乱集群。这些服务器将不会再接收到心跳，所以当选举超时，他们就会进行新的选举过程。他们会发送拥有新的任期号的请求投票 RPCs，这样会导致当前的领导人回退成跟随者状态。新的领导人最终会被选出来，但是被移除的服务器将会再次超时，然后这个过程会再次重复，导致整体可用性大幅降低。</p><p>为了避免这个问题，当服务器确认当前领导人存在时，服务器会忽略请求投票 RPCs。特别的，当服务器在当前最小选举超时时间内收到一个请求投票 RPC，他不会更新当前的任期号或者投出选票。这不会影响正常的选举，每个服务器在开始一次选举之前，至少等待一个最小选举超时时间。然而，这有利于避免被移除的服务器扰乱：如果领导人能够发送心跳给集群，那么他就不会被更大的任期号废黜。</p><h1 id=6-日志压缩>6. 日志压缩<a hidden class=anchor aria-hidden=true href=#6-日志压缩>#</a></h1><p>Raft 的日志在正常操作中不断的增长，但是在实际的系统中，日志不能无限制的增长。随着日志不断增长，他会占用越来越多的空间，花费越来越多的时间来重置。如果没有一定的机制去清除日志里积累的陈旧的信息，那么会带来可用性问题。</p><p>快照是最简单的压缩方法。在快照系统中，整个系统的状态都以快照的形式写入到稳定的持久化存储中，然后到那个时间点之前的日志全部丢弃。快照技术被使用在 Chubby 和 ZooKeeper 中，接下来的章节会介绍 Raft 中的快照技术。</p><p>增量压缩的方法，例如日志清理或者日志结构合并树，都是可行的。这些方法每次只对一小部分数据进行操作，这样就分散了压缩的负载压力。首先，他们先选择一个已经积累的大量已经被删除或者被覆盖对象的区域，然后重写那个区域还活跃的对象，之后释放那个区域。和简单操作整个数据集合的快照相比，需要增加复杂的机制来实现。状态机可以实现 LSM tree 使用和快照相同的接口，但是日志清除方法就需要修改 Raft 了。
<img loading=lazy src=https://github.com/Reid00/image-host/raw/main/20221103/image.adzdl5814os.webp alt=img12></p><blockquote><p>图 12：一个服务器用新的快照替换了从 1 到 5 的条目，快照值存储了当前的状态。快照中包含了最后的索引位置和任期号。</p></blockquote><p>图 12 展示了 Raft 中快照的基础思想。每个服务器独立的创建快照，只包括已经被提交的日志。主要的工作包括将状态机的状态写入到快照中。Raft 也包含一些少量的元数据到快照中：<code>最后被包含索引</code>指的是被快照取代的最后的条目在日志中的索引值（状态机最后应用的日志），<code>最后被包含的任期</code>指的是该条目的任期号。保留这些数据是为了支持快照后紧接着的第一个条目的附加日志请求时的一致性检查，因为这个条目需要前一日志条目的索引值和任期号。为了支持集群成员更新（第 5 节），快照中也将最后的一次配置作为最后一个条目存下来。一旦服务器完成一次快照，他就可以删除最后索引位置之前的所有日志和快照了。</p><p>尽管通常服务器都是独立的创建快照，但是领导人必须偶尔的发送快照给一些落后的跟随者。这通常发生在当领导人已经丢弃了下一条需要发送给跟随者的日志条目的时候。幸运的是这种情况不是常规操作：一个与领导人保持同步的跟随者通常都会有这个条目。然而一个运行非常缓慢的跟随者或者新加入集群的服务器(第5节)将不会有这个条目。这时让这个跟随者更新到最新的状态的方式就是通过网络把快照发送给他们。</p><h2 id=安装快照-rpc>安装快照 RPC：<a hidden class=anchor aria-hidden=true href=#安装快照-rpc>#</a></h2><p>由领导人调用以将快照的分块发送给跟随者。领导者总是按顺序发送分块。</p><table><thead><tr><th>参数</th><th>解释</th></tr></thead><tbody><tr><td>term</td><td>领导人的任期号</td></tr><tr><td>leaderId</td><td>领导人的 Id，以便于跟随者重定向请求</td></tr><tr><td>lastIncludedIndex</td><td>快照中包含的最后日志条目的索引值</td></tr><tr><td>lastIncludedTerm</td><td>快照中包含的最后日志条目的任期号</td></tr><tr><td>offset</td><td>分块在快照中的字节偏移量</td></tr><tr><td>data[]</td><td>从偏移量开始的快照分块的原始字节</td></tr><tr><td>done</td><td>如果这是最后一个分块则为 true</td></tr></tbody></table><table><thead><tr><th>结果</th><th>解释</th></tr></thead><tbody><tr><td>term</td><td>当前任期号（currentTerm），便于领导人更新自己</td></tr></tbody></table><p><strong>接收者实现：</strong></p><ul><li>如果term &lt; currentTerm就立即回复</li><li>如果是第一个分块（offset 为 0）就创建一个新的快照</li><li>在指定偏移量写入数据</li><li>如果 done 是 false，则继续等待更多的数据</li><li>保存快照文件，丢弃具有较小索引的任何现有或部分快照</li><li>如果现存的日志条目与快照中最后包含的日志条目具有相同的索引值和任期号，则保留其后的日志条目并进行回复</li><li>丢弃整个日志</li><li>使用快照重置状态机（并加载快照的集群配置）</li></ul><p><img loading=lazy src=https://github.com/Reid00/image-host/raw/main/20221103/image.4d01p5ktpji0.webp alt=img13>
在这种情况下领导人使用一种叫做安装快照的新的 RPC 来发送快照给太落后的跟随者；见图 13。当跟随者通过这种 RPC 接收到快照时，他必须自己决定对于已经存在的日志该如何处理。通常快照会包含没有在接收者日志中存在的信息。在这种情况下，跟随者丢弃其整个日志；它全部被快照取代，并且可能包含与快照冲突的未提交条目。如果接收到的快照是自己日志的前面部分（由于网络重传或者错误），那么被快照包含的条目将会被全部删除，但是快照后面的条目仍然有效，必须保留。</p><p>这种快照的方式背离了 Raft 的强领导人原则，因为跟随者可以在不知道领导人情况下创建快照。但是我们认为这种背离是值得的。领导人的存在，是为了解决在达成一致性的时候的冲突，但是在创建快照的时候，一致性已经达成，这时不存在冲突了，所以没有领导人也是可以的。数据依然是从领导人传给跟随者，只是跟随者可以重新组织他们的数据了。</p><p>我们考虑过一种替代的基于领导人的快照方案，即只有领导人创建快照，然后发送给所有的跟随者。但是这样做有两个缺点。第一，发送快照会浪费网络带宽并且延缓了快照处理的时间。每个跟随者都已经拥有了所有产生快照需要的信息，而且很显然，自己从本地的状态中创建快照比通过网络接收别人发来的要经济。第二，领导人的实现会更加复杂。例如，领导人需要发送快照的同时并行的将新的日志条目发送给跟随者，这样才不会阻塞新的客户端请求。</p><p>还有两个问题影响了快照的性能。首先，服务器必须决定什么时候应该创建快照。如果快照创建的过于频繁，那么就会浪费大量的磁盘带宽和其他资源；如果创建快照频率太低，他就要承受耗尽存储容量的风险，同时也增加了从日志重建的时间。一个简单的策略就是当日志大小达到一个固定大小的时候就创建一次快照。如果这个阈值设置的显著大于期望的快照的大小，那么快照对磁盘压力的影响就会很小了。</p><p>第二个影响性能的问题就是写入快照需要花费显著的一段时间，并且我们还不希望影响到正常操作。解决方案是通过写时复制的技术，这样新的更新就可以被接收而不影响到快照。例如，具有函数式数据结构的状态机天然支持这样的功能。另外，操作系统的写时复制技术的支持（如 Linux 上的 fork）可以被用来创建完整的状态机的内存快照（我们的实现就是这样的）。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://reid00.github.io/en/tags/raft/>Raft</a></li><li><a href=https://reid00.github.io/en/tags/mit6.824/>MIT6.824</a></li><li><a href=https://reid00.github.io/en/tags/consensus/>Consensus</a></li><li><a href=https://reid00.github.io/en/tags/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/>共识算法</a></li></ul><nav class=paginav><a class=prev href=https://reid00.github.io/en/posts/os_network/http%E9%95%BF%E8%BF%9E%E6%8E%A5%E5%92%8Ctcp%E9%95%BF%E8%BF%9E%E6%8E%A5%E7%9A%84%E5%8C%BA%E5%88%AB/><span class=title>« Prev</span><br><span>Http长连接和TCP长连接的区别</span>
</a><a class=next href=https://reid00.github.io/en/posts/computation/spark%E5%86%85%E5%AD%98%E7%A9%BA%E9%97%B4%E7%AE%A1%E7%90%86/><span class=title>Next »</span><br><span>Spark内存空间管理</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://reid00.github.io/en/>Reid's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>