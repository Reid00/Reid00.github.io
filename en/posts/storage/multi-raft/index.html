<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Multi Raft | Reid's Blog</title>
<meta name=keywords content="Raft,MIT6.824,Consensus,共识算法"><meta name=description content="Multi Raft"><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/en/posts/storage/multi-raft/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><head><meta name=referrer content="no-referrer"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK",{anonymize_ip:!1})}</script><meta property="og:title" content="Multi Raft"><meta property="og:description" content="Multi Raft"><meta property="og:type" content="article"><meta property="og:url" content="https://reid00.github.io/en/posts/storage/multi-raft/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-16T19:34:57+08:00"><meta property="article:modified_time" content="2023-03-16T19:34:57+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="Multi Raft"><meta name=twitter:description content="Multi Raft"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://reid00.github.io/en/posts/"},{"@type":"ListItem","position":2,"name":"存储, 分布式相关的文章","item":"https://reid00.github.io/en/posts/storage/"},{"@type":"ListItem","position":3,"name":"Multi Raft","item":"https://reid00.github.io/en/posts/storage/multi-raft/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Multi Raft","name":"Multi Raft","description":"Multi Raft","keywords":["Raft","MIT6.824","Consensus","共识算法"],"articleBody":"Mulit Raft Group 通过对 Raft 协议的描述我们知道：用户在对一组 Raft 系统进行更新操作时必须先经过 Leader，再由 Leader 同步给大多数 Follower。而在实际运用中，一组 Raft 的 Leader 往往存在单点的流量瓶颈，流量高便无法承载，同时每个节点都是全量数据，所以会受到节点的存储限制而导致容量瓶颈，无法扩展。\nMulit Raft Group 正是通过把整个数据从横向做切分，分为多个 Region 来解决磁盘瓶颈，然后每个 Region 都对应有独立的 Leader 和一个或多个 Follower 的 Raft 组进行横向扩展，此时系统便有多个写入的节点，从而分担写入压力，图如下： 具体细节可以参考TiKV 的文章\nMulti-Raft需要解决的一些核心问题： 数据何如分片 分片中的数据越来越大，需要分裂产生更多的分片，组成更多Raft-Group 分片的调度，让负载在系统中更平均（分片副本的迁移，补全，Leader切换等等） 一个节点上，所有的Raft-Group复用链接（否则Raft副本之间两两建链，链接爆炸了） 如何处理stale的请求（例如Proposal和Apply的时候，当前的副本不是Leader、分裂了、被销毁了等等） Snapshot如何管理（限制Snapshot，避免带宽、CPU、IO资源被过度占用） 数据何如分片 通常的数据分片算法就是 Hash 和 Range，TiKV 使用的 Range 来对数据进行数据分片。为什么使用 Range，主要原因是能更好的将相同前缀的 key 聚合在一起，便于 scan 等操作，这个 Hash 是没法支持的，当然，在 split/merge 上面 Range 也比 Hash 好处理很多，很多时候只会涉及到元信息的修改，都不用大范围的挪动数据。\n当然，Range 有一个问题在于很有可能某一个 Region 会因为频繁的操作成为性能热点，当然也有一些优化的方式，譬如通过 PD 将这些 Region 调度到更好的机器上面，提供 Follower 分担读压力等。\n总之，在 TiKV 里面，我们使用 Range 来对数据进行切分，将其分成一个一个的 Raft Group，每一个 Raft Group，我们使用 Region 来表示。\n分片如何调度 Elasticell实现细节 作为参考: 这部分的思路就和TiKV完全一致了。PD负责调度指令的下发，PD通过心跳收集调度需要的数据，这些数据包括：节点上的分片的个数，分片中leader的个数，节点的存储空间，剩余存储空间等等。一些最基本的调度:\nPD发现分片的副本数目缺少了，寻找一个合适的节点，把副本补全 PD发现系统中节点之间的分片数相差较多，就会转移一些分片的副本，保持系统中所有节点的分片数目大致相同（存储均衡） PD发现系统中节点之间分片的Leader数目不太一致，就会转移一些副本的Leader，保持系统中所有节点的分片副本的Leader数目大致相同（读写请求均衡） 新的分片如何形成Raft-Group 假设这个分片1有三个副本分别运行在Node1,Node2,Node3三台机器上，其中Node1机器上的副本是Leader，分片的大小限制是1GB。\n当分片1管理的数据量超过1GB的时候，分片1就会分裂成2个分片，分裂后，分片1修改数据范围，更新Epoch，继续服务。\n分片2形也有三个副本，分别也在Node1，Node2，Node3上，这些是元信息，但是只有在Node1上存在真正被创建的副本实例，Node2，Node3并不知道这个信息。这个时候Node1上的副本会立即进行Campaign Leader的操作，这个时候，Node2和Node3会收到来自分片2的Vote的Raft消息(整个描述指的是Leader Election)，Node2，Node3发现分片2在自己的节点上并没有副本，那么就会检查这个消息的合法性和正确性，通过后，立即创建分片2的副本，刚创建的副本没有任何数据，创建完成后会响应这个Vote消息，也一定会选择Node1的副本为Leader，选举完成后，Node1的分片2的Leader会给Node2，Node3的副本直接发送Snapshot，最终这个新的Raft-Group形成并且对外服务。\n按照Raft的协议，分片2在Node1 的副本成为Leader后不应该直接给Node2，Node3发送snapshot，但是这里我们沿用了TiKV的设计，Raft初始化的Log Index是5，那么按照Raft协议，Node1上的副本需要给Node2，Node3发送AppendEntries，这个时候Node1上的副本发现Log Index小于5的Raft Log不存在，所以就会转为直接发送Snapshot。\nSnapshot如何管理 我们的底层存储引擎使用的是RocksDB，这是一个LSM的实现，支持对一个范围的数据进行Snapshot和Apply Snapshot，我们基于这个特性来做。Raft中有一个RPC用于发送Snapshot数据，但是如果把所有的数据放在这个RPC里面，那么会有很多问题：\n一个RPC的数据量太大（取决于一个分片管理的数据，可能上GB，内存吃不消） 如果失败，整体重试代价太大 难以流控 我们修改为这样：\nRaft的snapshot RPC中的数据存放，snapshot文件的元信息（包括分片的ID，当前Raft的Term，Index，Epoch等信息） 发送Raft snapshot的RPC后，异步发送具体数据文件 数据文件分Chunk发送，重试的代价小 发送 Chunk的链接和Raft RPC的链接不复用 限制并行发送的Chunk个数，避免snapshot文件发送影响正常的Raft RPC 接收Raft snapshot的分片副本阻塞，直到接收完毕完整的snapshot数据文件 如何处理stale的请求 由于分片的副本会被调度（转移，销毁），分片自身也会分裂（分裂后分片所管理的数据范围发生了变化），所以在Raft的Proposal和Apply的时候，我们需要检查Stale请求，如何做呢？其实还是蛮简单的，TiKV使用Epoch的概念，我们沿用了下来。一个分片的副本有2个Epoch，一个在分片的副本成员发生变化的时候递增，一个在分片数据范围发生变化的时候递增，在请求到来的时候记录当前的Epoch，在Proposal和Apply的阶段检查Epoch，让客户端重试Stale的请求。\n","wordCount":"2467","inLanguage":"en","datePublished":"2023-03-16T19:34:57+08:00","dateModified":"2023-03-16T19:34:57+08:00","author":[{"@type":"Person","name":"Reid"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://reid00.github.io/en/posts/storage/multi-raft/"},"publisher":{"@type":"Organization","name":"Reid's Blog","logo":{"@type":"ImageObject","url":"https://reid00.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://reid00.github.io/en/ accesskey=h title="Reid's Blog (Alt + H)">Reid's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://reid00.github.io/en/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://reid00.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://reid00.github.io/en/categories/ title=Categorys><span>Categorys</span></a></li><li><a href=https://reid00.github.io/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://reid00.github.io/en/>Home</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/en/posts/storage/>存储, 分布式相关的文章</a></div><h1 class=post-title>Multi Raft</h1><div class=post-description>Multi Raft</div><div class=post-meta><span title='2023-03-16 19:34:57 +0800 +0800'>2023-03-16 19:34</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;2467 words&nbsp;·&nbsp;Reid</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#mulit-raft-group aria-label="Mulit Raft Group">Mulit Raft Group</a></li><li><a href=#multi-raft%e9%9c%80%e8%a6%81%e8%a7%a3%e5%86%b3%e7%9a%84%e4%b8%80%e4%ba%9b%e6%a0%b8%e5%bf%83%e9%97%ae%e9%a2%98 aria-label=Multi-Raft需要解决的一些核心问题：>Multi-Raft需要解决的一些核心问题：</a><ul><li><a href=#%e6%95%b0%e6%8d%ae%e4%bd%95%e5%a6%82%e5%88%86%e7%89%87 aria-label=数据何如分片>数据何如分片</a></li><li><a href=#%e5%88%86%e7%89%87%e5%a6%82%e4%bd%95%e8%b0%83%e5%ba%a6 aria-label=分片如何调度>分片如何调度</a></li><li><a href=#%e6%96%b0%e7%9a%84%e5%88%86%e7%89%87%e5%a6%82%e4%bd%95%e5%bd%a2%e6%88%90raft-group aria-label=新的分片如何形成Raft-Group>新的分片如何形成Raft-Group</a></li><li><a href=#snapshot%e5%a6%82%e4%bd%95%e7%ae%a1%e7%90%86 aria-label=Snapshot如何管理>Snapshot如何管理</a></li><li><a href=#%e5%a6%82%e4%bd%95%e5%a4%84%e7%90%86stale%e7%9a%84%e8%af%b7%e6%b1%82 aria-label=如何处理stale的请求>如何处理stale的请求</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=mulit-raft-group>Mulit Raft Group<a hidden class=anchor aria-hidden=true href=#mulit-raft-group>#</a></h1><p>通过对 Raft 协议的描述我们知道：用户在对一组 Raft 系统进行更新操作时必须先经过 Leader，再由 Leader 同步给大多数 Follower。而在实际运用中，一组 Raft 的 Leader 往往存在单点的流量瓶颈，流量高便无法承载，同时每个节点都是全量数据，所以会受到节点的存储限制而导致容量瓶颈，无法扩展。</p><p><code>Mulit Raft Group</code> 正是通过把整个数据从横向做切分，分为多个 Region 来解决磁盘瓶颈，然后每个 Region 都对应有独立的 Leader 和一个或多个 Follower 的 Raft 组进行横向扩展，此时系统便有多个写入的节点，从而分担写入压力，图如下：
<img loading=lazy src=https://github.com/Reid00/image-host/raw/main/20230210/image.4whi69ph2q00.webp alt=multi-raft></p><p>具体细节可以参考TiKV 的<a href=https://cn.pingcap.com/blog/tidb-internal-1>文章</a></p><h1 id=multi-raft需要解决的一些核心问题>Multi-Raft需要解决的一些核心问题：<a hidden class=anchor aria-hidden=true href=#multi-raft需要解决的一些核心问题>#</a></h1><ol><li>数据何如分片</li><li>分片中的数据越来越大，需要分裂产生更多的分片，组成更多Raft-Group</li><li>分片的调度，让负载在系统中更平均（分片副本的迁移，补全，Leader切换等等）</li><li>一个节点上，所有的Raft-Group复用链接（否则Raft副本之间两两建链，链接爆炸了）</li><li>如何处理stale的请求（例如Proposal和Apply的时候，当前的副本不是Leader、分裂了、被销毁了等等）
Snapshot如何管理（限制Snapshot，避免带宽、CPU、IO资源被过度占用）</li></ol><h2 id=数据何如分片>数据何如分片<a hidden class=anchor aria-hidden=true href=#数据何如分片>#</a></h2><p>通常的数据分片算法就是 Hash 和 Range，TiKV 使用的 Range 来对数据进行数据分片。为什么使用 Range，主要原因是能更好的将相同前缀的 key 聚合在一起，便于 scan 等操作，这个 Hash 是没法支持的，当然，在 split/merge 上面 Range 也比 Hash 好处理很多，很多时候只会涉及到元信息的修改，都不用大范围的挪动数据。</p><p>当然，Range 有一个问题在于很有可能某一个 Region 会因为频繁的操作成为性能热点，当然也有一些优化的方式，譬如通过 PD 将这些 Region 调度到更好的机器上面，提供 Follower 分担读压力等。</p><p>总之，在 TiKV 里面，我们使用 Range 来对数据进行切分，将其分成一个一个的 Raft Group，每一个 Raft Group，我们使用 Region 来表示。</p><h2 id=分片如何调度>分片如何调度<a hidden class=anchor aria-hidden=true href=#分片如何调度>#</a></h2><p>Elasticell实现细节 作为参考:
这部分的思路就和TiKV完全一致了。PD负责调度指令的下发，PD通过心跳收集调度需要的数据，这些数据包括：节点上的分片的个数，分片中leader的个数，节点的存储空间，剩余存储空间等等。一些最基本的调度:</p><ol><li>PD发现分片的副本数目缺少了，寻找一个合适的节点，把副本补全</li><li>PD发现系统中节点之间的分片数相差较多，就会转移一些分片的副本，保持系统中所有节点的分片数目大致相同（存储均衡）</li><li>PD发现系统中节点之间分片的Leader数目不太一致，就会转移一些副本的Leader，保持系统中所有节点的分片副本的Leader数目大致相同（读写请求均衡）</li></ol><h2 id=新的分片如何形成raft-group>新的分片如何形成Raft-Group<a hidden class=anchor aria-hidden=true href=#新的分片如何形成raft-group>#</a></h2><p>假设这个分片1有三个副本分别运行在Node1,Node2,Node3三台机器上，其中Node1机器上的副本是Leader，分片的大小限制是1GB。</p><p>当分片1管理的数据量超过1GB的时候，分片1就会分裂成2个分片，分裂后，分片1修改数据范围，更新Epoch，继续服务。</p><p>分片2形也有三个副本，分别也在Node1，Node2，Node3上，这些是元信息，但是只有在Node1上存在真正被创建的副本实例，Node2，Node3并不知道这个信息。这个时候Node1上的副本会立即进行Campaign Leader的操作，这个时候，Node2和Node3会收到来自分片2的Vote的Raft消息(整个描述指的是Leader Election)，Node2，Node3发现分片2在自己的节点上并没有副本，那么就会检查这个消息的合法性和正确性，通过后，立即创建分片2的副本，刚创建的副本没有任何数据，创建完成后会响应这个Vote消息，也一定会选择Node1的副本为Leader，选举完成后，Node1的分片2的Leader会给Node2，Node3的副本直接发送Snapshot，最终这个新的Raft-Group形成并且对外服务。</p><p>按照Raft的协议，分片2在Node1 的副本成为Leader后不应该直接给Node2，Node3发送snapshot，但是这里我们沿用了TiKV的设计，Raft初始化的Log Index是5，那么按照Raft协议，Node1上的副本需要给Node2，Node3发送AppendEntries，这个时候Node1上的副本发现Log Index小于5的Raft Log不存在，所以就会转为直接发送Snapshot。</p><h2 id=snapshot如何管理>Snapshot如何管理<a hidden class=anchor aria-hidden=true href=#snapshot如何管理>#</a></h2><p>我们的底层存储引擎使用的是RocksDB，这是一个LSM的实现，支持对一个范围的数据进行Snapshot和Apply Snapshot，我们基于这个特性来做。Raft中有一个RPC用于发送Snapshot数据，但是如果把所有的数据放在这个RPC里面，那么会有很多问题：</p><ul><li>一个RPC的数据量太大（取决于一个分片管理的数据，可能上GB，内存吃不消）</li><li>如果失败，整体重试代价太大</li><li>难以流控</li></ul><p>我们修改为这样：</p><ul><li>Raft的snapshot RPC中的数据存放，snapshot文件的元信息（包括分片的ID，当前Raft的Term，Index，Epoch等信息）</li><li>发送Raft snapshot的RPC后，异步发送具体数据文件</li><li>数据文件分Chunk发送，重试的代价小</li><li>发送 Chunk的链接和Raft RPC的链接不复用</li><li>限制并行发送的Chunk个数，避免snapshot文件发送影响正常的Raft RPC</li><li>接收Raft snapshot的分片副本阻塞，直到接收完毕完整的snapshot数据文件</li></ul><h2 id=如何处理stale的请求>如何处理stale的请求<a hidden class=anchor aria-hidden=true href=#如何处理stale的请求>#</a></h2><p>由于分片的副本会被调度（转移，销毁），分片自身也会分裂（分裂后分片所管理的数据范围发生了变化），所以在Raft的Proposal和Apply的时候，我们需要检查Stale请求，如何做呢？其实还是蛮简单的，TiKV使用Epoch的概念，我们沿用了下来。一个分片的副本有2个Epoch，一个在分片的副本成员发生变化的时候递增，一个在分片数据范围发生变化的时候递增，在请求到来的时候记录当前的Epoch，在Proposal和Apply的阶段检查Epoch，让客户端重试Stale的请求。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://reid00.github.io/en/tags/raft/>Raft</a></li><li><a href=https://reid00.github.io/en/tags/mit6.824/>MIT6.824</a></li><li><a href=https://reid00.github.io/en/tags/consensus/>Consensus</a></li><li><a href=https://reid00.github.io/en/tags/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/>共识算法</a></li></ul><nav class=paginav><a class=prev href=https://reid00.github.io/en/posts/storage/mit6.824-2022-raft-%E4%B8%BA%E4%BB%80%E4%B9%88raft%E5%8D%8F%E8%AE%AE%E4%B8%8D%E8%83%BD%E6%8F%90%E4%BA%A4%E4%B9%8B%E5%89%8D%E4%BB%BB%E6%9C%9F%E7%9A%84%E6%97%A5%E5%BF%97/><span class=title>« Prev</span><br><span>MIT6.824 2022 Raft 为什么Raft协议不能提交之前任期的日志</span>
</a><a class=next href=https://reid00.github.io/en/posts/storage/mit6.824-2022-raft-lab2c-log-compaction/><span class=title>Next »</span><br><span>MIT6.824 2022 Raft Lab2C Log Compaction</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main></body></html>