<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GBDT | Reid's Blog</title><meta name=keywords content><meta name=description content="Reid's Personal Notes -- https://github.com/Reid00"><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/tags/gbdt/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://reid00.github.io/tags/gbdt/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK",{anonymize_ip:!1})}</script><meta property="og:title" content="GBDT"><meta property="og:description" content="Reid's Personal Notes -- https://github.com/Reid00"><meta property="og:type" content="website"><meta property="og:url" content="https://reid00.github.io/tags/gbdt/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="GBDT"><meta name=twitter:description content="Reid's Personal Notes -- https://github.com/Reid00"></head><body class=list id=top><head><meta name=referrer content="no-referrer"></head><main class=main><header class=page-header><div class=breadcrumbs><a href=https://reid00.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/tags/>Tags</a></div><h1>GBDT
<a href=index.xml title=RSS aria-label=RSS><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class="post-entry tag-entry"><header class=entry-header><h2>集成学习之GBD</h2></header><div class=entry-content><p>什么是GBDT 到底什么是梯度提升树？所谓的GBDT实际上就是：
GBDT = Gradient Descent + Boosting + Desicion Tree
与Adaboost算法类似，GBDT也是使用了前向分布算法的加法模型。只不过弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同。
在Adaboost算法中，我们是利用前一轮迭代弱学习器的误差率来更新训练集的权重。而Gradient Boosting是通过算梯度（gradient）来定位模型的不足。
https://mp.weixin.qq.com/s/rmStKvdHq-BOCJo8ZuvgfQ
最常用的决策树算法: RF, Adaboost, GBDT
https://mp.weixin.qq.com/s/tUl3zhVxLfUd7o06_1Zg2g
Xgboost 的优势和原理 原理: https://www.jianshu.com/p/920592e8bcd2
​ https://www.jianshu.com/p/ac1c12f3fba1
优势: https://snaildove.github.io/2018/10/02/get-started-XGBoost/
LightGBM 详解 https://blog.csdn.net/VariableX/article/details/106242202
GBDT分类算法流程 GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。
为了解决这个问题，主要有两个方法：
用指数损失函数，此时GBDT退化为Adaboost算法。 用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。 下面我们用对数似然损失函数的GBDT分类。而对于对数似然损失函数，又有二元分类和多元分类的区别。
sklearn中的GBDT调参大法 https://mp.weixin.qq.com/s/756Xsy0uhnb8_rheySqLLg
Boosting重要参数 分类和回归算法的参数大致相同，不同之处会指出。
n_estimators: 弱学习器的个数。个数太小容易欠拟合，个数太大容易过拟合。默认是100，在实际调参的过程中，常常将n_estimators和参数learning_rate一起考虑。
learning_rate: 每个弱学习器的权重缩减系数，也称作步长。如果我们在强学习器的迭代公式加上了正则化项：，则通过learning_rate来控制其权重。对于同样的训练集拟合效果，较小的learning_rate意味着需要更多的弱学习器。通常用二者一起决定算法的拟合效果。所以两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的补偿开始调参，默认是1。
subsample: 不放回抽样的子采样，取值为(0,1]。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。
init: 初始化时的弱学习器，即。如果我们对数据有先验知识，或者之前做过一些拟合，可以用init参数提供的学习器做初始化分类回归预测。一般情况下不输入，直接用训练集样本来做样本集的初始化分类回归预测。
loss: GBDT算法中的损失函数。分类模型和回归模型的损失函数是不一样。
对于回归模型，可以使用均方误差ls，绝对损失lad，Huber损失huber和分位数损失quantile，默认使用均方误差ls。如果数据的噪音点不多，用默认的均方差ls比较好；如果噪音点较多，则推荐用抗噪音的损失函数huber；而如果需要对训练集进行分段预测，则采用quantile。 对于分类模型，可以使用对数似然损失函数deviance和指数损失函数exponential。默认是对数似然损失函数deviance。在原理篇中对这些分类损失函数有详细的介绍。一般来说，推荐使用默认的"deviance"。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。 alpha: 这个参数只有回归算法有，当使用Huber损失huber和分位数损失quantile时，需要指定分位数的值。默认是0.9，如果噪音点较多，可以适当降低这个分位数的值。
弱学习器参数 GBDT使用了CART回归决策树，因此它的参数基本和决策树类似。
max_features: 划分时考虑的最大特征数，默认是"None"。默认时表示划分时考虑所有的特征数；如果是"log2"意味着划分时最多考虑个log2N特征；如果是"sqrt"或者"auto"意味着划分时最多考虑根号N个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比*N）取整后的特征数。其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的"None"就可以了，如果特征数非常多，可以灵活控制划分时考虑的最大特征数，以控制决策树的生成时间。 max_depth: 决策树最大深度。如果不输入，默认值是3。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。 min_samples_split: 内部节点再划分所需最小样本数。限制子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。默认是2，如果样本量数量级非常大，则增大这个值。 min_samples_leaf: 叶子节点最少样本数。限制叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。 min_weight_fraction_leaf: 叶子节点最小的样本权重和这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。 max_leaf_nodes: 最大叶子节点数。通过限制最大叶子节点数，可以防止过拟合，默认是None，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。 min_impurity_split: 节点划分最小不纯度。这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7。 GBDT有很多优点：...</p></div><footer class=entry-footer><span title='2023-03-16 19:35:27 +0800 +0800'>2023-03-16</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Reid</footer><a class=entry-link aria-label="post link to 集成学习之GBD" href=https://reid00.github.io/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Bgbdt/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>GBDT+LR</h2></header><div class=entry-content><p>概述 GBDT的加入，是为了弥补LR难以实现特征组合的缺点。
LR LR作为一个线性模型，以概率形式输出结果，在工业上得到了十分广泛的应用。 其具有简单快速高效，结果可解释，可以分布式计算。搭配L1，L2正则，可以有很好地鲁棒性以及挑选特征的能力。
但由于其简单，也伴随着拟合能力不足，无法做特征组合的缺点。
通过梯度下降法可以优化参数
可以称之上是 CTR 预估模型的开山鼻祖，也是工业界使用最为广泛的 CTR 预估模型
但是在CTR领域，单纯的LR虽然可以快速处理海量高维离散特征，但是由于线性模型的局限性，其在特征组合方面仍有不足，所以后续才发展出了FM来引入特征交叉。在此之前，业界也有使用GBDT来作为特征组合的工具，其结果输出给LR。
LR 优缺点 优点：由于 LR 模型简单，训练时便于并行化，在预测时只需要对特征进行线性加权，所以性能比较好，往往适合处理海量 id 类特征，用 id 类特征有一个很重要的好处，就是防止信息损失（相对于范化的 CTR 特征），对于头部资源会有更细致的描述。
缺点：LR 的缺点也很明显，首先对连续特征的处理需要先进行离散化，如上文所说，人工分桶的方式会引入多种问题。另外 LR 需要进行人工特征组合，这就需要开发者有非常丰富的领域经验，才能不走弯路。这样的模型迁移起来比较困难，换一个领域又需要重新进行大量的特征工程。
GBDT+LR 首先，GBDT是一堆树的组合，假设有k棵树 。 对于第i棵树 ，其存在 个叶子节点。而从根节点到叶子节点，可以认为是一条路径，这条路径是一些特征的组合，例如从根节点到某一个叶子节点的路径可能是“ ”这就是一组特征组合。到达这个叶子节点的样本都拥有这样的组合特征，而这个组合特征使得这个样本得到了GBDT的预测结果。 所以对于GBDT子树 ，会返回一个 维的one-hot向量 对于整个GBDT，会返回一个 维的向量 ，这个向量由0-1组成。
然后，这个 ,会作为输入，送进LR模型，最终输出结果
模型大致如图所示。上图中由两棵子树，分别有3和2个叶子节点。对于一个样本x，最终可以落入第一棵树的某一个叶子和第二棵树的某一个叶子，得到两个独热编码的结果例如 [0,0,1],[1,0]组合得[0,0,1,1,0]输入到LR模型最后输出结果。
由于LR善于处理离散特征，GBDT善于处理连续特征。所以也可以交由GBDT处理连续特征，输出结果拼接上离散特征一起输入LR。
讨论 至于GBDT为何不善于处理高维离散特征？
https://cloud.tencent.com/developer/article/1005416
缺点：对于海量的 id 类特征，GBDT 由于树的深度和棵树限制（防止过拟合），不能有效的存储；另外海量特征在也会存在性能瓶颈，经笔者测试，当 GBDT 的 one hot 特征大于 10 万维时，就必须做分布式的训练才能保证不爆内存。所以 GBDT 通常配合少量的反馈 CTR 特征来表达，这样虽然具有一定的范化能力，但是同时会有信息损失，对于头部资源不能有效的表达。
https://www.zhihu.com/question/35821566
后来思考后发现原因是因为现在的模型普遍都会带着正则项，而 lr 等线性模型的正则项是对权重的惩罚，也就是 W1一旦过大，惩罚就会很大，进一步压缩 W1的值，使他不至于过大，而树模型则不一样，树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于上面这种 case，树只需要一个节点就可以完美分割9990和10个样本，惩罚项极其之小....</p></div><footer class=entry-footer><span title='2023-03-16 19:35:17 +0800 +0800'>2023-03-16</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Reid</footer><a class=entry-link aria-label="post link to GBDT+LR" href=https://reid00.github.io/posts/ml/gbdt+lr/></a></article></main><footer class=footer><span>&copy; 2023 <a href=https://reid00.github.io/>Reid's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>