<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>LR on Reid&#39;s Blog</title>
    <link>https://reid00.github.io/tags/lr/</link>
    <description>Recent content in LR on Reid&#39;s Blog</description>
    <image>
      <url>https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png</url>
      <link>https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 08 Jun 2022 14:10:06 +0800</lastBuildDate><atom:link href="https://reid00.github.io/tags/lr/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GBDT and LR</title>
      <link>https://reid00.github.io/posts/ml/gbdt&#43;lr/</link>
      <pubDate>Wed, 08 Jun 2022 14:10:06 +0800</pubDate>
      
      <guid>https://reid00.github.io/posts/ml/gbdt&#43;lr/</guid>
      <description>概述 GBDT的加入，是为了弥补LR难以实现特征组合的缺点。
LR LR作为一个线性模型，以概率形式输出结果，在工业上得到了十分广泛的应用。 其具有简单快速高效，结果可解释，可以分布式计算。搭配L1，L2正则，可以有很好地鲁棒性以及挑选特征的能力。
但由于其简单，也伴随着拟合能力不足，无法做特征组合的缺点。
通过梯度下降法可以优化参数
可以称之上是 CTR 预估模型的开山鼻祖，也是工业界使用最为广泛的 CTR 预估模型
但是在CTR领域，单纯的LR虽然可以快速处理海量高维离散特征，但是由于线性模型的局限性，其在特征组合方面仍有不足，所以后续才发展出了FM来引入特征交叉。在此之前，业界也有使用GBDT来作为特征组合的工具，其结果输出给LR。
LR 优缺点 优点：由于 LR 模型简单，训练时便于并行化，在预测时只需要对特征进行线性加权，所以性能比较好，往往适合处理海量 id 类特征，用 id 类特征有一个很重要的好处，就是防止信息损失（相对于范化的 CTR 特征），对于头部资源会有更细致的描述。
缺点：LR 的缺点也很明显，首先对连续特征的处理需要先进行离散化，如上文所说，人工分桶的方式会引入多种问题。另外 LR 需要进行人工特征组合，这就需要开发者有非常丰富的领域经验，才能不走弯路。这样的模型迁移起来比较困难，换一个领域又需要重新进行大量的特征工程。
GBDT+LR 首先，GBDT是一堆树的组合，假设有k棵树 。 对于第i棵树 ，其存在 个叶子节点。而从根节点到叶子节点，可以认为是一条路径，这条路径是一些特征的组合，例如从根节点到某一个叶子节点的路径可能是“ ”这就是一组特征组合。到达这个叶子节点的样本都拥有这样的组合特征，而这个组合特征使得这个样本得到了GBDT的预测结果。 所以对于GBDT子树 ，会返回一个 维的one-hot向量 对于整个GBDT，会返回一个 维的向量 ，这个向量由0-1组成。
然后，这个 ,会作为输入，送进LR模型，最终输出结果
模型大致如图所示。上图中由两棵子树，分别有3和2个叶子节点。对于一个样本x，最终可以落入第一棵树的某一个叶子和第二棵树的某一个叶子，得到两个独热编码的结果例如 [0,0,1],[1,0]组合得[0,0,1,1,0]输入到LR模型最后输出结果。
由于LR善于处理离散特征，GBDT善于处理连续特征。所以也可以交由GBDT处理连续特征，输出结果拼接上离散特征一起输入LR。
讨论 至于GBDT为何不善于处理高维离散特征？
https://cloud.tencent.com/developer/article/1005416
缺点：对于海量的 id 类特征，GBDT 由于树的深度和棵树限制（防止过拟合），不能有效的存储；另外海量特征在也会存在性能瓶颈，经笔者测试，当 GBDT 的 one hot 特征大于 10 万维时，就必须做分布式的训练才能保证不爆内存。所以 GBDT 通常配合少量的反馈 CTR 特征来表达，这样虽然具有一定的范化能力，但是同时会有信息损失，对于头部资源不能有效的表达。
https://www.zhihu.com/question/35821566
后来思考后发现原因是因为现在的模型普遍都会带着正则项，而 lr 等线性模型的正则项是对权重的惩罚，也就是 W1一旦过大，惩罚就会很大，进一步压缩 W1的值，使他不至于过大，而树模型则不一样，树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于上面这种 case，树只需要一个节点就可以完美分割9990和10个样本，惩罚项极其之小.</description>
    </item>
    
    <item>
      <title>逻辑回归的常见面试题总结</title>
      <link>https://reid00.github.io/posts/ml/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/</link>
      <pubDate>Wed, 08 Jun 2022 09:27:08 +0800</pubDate>
      
      <guid>https://reid00.github.io/posts/ml/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/</guid>
      <description>1.简介 逻辑回归是面试当中非常喜欢问到的一个机器学习算法，因为表面上看逻辑回归形式上很简单，很好掌握，但是一问起来就容易懵逼。所以在面试的时候给大家的第一个建议不要说自己精通逻辑回归，非常容易被问倒，从而减分。下面总结了一些平常我在作为面试官面试别人和被别人面试的时候，经常遇到的一些问题。
Regression问题的常规步骤为：
寻找h函数（即假设估计的函数）； 构造J函数（损失函数）； 想办法使得J函数最小并求得回归参数（θ）； 数据拟合问题 2.正式介绍 如何凸显你是一个对逻辑回归已经非常了解的人呢。那就是用一句话概括它！逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。
这里面其实包含了5个点 1：逻辑回归的假设，2：逻辑回归的损失函数，3：逻辑回归的求解方法，4：逻辑回归的目的，5:逻辑回归如何分类。这些问题是考核你对逻辑回归的基本了解。
逻辑回归的基本假设 任何的模型都是有自己的假设，在这个假设下模型才是适用的。逻辑回归的第一个基本假设是**假设数据服从伯努利分布。**伯努利分布有一个简单的例子是抛硬币，抛中为正面的概率是pp,抛中为负面的概率是1−p1−p.在逻辑回归这个模型里面是假设 hθ(x)hθ(x) 为样本为正的概率，1−hθ(x)1−hθ(x)为样本为负的概率。那么整个模型可以描述为
hθ(x;θ)=phθ(x;θ)=p
逻辑回归的第二个假设是假设样本为正的概率是
p=11+e−θTxp=11+e−θTx
所以逻辑回归的最终形式
hθ(x;θ)=11+e−θTx
逻辑回归的求解方法 由于该极大似然函数无法直接求解，我们一般通过对该函数进行梯度下降来不断逼急最优解。在这个地方其实会有个加分的项，考察你对其他优化方法的了解。因为就梯度下降本身来看的话就有随机梯度下降，批梯度下降，small batch 梯度下降三种方式，面试官可能会问这三种方式的优劣以及如何选择最合适的梯度下降方式。
简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。
随机梯度下降是以高方差频繁更新，优点是使得sgd（随机梯度下降）会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。
如果使用梯度下降法(批量梯度下降法)，那么每次迭代过程中都要对 个样本进行求梯度，所以开销非常大，随机梯度下降的思想就是随机采样一个样本 来更新参数，那么计算开销就从 下降到 。
随机梯度下降虽然提高了计算效率，降低了计算开销，但是由于每次迭代只随机选择一个样本，因此随机性比较大，所以下降过程中非常曲折
可以看到多了随机两个字，随机也就是说我们用样本中的一个例子来近似我所有的样本，来调整θ，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，**对于最优化问题，凸问题，**虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。
小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。小批量梯度下降的开销为 其中 是批量大小。
其实这里还有一个隐藏的更加深的加分项，看你了不了解诸如Adam，动量法等优化方法。因为上述方法其实还有两个致命的问题。 第一个是如何对模型选择合适的学习率。自始至终保持同样的学习率其实不太合适。因为一开始参数刚刚开始学习的时候，此时的参数和最优解隔的比较远，需要保持一个较大的学习率尽快逼近最优解。但是学习到后面的时候，参数和最优解已经隔的比较近了，你还保持最初的学习率，容易越过最优点，在最优点附近来回振荡，通俗一点说，就很容易学过头了，跑偏了。 第二个是如何对参数选择合适的学习率。在实践中，对每个参数都保持的同样的学习率也是很不合理的。有些参数更新频繁，那么学习率可以适当小一点。有些参数更新缓慢，那么学习率就应该大一点。这里我们不展开，有空我会专门出一个专题介绍。 逻辑回归的目的 该函数的目的便是将数据二分类，提高准确率。 逻辑回归如何分类 逻辑回归作为一个回归(也就是y值是连续的)，如何应用到分类上去呢。y值确实是一个连续的变量。逻辑回归的做法是划定一个阈值，y值大于这个阈值的是一类，y值小于这个阈值的是另外一类。阈值具体如何调整根据实际情况选择。一般会选择0.5做为阈值来划分。 逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？ 损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。至于原因大家可以求出这个式子的梯度更新
这个式子的更新速度只和相关。和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。
为什么不选平方损失函数的呢？其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和sigmod函数本身的梯度是很相关的。sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。
逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？ 先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。
但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一
如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。
为什么我们还是会在训练的过程当中将高度相关的特征去掉？ 去掉高度相关的特征会让模型的可解释性更好 可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。 4.逻辑回归的优缺点总结 优点
形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。
训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。
资源占用小,尤其是内存。因为只需要存储各个维度的特征值，。
方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。
但是逻辑回归本身也有许多的缺点:
准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。
处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。</description>
    </item>
    
  </channel>
</rss>
