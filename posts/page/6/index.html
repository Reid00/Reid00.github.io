<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | Reid's Blog</title><meta name=keywords content><meta name=description content="Posts - Reid's Blog"><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/posts/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://reid00.github.io/posts/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK",{anonymize_ip:!1})}</script><meta property="og:title" content="Posts"><meta property="og:description" content="Reid's Personal Notes -- https://github.com/Reid00"><meta property="og:type" content="website"><meta property="og:url" content="https://reid00.github.io/posts/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="Posts"><meta name=twitter:description content="Reid's Personal Notes -- https://github.com/Reid00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://reid00.github.io/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://reid00.github.io/ accesskey=h title="Reid's Blog (Alt + H)">Reid's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://reid00.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://reid00.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://reid00.github.io/categories/ title=Categorys><span>Categorys</span></a></li><li><a href=https://reid00.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://reid00.github.io/>Home</a></div><h1>Posts
<a href=index.xml title=RSS aria-label=RSS><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class=post-entry><header class=entry-header><h2>集成学习之xgboost</h2></header><div class=entry-content><p>一、XGBoost和GBDT xgboost是一种集成学习算法，属于3类常用的集成方法(bagging,boosting,stacking)中的boosting算法类别。它是一个加法模型，基模型一般选择树模型，但也可以选择其它类型的模型如逻辑回归等。
xgboost属于梯度提升树(GBDT)模型这个范畴，GBDT的基本想法是让新的基模型（GBDT以CART分类回归树为基模型）去拟合前面模型的偏差，从而不断将加法模型的偏差降低。
相比于经典的GBDT，xgboost做了一些改进，从而在效果和性能上有明显的提升（划重点面试常考）。
第一，GBDT将目标函数泰勒展开到一阶，而xgboost将目标函数泰勒展开到了二阶。保留了更多有关目标函数的信息，对提升效果有帮助。
第二，GBDT是给新的基模型寻找新的拟合标签（前面加法模型的负梯度），而xgboost是给新的基模型寻找新的目标函数（目标函数关于新的基模型的二阶泰勒展开）。
第三，xgboost加入了和叶子权重的L2正则化项，因而有利于模型获得更低的方差。
**第四，xgboost增加了自动处理缺失值特征的策略。**通过把带缺失值样本分别划分到左子树或者右子树，比较两种方案下目标函数的优劣，从而自动对有缺失值的样本进行划分，无需对缺失特征进行填充预处理。
此外，xgboost还支持候选分位点切割，特征并行等，可以提升性能。
二、XGBoost原理概述 面从假设空间，目标函数，优化算法3个角度对xgboost的原理进行概括性的介绍。
1，假设空间
2，目标函数
3，优化算法
基本思想：贪心法，逐棵树进行学习，每棵树拟合之前模型的偏差。
三、第t棵树学什么？ 要完成构建xgboost模型，我们需要确定以下一些事情。
1，如何boost? 如果已经得到了前面t-1棵树构成的加法模型，如何确定第t棵树的学习目标？
2，如何生成树？已知第t棵树的学习目标的前提下，如何学习这棵树？具体又包括是否进行分裂？选择哪个特征进行分裂？选择什么分裂点位？分裂的叶子节点如何取值？
我们首先考虑如何boost的问题，顺便解决分裂的叶子节点如何取值的问题。
四、如何生成第t棵树？ xgboost采用二叉树，开始的时候，全部样本都在一个叶子节点上。然后叶子节点不断通过二分裂，逐渐生成一棵树。
xgboost使用levelwise的生成策略，即每次对同一层级的全部叶子节点尝试进行分裂。
对叶子节点分裂生成树的过程有几个基本的问题：是否要进行分裂？选择哪个特征进行分裂？在特征的什么点位进行分裂？以及分裂后新的叶子上取什么值？
叶子节点的取值问题前面已经解决了。我们重点讨论几个剩下的问题。
1，是否要进行分裂？ 根据树的剪枝策略的不同，这个问题有两种不同的处理。如果是预剪枝策略，那么只有当存在某种分裂方式使得分裂后目标函数发生下降，才会进行分裂。
但如果是后剪枝策略，则会无条件进行分裂，等树生成完成后，再从上而下检查树的各个分枝是否对目标函数下降产生正向贡献从而进行剪枝。
xgboost采用预剪枝策略，只有分裂后的增益大于0才会进行分裂。
2，选择什么特征进行分裂？
xgboost采用特征并行的方法进行计算选择要分裂的特征，即用多个线程，尝试把各个特征都作为分裂的特征，找到各个特征的最优分割点，计算根据它们分裂后产生的增益，选择增益最大的那个特征作为分裂的特征。
3，选择什么分裂点位？
xgboost选择某个特征的分裂点位的方法有两种，一种是全局扫描法，另一种是候选分位点法。 全局扫描法将所有样本该特征的取值按从小到大排列，将所有可能的分裂位置都试一遍，找到其中增益最大的那个分裂点，其计算复杂度和叶子节点上的样本特征不同的取值个数成正比。 而候选分位点法是一种近似算法，仅选择常数个（如256个）候选分裂位置，然后从候选分裂位置中找出最优的那个。
五、XGBoost算法原理小结 XGBoost（eXtreme Gradient Boosting）全名叫极端梯度提升，XGBoost是集成学习方法的王牌，在Kaggle数据挖掘比赛中，大部分获胜者用了XGBoost，XGBoost在绝大多数的回归和分类问题上表现的十分顶尖，本文较详细的介绍了XGBoost的算法原理。
目录
最优模型的构建方法
Boosting的回归思想
XGBoost的目标函数推导
XGBoost的回归树构建方法
XGBoost与GDBT的区别
最优模型的构建方法
构建最优模型的一般方法是最小化训练数据的损失函数，我们用字母 L表示，如下式：
式（1）称为经验风险最小化，训练得到的模型复杂度较高。当训练数据较小时，模型很容易出现过拟合问题。
因此，为了降低模型的复杂度，常采用下式：
其中J(f)为模型的复杂度，式（2）称为结构风险最小化，结构风险最小化的模型往往对训练数据以及未知的测试数据都有较好的预测 。
应用：决策树的生成和剪枝分别对应了经验风险最小化和结构风险最小化，XGBoost的决策树生成是结构风险最小化的结果，后续会详细介绍。
Boosting方法的回归思想
Boosting法是结合多个弱学习器给出最终的学习结果，不管任务是分类或回归，我们都用回归任务的思想来构建最优Boosting模型 。
回归思想：把每个弱学习器的输出结果当成连续值，这样做的目的是可以对每个弱学习器的结果进行累加处理，且能更好的利用损失函数来优化模型。
假设
是第 t 轮弱学习器的输出结果，
是模型的输出结果，
是实际输出结果，表达式如下：
上面两式就是加法模型，都默认弱学习器的输出结果是连续值。因为回归任务的弱学习器本身是连续值，所以不做讨论，下面详细介绍分类任务的回归思想。
分类任务的回归思想：
根据2.1式的结果，得到最终的分类器：
分类的损失函数一般选择指数函数或对数函数，这里假设损失函数为对数函数，学习器的损失函数是
若实际输出结果yi=1，则：
求（2.5）式对...</p></div><footer class=entry-footer><span title='2022-06-08 11:09:50 +0800 +0800'>2022-06-08</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Reid</footer><a class=entry-link aria-label="post link to 集成学习之xgboost" href=https://reid00.github.io/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Bxgboost/></a></article><article class=post-entry><header class=entry-header><h2>机器学习之常见损失函数</h2></header><div class=entry-content><p>简介 损失函数用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。
损失函数分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。
常见的损失函数以及其优缺点如下：
1. 0-1损失函数(zero-one loss) 0-1损失是指预测值和目标值不相等为1， 否则为0:
特点：
(1) 0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用.
(2) 感知机就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足 时认为相等，
2. 绝对值损失函数 绝对值损失函数是计算预测值与目标值的差的绝对值：
3. log对数损失函数 log对数损失函数的标准形式如下：
特点：
(1) log对数损失函数能非常好的表征概率分布，在很多场景尤其是多分类，如果需要知道结果属于每个类别的置信度，那它非常适合。
(2) 健壮性不强，相比于hinge loss对噪声更敏感。
(3) 辑回归的损失函数就是log对数损失函数。
4. 平方损失函数 平方损失函数标准形式如下：
特点：
(1)经常应用与回归问题
5. 指数损失函数（exponential loss） 指数损失函数的标准形式如下：
特点：
(1)对离群点、噪声非常敏感。经常用在AdaBoost算法中。
6. Hinge 损失函数 Hinge损失函数标准形式如下：
特点：
(1) hinge损失函数表示如果被分类正确，损失为0，否则损失就为 。SVM就是使用这个损失函数。
(2) 一般的 是预测值，在-1到1之间， 是目标值(-1或1)。其含义是， 的值在-1和+1之间就可以了，并不鼓励 ，即并不鼓励分类器过度自信，让某个正确分类的样本距离分割线超过1并不会有任何奖励，从而使分类器可以更专注于整体的误差。
(3) 健壮性相对较高，对异常点、噪声不敏感，但它没太好的概率解释。
7. 感知损失(perceptron loss)函数 感知损失函数的标准形式如下：
特点：
(1)是Hinge损失函数的一个变种，Hinge loss对判定边界附近的点(正确端)惩罚力度很高。而perceptron loss只要样本的判定类别正确的话，它就满意，不管其判定边界的距离。它比Hinge loss简单，因为不是max-margin boundary，所以模型的泛化能力没 hinge loss强。
8. 交叉熵损失函数 (Cross-entropy loss function) 交叉熵损失函数的标准形式如下:...</p></div><footer class=entry-footer><span title='2022-06-08 11:08:15 +0800 +0800'>2022-06-08</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Reid</footer><a class=entry-link aria-label="post link to 机器学习之常见损失函数" href=https://reid00.github.io/posts/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%B8%B8%E8%A7%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/></a></article><article class=post-entry><header class=entry-header><h2>MySql索引优化</h2></header><div class=entry-content><p>数据库表结构：
1 2 3 4 5 6 7 8 9 create table user ( id int primary key, name varchar(20), sex varchar(5), index(name) )engine=innodb; select id,name where name='shenjian' select id,name,sex where name='shenjian' 多查询了一个属性，为何检索过程完全不同？
什么是回表查询？
什么是索引覆盖？
如何实现索引覆盖？
哪些场景，可以利用索引覆盖来优化SQL？
一、什么是回表查询？ 这先要从InnoDB的索引实现说起，InnoDB有两大类索引：
聚集索引(clustered index) 普通索引(secondary index) **InnoDB聚集索引和普通索引有什么差异？
**
InnoDB聚集索引的叶子节点存储行记录，因此， InnoDB必须要有，且只有一个聚集索引：
（1）如果表定义了PK，则PK就是聚集索引；
（2）如果表没有定义PK，则第一个not NULL unique列是聚集索引；
（3）否则，InnoDB会创建一个隐藏的row-id作为聚集索引；
画外音：所以PK查询非常快，直接定位行记录。
InnoDB普通索引的叶子节点存储主键值。
画外音：注意，不是存储行记录头指针，MyISAM的索引叶子节点存储记录指针。
举个栗子，不妨设有表：
t(id PK, name KEY, sex, flag);
画外音：id是聚集索引，name是普通索引。
表中有四条记录：
1, shenjian, m, A
3, zhangsan, m, A...</p></div><footer class=entry-footer><span title='2022-06-08 11:04:01 +0800 +0800'>2022-06-08</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Reid</footer><a class=entry-link aria-label="post link to MySql索引优化" href=https://reid00.github.io/posts/mysql%E7%B4%A2%E5%BC%95%E4%BC%98%E5%8C%96/></a></article><article class=post-entry><header class=entry-header><h2>MySql索引介绍</h2></header><div class=entry-content><p>什么是索引，索引的作用 当我们要在新华字典里查某个字（如「先」）具体含义的时候，通常都会拿起一本新华字典来查，你可以先从头到尾查询每一页是否有「先」这个字，这样做（对应数据库中的全表扫描）确实能找到，但效率无疑是非常低下的，更高效的方相信大家也都知道，就是在首页的索引里先查找「先」对应的页数，然后直接跳到相应的页面查找，这样查询时候大大减少了，可以为是 O(1)。
数据库中的索引也是类似的，通过索引定位到要读取的页，大大减少了需要扫描的行数，能极大的提升效率，简而言之，索引主要有以下几个作用:
即上述所说，索引能极大地减少扫描行数 索引可以帮助服务器避免排序和临时表 索引可以将随机 IO 变成顺序 IO MySQL中索引的存储类型有两种：BTREE和HASH，具体和表的存储引擎相关；
MyISAM和InnoDB存储引擎只支持BTREE索引，MEMORY/HEAP存储引擎可以支持HASH和BTREE索引。
第一点上文已经解释了，我们来看下第二点和第三点
先来看第二点，假设我们不用索引，试想运行如下语句
1 select * from user order by age desc 则 MySQL 的流程是这样的，扫描所有行，把所有行加载到内存后，再按 age 排序生成一张临时表，再把这表排序后将相应行返回给客户端，更糟的，如果这张临时表的大小大于 tmp_table_size 的值（默认为 16 M），内存临时表会转为磁盘临时表，性能会更差，如果加了索引，索引本身是有序的 ，所以从磁盘读的行数本身就是按 age 排序好的，也就不会生成临时表，就不用再额外排序 ，无疑提升了性能。
再来看随机 IO 和顺序 IO。先来解释下这两个概念。
相信不少人应该吃过旋转火锅，服务员把一盘盘的菜放在旋转传输带上，然后等到这些菜转到我们面前，我们就可以拿到菜了，假设装一圈需要 4 分钟，则最短等待时间是 0（即菜就在你跟前），最长等待时间是 4 分钟（菜刚好在你跟前错过），那么平均等待时间即为 2 分钟，假设我们现在要拿四盘菜，这四盘菜随机分配在传输带上，则可知拿到这四盘菜的平均等待时间是 8 分钟（随机 IO），如果这四盘菜刚好紧邻着排在一起，则等待时间只需 2 分钟（顺序 IO）。
上述中传输带就类比磁道，磁道上的菜就类比扇区（sector）中的信息，磁盘块（block）是由多个相邻的扇区组成的，是操作系统读取的最小单元，这样如果信息能以 block 的形式聚集在一起，就能极大减少磁盘 IO 时间,这就是顺序 IO 带来的性能提升，下文中我们将会看到 B+ 树索引就起到这样的作用。
如图示：多个扇区组成了一个 block，如果要读的信息都在这个 block 中，则只需一次 IO 读
而如果信息在一个磁道中, 分散地分布在各个扇区中，或者分布在不同磁道的扇区上（寻道时间是随机IO主要瓶颈所在），将会造成随机 IO，影响性能。...</p></div><footer class=entry-footer><span title='2022-06-08 11:02:52 +0800 +0800'>2022-06-08</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Reid</footer><a class=entry-link aria-label="post link to MySql索引介绍" href=https://reid00.github.io/posts/mysql%E7%B4%A2%E5%BC%95%E4%BB%8B%E7%BB%8D/></a></article><article class=post-entry><header class=entry-header><h2>MySql高频面试问题</h2></header><div class=entry-content><p>本文主要受众为开发人员,所以不涉及到MySQL的服务部署等操作,且内容较多,大家准备好耐心和瓜子矿泉水。
前一阵系统的学习了一下MySQL,也有一些实际操作经验,偶然看到一篇和MySQL相关的面试文章,发现其中的一些问题自己也回答不好,虽然知识点大部分都知道,但是无法将知识串联起来。
因此决定搞一个MySQL灵魂100问,试着用回答问题的方式,让自己对知识点的理解更加深入一点。
此文不会事无巨细的从select的用法开始讲解mysql,主要针对的是开发人员需要知道的一些MySQL的知识点
主要包括索引,事务,优化等方面,以在面试中高频的问句形式给出答案。
MySQL 重要笔记 三万字、91道MySQL面试题（收藏版）
https://mp.weixin.qq.com/s/KRWyl-zU1Cd6sxbya4dP_g
书写高质量SQL的30条建议
https://mp.weixin.qq.com/s/nM6fwEyi2VZeRMWtdZGpGQ
数据分析面试必备SQL语句+语法
https://mp.weixin.qq.com/s/8UZAaDyB38gsZANPLxNKgg
索引相关 关于MySQL的索引,曾经进行过一次总结,文章链接在这里 Mysql索引原理及其优化.
1. 什么是索引?
索引是一种数据结构,可以帮助我们快速的进行数据的查找.
2. 索引是个什么样的数据结构呢?
索引的数据结构和具体存储引擎的实现有关, 在MySQL中使用较多的索引有Hash索引,B+树索引等,而我们经常使用的InnoDB存储引擎的默认索引实现为:B+树索引.
3. Hash索引和B+树所有有什么区别或者说优劣呢?
首先要知道Hash索引和B+树索引的底层实现原理:
hash索引底层就是hash表,进行查找时,调用一次hash函数就可以获取到相应的键值,之后进行回表查询获得实际数据.B+树底层实现是多路平衡查找树.
对于每一次的查询都是从根节点出发,查找到叶子节点方可以获得所查键值,然后根据查询判断是否需要回表查询数据.
那么可以看出他们有以下的不同:
hash索引进行等值查询更快(一般情况下),但是却无法进行范围查询. 因为在hash索引中经过hash函数建立索引之后,索引的顺序与原顺序无法保持一致,不能支持范围查询.
而B+树的的所有节点皆遵循(左节点小于父节点,右节点大于父节点,多叉树也类似),天然支持范围.
hash索引不支持使用索引进行排序,原理同上.
hash索引不支持模糊查询以及多列索引的最左前缀匹配.原理也是因为hash函数的不可预测.AAAA和AAAAB的索引没有相关性.
hash索引任何时候都避免不了回表查询数据,而B+树在符合某些条件(聚簇索引,覆盖索引等)的时候可以只通过索引完成查询.
hash索引虽然在等值查询上较快,但是不稳定.性能不可预测,当某个键值存在大量重复的时候,发生hash碰撞,此时效率可能极差.而B+树的查询效率比较稳定,对于所有的查询都是从根节点到叶子节点,且树的高度较低.
因此,在大多数情况下,直接选择B+树索引可以获得稳定且较好的查询速度.而不需要使用hash索引.
4. 上面提到了B+树在满足聚簇索引和覆盖索引的时候不需要回表查询数据,什么是聚簇索引?
在B+树的索引中,叶子节点可能存储了当前的key值,也可能存储了当前的key值以及整行的数据,这就是聚簇索引和非聚簇索引.
在InnoDB中,只有主键索引是聚簇索引,如果没有主键,则挑选一个唯一键建立聚簇索引.如果没有唯一键,则隐式的生成一个键来建立聚簇索引.
当查询使用聚簇索引时,在对应的叶子节点,可以获取到整行数据,因此不用再次进行回表查询.
5. 非聚簇索引一定会回表查询吗?
不一定,这涉及到查询语句所要求的字段是否全部命中了索引,如果全部命中了索引,那么就不必再进行回表查询.
举个简单的例子,假设我们在员工表的年龄上建立了索引,那么当进行select age from employee where age &lt; 20的查询时,在索引的叶子节点上,已经包含了age信息,不会再次进行回表查询.
6. 在建立索引的时候,都有哪些需要考虑的因素呢?
建立索引的时候一般要考虑到字段的使用频率,经常作为条件进行查询的字段比较适合.如果需要建立联合索引的话,还需要考虑联合索引中的顺序.
此外也要考虑其他方面,比如防止过多的所有对表造成太大的压力.这些都和实际的表结构以及查询方式有关.
7. 联合索引是什么?为什么需要注意联合索引中的顺序?
MySQL可以使用多个字段同时建立一个索引,叫做联合索引.在联合索引中,如果想要命中索引,需要按照建立索引时的字段顺序挨个使用,否则无法命中索引.
具体原因为:
MySQL使用索引时需要索引有序,假设现在建立了"name,age,school"的联合索引
那么索引的排序为: 先按照name排序,如果name相同,则按照age排序,如果age的值也相等,则按照school进行排序.
当进行查询时,此时索引仅仅按照name严格有序,因此必须首先使用name字段进行等值查询,之后对于匹配到的列而言,其按照age字段严格有序,此时可以使用age字段用做索引查找,以此类推.
因此在建立联合索引的时候应该注意索引列的顺序,一般情况下,将查询需求频繁或者字段选择性高的列放在前面.此外可以根据特例的查询或者表结构进行单独的调整.
8. 创建的索引有没有被使用到?或者说怎么才可以知道这条语句运行很慢的原因?
MySQL提供了explain命令来查看语句的执行计划,MySQL在执行某个语句之前,会将该语句过一遍查询优化器,之后会拿到对语句的分析,也就是执行计划,其中包含了许多信息.
可以通过其中和索引有关的信息来分析是否命中了索引,例如possilbe_key,key,key_len等字段,分别说明了此语句可能会使用的索引,实际使用的索引以及使用的索引长度....</p></div><footer class=entry-footer><span title='2022-06-08 11:00:46 +0800 +0800'>2022-06-08</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Reid</footer><a class=entry-link aria-label="post link to MySql高频面试问题" href=https://reid00.github.io/posts/mysql%E9%AB%98%E9%A2%91%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98/></a></article><article class=post-entry><header class=entry-header><h2>机器学习之优化算法</h2></header><div class=entry-content><p>在调整模型更新权重和偏差参数的方式时，你是否考虑过哪种优化算法能使模型产生更好且更快的效果？应该用梯度下降，随机梯度下降，还是Adam方法？
这篇文章介绍了不同优化算法之间的主要区别，以及如何选择最佳的优化方法。
梯度: 是多元函数对当前给定点，上升最快的方向。梯度是一组向量，所以带有方向;
梯度下降流程: https://zhuanlan.zhihu.com/p/68468520 w, b 每轮是每个样本的权重梯度向量和偏差梯度向量的平均值；
梯度下降本质是沿着负梯度值方向寻找损失函数Loss的最小值解 时的参数w,b , 从而得出对样本数据拟合最好的参数w,b。 https://www.jianshu.com/p/c7e642877b0e
什么是优化算法？ 优化算法的功能，是通过改善训练方式，来最小化(或最大化)损失函数E(x)。
模型内部有些参数，是用来计算测试集中目标值Y的真实值和预测值的偏差程度的，基于这些参数，就形成了损失函数E(x)。
比如说，权重(W)和偏差(b)就是这样的内部参数，一般用于计算输出值，在训练神经网络模型时起到主要作用。
**在有效地训练模型并产生准确结果时，模型的内部参数起到了非常重要的作用。**这也是为什么我们应该用各种优化策略和算法，来更新和计算影响模型训练和模型输出的网络参数，使其逼近或达到最优值。
优化算法分为两大类：
1. 一阶优化算法
这种算法使用各参数的梯度值来最小化或最大化损失函数E(x)。最常用的一阶优化算法是梯度下降。
函数梯度：导数dy/dx的多变量表达式，用来表示y相对于x的瞬时变化率。往往为了计算多变量函数的导数时，会用梯度取代导数，并使用偏导数来计算梯度。梯度和导数之间的一个主要区别是函数的梯度形成了一个向量场。
因此，对单变量函数，使用导数来分析；而梯度是基于多变量函数而产生的。更多理论细节在这里不再进行详细解释。
2. 二阶优化算法
二阶优化算法使用了二阶导数(也叫做Hessian方法)来最小化或最大化损失函数。由于二阶导数的计算成本很高，所以这种方法并没有广泛使用。
详解各种神经网络优化算法 梯度下降 在训练和优化智能系统时，梯度下降是一种最重要的技术和基础。梯度下降的功能是：
通过寻找最小值，控制方差，更新模型参数，最终使模型收敛。
网络更新参数的公式为：θ=θ−η×∇(θ).J(θ) ，其中η是学习率，∇(θ).J(θ)是损失函数J(θ)的梯度。
这是在神经网络中最常用的优化算法。
如今，梯度下降主要用于在神经网络模型中进行权重更新，即在一个方向上更新和调整模型的参数，来最小化损失函数。
2006年引入的反向传播技术，使得训练深层神经网络成为可能。反向传播技术是先在前向传播中计算输入信号的乘积及其对应的权重，然后将激活函数作用于这些乘积的总和。这种将输入信号转换为输出信号的方式，是一种对复杂非线性函数进行建模的重要手段，并引入了非线性激活函数，使得模型能够学习到几乎任意形式的函数映射。然后，在网络的反向传播过程中回传相关误差，使用梯度下降更新权重值，通过计算误差函数E相对于权重参数W的梯度，在损失函数梯度的相反方向上更新权重参数。
**图1：**权重更新方向与梯度方向相反 图1显示了权重更新过程与梯度矢量误差的方向相反，其中U形曲线为梯度。要注意到，当权重值W太小或太大时，会存在较大的误差，需要更新和优化权重，使其转化为合适值，所以我们试图在与梯度相反的方向找到一个局部最优值。
梯度下降的变体 传统的批量梯度下降将计算整个数据集梯度，但只会进行一次更新，因此在处理大型数据集时速度很慢且难以控制，甚至导致内存溢出。
权重更新的快慢是由学习率η决定的，并且可以在凸面误差曲面中收敛到全局最优值，在非凸曲面中可能趋于局部最优值。
使用标准形式的批量梯度下降还有一个问题，就是在训练大型数据集时存在冗余的权重更新。
标准梯度下降的上述问题在随机梯度下降方法中得到了解决。
1. 随机梯度下降(SDG)
随机梯度下降（Stochastic gradient descent，SGD）对每个训练样本进行参数更新，每次执行都进行一次更新，且执行速度更快。
θ=θ−η⋅∇(θ) × J(θ;x(i);y(i))，其中x(i)和y(i)为训练样本。
频繁的更新使得参数间具有高方差，损失函数会以不同的强度波动。这实际上是一件好事，因为它有助于我们发现新的和可能更优的局部最小值，而标准梯度下降将只会收敛到某个局部最优值。
但SGD的问题是，由于频繁的更新和波动，最终将收敛到最小限度，并会因波动频繁存在超调量。
虽然已经表明，当缓慢降低学习率η时，标准梯度下降的收敛模式与SGD的模式相同。
**图2：**每个训练样本中高方差的参数更新会导致损失函数大幅波动，因此我们可能无法获得给出损失函数的最小值。 另一种称为“小批量梯度下降”的变体，则可以解决高方差的参数更新和不稳定收敛的问题。
2. 小批量梯度下降
为了避免SGD和标准梯度下降中存在的问题，一个改进方法为小批量梯度下降（Mini Batch Gradient Descent），因为对每个批次中的n个训练样本，这种方法只执行一次更新。
使用小批量梯度下降的优点是：
1) 可以减少参数更新的波动，最终得到效果更好和更稳定的收敛。
2) 还可以使用最新的深层学习库中通用的矩阵优化方法，使计算小批量数据的梯度更加高效。
3) 通常来说，小批量样本的大小范围是从50到256，可以根据实际问题而有所不同。...</p></div><footer class=entry-footer><span title='2022-06-08 10:58:09 +0800 +0800'>2022-06-08</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Reid</footer><a class=entry-link aria-label="post link to 机器学习之优化算法" href=https://reid00.github.io/posts/ml/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/></a></article><article class=post-entry><header class=entry-header><h2>Python Import导入上级目录文件</h2></header><div class=entry-content><p>假设有如下目录结构：
1 2 3 4 5 6 7 -- dir0 | file1.py | file2.py | dir3 | file3.py | dir4 | file4.py dir0文件夹下有file1.py、file2.py两个文件和dir3、dir4两个子文件夹，dir3中有file3.py文件，dir4中有file4.py文件。
1.导入同级模块 python导入同级模块（在同一个文件夹中的py文件）直接导入即可。
1 import xxx 如在file1.py中想导入file2.py，注意无需加后缀".py"：
1 2 3 import file2 # 使用file2中函数时需加上前缀"file2."，即： # file2.fuction_name() 2.导入下级模块 导入下级目录模块也很容易，需在下级目录中新建一个空白的__init__.py文件再导入：
1 from dirname import xxx 如在file1.py中想导入dir3下的file3.py，首先要在dir3中新建一个空白的__init__.py文件。
1 2 3 4 5 6 7 8 -- dir0 | file1.py | file2.py | dir3 | __init__.py | file3.py | dir4 | file4.py 再使用如下语句：...</p></div><footer class=entry-footer><span title='2022-06-08 10:55:54 +0800 +0800'>2022-06-08</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Reid</footer><a class=entry-link aria-label="post link to Python Import导入上级目录文件" href=https://reid00.github.io/posts/python-import%E5%AF%BC%E5%85%A5%E4%B8%8A%E7%BA%A7%E7%9B%AE%E5%BD%95%E6%96%87%E4%BB%B6/></a></article><article class=post-entry><header class=entry-header><h2>集成学习之GBDT</h2></header><div class=entry-content><p>什么是GBDT 到底什么是梯度提升树？所谓的GBDT实际上就是：
GBDT = Gradient Descent + Boosting + Desicion Tree
与Adaboost算法类似，GBDT也是使用了前向分布算法的加法模型。只不过弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同。
在Adaboost算法中，我们是利用前一轮迭代弱学习器的误差率来更新训练集的权重。而Gradient Boosting是通过算梯度（gradient）来定位模型的不足。
https://mp.weixin.qq.com/s/rmStKvdHq-BOCJo8ZuvgfQ
最常用的决策树算法: RF, Adaboost, GBDT
https://mp.weixin.qq.com/s/tUl3zhVxLfUd7o06_1Zg2g
Xgboost 的优势和原理 原理: https://www.jianshu.com/p/920592e8bcd2
​ https://www.jianshu.com/p/ac1c12f3fba1
优势: https://snaildove.github.io/2018/10/02/get-started-XGBoost/
LightGBM 详解 https://blog.csdn.net/VariableX/article/details/106242202
GBDT分类算法流程 GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。
为了解决这个问题，主要有两个方法：
用指数损失函数，此时GBDT退化为Adaboost算法。 用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。 下面我们用对数似然损失函数的GBDT分类。而对于对数似然损失函数，又有二元分类和多元分类的区别。
sklearn中的GBDT调参大法 https://mp.weixin.qq.com/s/756Xsy0uhnb8_rheySqLLg
Boosting重要参数 分类和回归算法的参数大致相同，不同之处会指出。
n_estimators: 弱学习器的个数。个数太小容易欠拟合，个数太大容易过拟合。默认是100，在实际调参的过程中，常常将n_estimators和参数learning_rate一起考虑。
learning_rate: 每个弱学习器的权重缩减系数，也称作步长。如果我们在强学习器的迭代公式加上了正则化项：，则通过learning_rate来控制其权重。对于同样的训练集拟合效果，较小的learning_rate意味着需要更多的弱学习器。通常用二者一起决定算法的拟合效果。所以两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的补偿开始调参，默认是1。
subsample: 不放回抽样的子采样，取值为(0,1]。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。
init: 初始化时的弱学习器，即。如果我们对数据有先验知识，或者之前做过一些拟合，可以用init参数提供的学习器做初始化分类回归预测。一般情况下不输入，直接用训练集样本来做样本集的初始化分类回归预测。
loss: GBDT算法中的损失函数。分类模型和回归模型的损失函数是不一样。
对于回归模型，可以使用均方误差ls，绝对损失lad，Huber损失huber和分位数损失quantile，默认使用均方误差ls。如果数据的噪音点不多，用默认的均方差ls比较好；如果噪音点较多，则推荐用抗噪音的损失函数huber；而如果需要对训练集进行分段预测，则采用quantile。 对于分类模型，可以使用对数似然损失函数deviance和指数损失函数exponential。默认是对数似然损失函数deviance。在原理篇中对这些分类损失函数有详细的介绍。一般来说，推荐使用默认的"deviance"。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。 alpha: 这个参数只有回归算法有，当使用Huber损失huber和分位数损失quantile时，需要指定分位数的值。默认是0.9，如果噪音点较多，可以适当降低这个分位数的值。
弱学习器参数 GBDT使用了CART回归决策树，因此它的参数基本和决策树类似。
max_features: 划分时考虑的最大特征数，默认是"None"。默认时表示划分时考虑所有的特征数；如果是"log2"意味着划分时最多考虑个log2N特征；如果是"sqrt"或者"auto"意味着划分时最多考虑根号N个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比*N）取整后的特征数。其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的"None"就可以了，如果特征数非常多，可以灵活控制划分时考虑的最大特征数，以控制决策树的生成时间。 max_depth: 决策树最大深度。如果不输入，默认值是3。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。 min_samples_split: 内部节点再划分所需最小样本数。限制子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。默认是2，如果样本量数量级非常大，则增大这个值。 min_samples_leaf: 叶子节点最少样本数。限制叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。 min_weight_fraction_leaf: 叶子节点最小的样本权重和这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。 max_leaf_nodes: 最大叶子节点数。通过限制最大叶子节点数，可以防止过拟合，默认是None，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。 min_impurity_split: 节点划分最小不纯度。这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7。 GBDT有很多优点：...</p></div><footer class=entry-footer><span title='2022-06-08 10:54:50 +0800 +0800'>2022-06-08</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Reid</footer><a class=entry-link aria-label="post link to 集成学习之GBDT" href=https://reid00.github.io/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Bgbdt/></a></article><article class=post-entry><header class=entry-header><h2>集成学习之Bagging,Boosting</h2></header><div class=entry-content><p>生成子模型的两种取样方式 那么为了造成子模型之间的差距，每个子模型只看样本中的一部分，这就涉及到两种取样方式：
放回取样：Bagging，在统计学中也被称为bootstrap。 不放回取样：Boosting 在集成学习中我们通常采用 Bagging 的方式，具体原因如下：
因为取样后放回，所以不受样本数据量的限制，允许对同一种分类器上对训练集进行进行多次采样，可以训练更多的子模型。 在 train_test_split 时，不那么强烈的依赖随机；而 Boosting的方式，会受到随机的影响； Boosting的随机问题：Pasting 的方式等同于将 500 个样本分成 5 份，每份 100 个样本，怎么分，将对子模型有较大影响，进而对集成系统的准确率有较大影响。 什么是Bagging Bagging，即bootstrap aggregating的缩写，每个训练集称为bootstrap。
Bagging是一种根据均匀概率分布从数据中重复抽样（有放回）的技术 。
Bagging能提升机器学习算法的稳定性和准确性，它可以减少模型的方差从而避免overfitting。它通常应用在决策树方法中，其实它可以应用到任何其它机器学习算法中。
Bagging方法在不稳定模型（unstable models）集合中表现比较好。这里说的不稳定的模型，即在训练数据发生微小变化时产生不同泛化行为的模型（高方差模型），如决策树和神经网络。
但是Bagging在过于简单模型集合中表现并不好，因为Bagging是从总体数据集随机选取样本来训练模型，过于简单的模型可能会产生相同的预测结果，失去了多样性。
总结一下Bagging方法：
Bagging通过降低基分类器的方差，改善了泛化误差 其性能依赖于基分类器的稳定性；如果基分类器不稳定，bagging有助于降低训练数据的随机波动导致的误差；如果稳定，则集成分类器的误差主要由基分类器的偏差引起 由于每个样本被选中的概率相同，因此bagging并不侧重于训练数据集中的任何特定实例 Bagging的使用 sklearn为Bagging提供了一个简单的API：BaggingClassifier类（回归是BaggingRegressor）。首先需要传入一个模型作为参数，可以使用决策树；然后需要传入参数n_estimator即集成多少个子模型；参数max_samples表示每次从数据集中取多少样本；参数bootstrap设置为True表示使用有放回取样Bagging，设置为False表示使用无放回取样Pasting。可以通过n_jobs参数来分配训练所需CPU核的数量，-1表示会使用所有空闲核（集成学习思路，极易并行化处理）。
bagging是不能减小模型的偏差的，因此我们要选择具有低偏差的分类器来集成，例如：没有修剪的决策树。
Bootstrap 在每个预测器被训练的子集中引入了更多的分集，所以 Bagging 结束时的偏差比 Pasting 更高，但这也意味着预测因子最终变得不相关，从而减少了集合的方差。总体而言，Bagging 通常会导致更好的模型，这就解释了为什么它通常是首选的。然而，如果你有空闲时间和 CPU 功率，可以使用交叉验证来评估 Bagging 和 Pasting 哪一个更好。
Out-of-Bag 对于Bagging来说，一些实例可能被一些分类器重复采样，但其他的有可能不会被采样。由于每个bootstrap的M个样本是有放回随机选取的，因此每个样本不被选中的概率为。当N和M都非常大时，比如N=M=10000，一个样本不被选中的概率p = 36.8%。因此一个bootstrap约包含原样本63.2%，约36.8%的样本未被选中。这些没有被采样的训练实例就叫做Out-of-Bag实例。但注意对于每一个的分类器来说，它们各自的未选中部分不是相同的。
那么这些未选中的样本有什么用呢？
因为在训练中分类器从来没有看到过Out-of-Bag实例，所以它可以在这些样本上进行预测，就不用分样本测试集和测试数据集了。
在sklearn中，可以在训练后需要创建一个BaggingClassifier时设置oob_score=True来进行自动评估。
1 2 3 4 5 bagging_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=5000, max_samples=100, bootstrap=True, oob_score=True) bagging_clf.fit(X, y) bagging_clf....</p></div><footer class=entry-footer><span title='2022-06-08 10:53:37 +0800 +0800'>2022-06-08</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Reid</footer><a class=entry-link aria-label="post link to 集成学习之Bagging,Boosting" href=https://reid00.github.io/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Bbaggingboosting/></a></article><article class=post-entry><header class=entry-header><h2>集成学习之AdaBoost</h2></header><div class=entry-content><p>Boosting算法的工作机制 用初始权重D(1)从数据集中训练出一个弱学习器1 根据弱学习1的学习误差率表现来更新训练样本的权重D(2)，使得之前弱学习器1学习误差率高的样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。 然后基于调整权重后的训练集来训练弱学习器2 如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。 现如今已经有很多的提升方法了，但最著名的就是Adaboost（适应性提升，是Adaptive Boosting的简称）和Gradient Boosting（梯度提升）。让我们先从 Adaboost 说起。
什么是AdaBoost AdaBoost是一个具有里程碑意义的算法，其中，适应性（adaptive）是指：后续的分类器为更好地支持被先前分类器分类错误的样本实例而进行调整。通过对之前分类结果不对的训练实例多加关注，使新的预测因子越来越多地聚焦于之前错误的情况。
具体说来，整个AdaBoost迭代算法就3步：
初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：。 训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。 加法模型与前向分布 在学习AdaBoost之前需要了解两个数学问题，这两个数学问题可以帮助我们更好地理解AdaBoost算法，并且在面试官问你算法原理时不至于发懵。下面我们就来看看加法模型与前向分布。
什么是加法模型 当别人问你“什么是加法模型”时，你应当知道：加法模型顾名思义就是把各种东西加起来求和。如果想要更严谨的定义，不妨用数学公式来表达： 这个公式看上去可能有些糊涂，如果我们套用到提升树模型中就比较容易理解一些。FM(x)表示最终生成的最好的提升树，其中M表示累加的树的个数。b(x;ym)表示一个决策树，$阿尔法m$ 表示第m个决策树的权重，ym表示决策树的参数（如叶节点的个数）。
什么是前向分布 那么什么是前向分布算法呢？在损失函数 的条件下，加法模型FM(x)成为一个经验风险极小化问题，即使得损失函数极小化： 前向分布算法就是求解这个优化问题的一个思想：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数（一棵决策树）及其权重，利用残差逐步逼近优化问题，那么就可以简化优化的复杂度。从而得到前向分布算法为：
套用在提升树模型中进行理解就是：$fm-1(x)$是前一棵提升树（之前树的累加），在其基础上再加上一棵树$Bxi, Ym$乘上它的权重系数，用这棵树去拟合的残差!$阿尔法m$（观察值与估计值之间的差），再将这两棵树合在一起就得到了新的提升树。实际上就是让下一个基分类器去拟合当前分类器学习出来的残差。
前向分布与Adaboost损失函数优化的关系 现在了解了加法模型与前向分布。那这两个概念与Adaboost又有什么关系呢？
Adaboost可以认为其模型是加法模型、损失函数为指数函数、学习算法为前向分步算法的二类分类学习方法。我们可以使用前向分布算法作为框架，推导出Adaboost算法的损失函数优化问题的。
在Adaboost中，各个基本分类器就相当于加法模型中的基函数$fm-1(x)$，且其损失函数为指数函数$b(xi;ym)$。
即，需要优化的问题如下： 如果我们令，则上述公式可以改写成为： 因为与要么相等、要么不等。所以可以将其拆成两部分相加的形式：
算法中需要关注的内容 首先看看算法中都关注了哪些内容： 首先，我们假设训练样本为$(x1,y1), (x2, y2)…(xn, yn)$
由于AdaBoost是由一个个的弱分类器迭代训练得到一个强分类器的，因此我们有如下定义：
弱分类器表达式：$Ht(x)$ 先以二分类为例，它输出的值为1或-1，则有：$Ht(x) ∈{-1, 1}$
首先，我们假设训练样本为 由于AdaBoost是由一个个的弱分类器迭代训练得到一个强分类器的，因此我们有如下定义：
弱分类器表达式： 公式推导（通过Z最小化训练误差) Adaboost算法之所以称为十大算法之一，有一个重要原因就是它有完美的数学推导过程，其参数不是人工设定的，而是有解析解的，并且可以证明其误差上界越来越小，趋近于零；且可以推导出来。下面就来看一下公式推导。
权重公式: 首先要把模型的误差表示出来，只有用数学公式表示出来，才能够讲模型的优化。
先看第i个样本在t+1个弱学习器的权重是怎样的? 模型误差上限 模型误差上限最小化与Z 求出Z 既然最小化Zt就等同于最小化模型误差上界，那我们得先知道Zt长什么样，然后才能去最小化它。
我们在前面已经说过，为了保证所有样本的权重加起来等于1。因此需要对每个权重除以归一化系数。即Zt实际上就是t+1时刻所有样本原始权重和，也就是时刻的各点权重乘以调整幅度再累加：
求出使得Z最小的参数a AdaBoost计算步骤梳理及优缺点 理论上任何学习器都可以用于Adaboost。但一般来说，使用最广泛的Adaboost弱学习器是决策树和神经网络。对于决策树，Adaboost分类用了CART分类树，而Adaboost回归用了CART回归树。</p></div><footer class=entry-footer><span title='2022-06-08 10:29:01 +0800 +0800'>2022-06-08</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Reid</footer><a class=entry-link aria-label="post link to 集成学习之AdaBoost" href=https://reid00.github.io/posts/ml/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8Badaboost/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://reid00.github.io/posts/page/5/>« Prev</a>
<a class=next href=https://reid00.github.io/posts/page/7/>Next »</a></nav></footer></main><footer class=footer><span>&copy; 2023 <a href=https://reid00.github.io/>Reid's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>