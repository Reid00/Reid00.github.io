<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | Reid's Blog</title><meta name=keywords content><meta name=description content="Posts - Reid's Blog"><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/posts/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://reid00.github.io/posts/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK",{anonymize_ip:!1})}</script><meta property="og:title" content="Posts"><meta property="og:description" content="Reid's Personal Notes -- https://github.com/Reid00"><meta property="og:type" content="website"><meta property="og:url" content="https://reid00.github.io/posts/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="Posts"><meta name=twitter:description content="Reid's Personal Notes -- https://github.com/Reid00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://reid00.github.io/posts/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://reid00.github.io/ accesskey=h title="Reid's Blog (Alt + H)">Reid's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://reid00.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://reid00.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://reid00.github.io/categories/ title=Categorys><span>Categorys</span></a></li><li><a href=https://reid00.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://reid00.github.io/>Home</a></div><h1>Posts
<a href=index.xml title=RSS aria-label=RSS><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class=post-entry><header class=entry-header><h2>朴素贝叶斯</h2></header><div class=entry-content><p>贝叶斯准备知识 贝叶斯决策论是概率框架下实施决策的基本方法。要了解贝叶斯决策论，首先得先了解以下几个概念：先验概率、条件概率、后验概率、误判损失、条件风险、贝叶斯判别准则
先验概率： 所谓先验概率，就是根据以往的经验或者现有数据的分析所得到的概率。如，随机扔一枚硬币，则p(正面) = p(反面) = 1/2，这是我们根据已知的知识所知道的信息，即p(正面) = 1/2为先验概率。
条件概率： 所谓条件概率是指事件A在另一事件B发生的条件下发送的概率。用数学符号表示为：P(B\|A)，即B在A发生的条件下发生的概率。举个栗子，你早上误喝了一瓶过期了的牛奶（A），那我们来算一下你今天拉肚子的概率（B），这个就叫做条件概率。即P（拉肚子\|喝了过期牛奶）， 易见，条件概率是有因求果（知道原因推测结果）。
后验概率： 后验概率跟条件概率的表达形式有点相似。数学表达式为p(A\|B), 即A在B发生的条件下发生的概率。以误喝牛奶的例子为例，现在知道了你今天拉肚子了（B），算一下你早上误喝了一瓶过期了的牛奶(A)的概率, 即P（A|B），这就是后验概率，后验概率是有果求因（知道结果推出原因）
误判损失： 数学表达式：L(j|i)， 判别损失表示把一个标记为i类的样本误分类为j类所造成的损失。 比如，当你去参加体检时，明明你各项指标都是正常的，但是医生却把你分为癌症病人，这就造成了误判损失，用数学表示为：L(癌症|正常)。
条件风险： 是指基于后验概率P(i|x)可获得将样本x分类为i所产生的期望损失，公式为：R(i|x) = ∑L(i|j)P(j|x)。(其实就是所有判别损失的加权和，而这个权就是样本判为j类的概率，样本本来应该含有P(j|x)的概率判为j类，但是却判为了i类，这就造成了错判损失，而将所有的错判损失与正确判断的概率的乘积相加，就能得到样本错判为i类的平均损失，即条件风险。)
举个栗子，假设把癌症病人判为正常人的误判损失是100，把正常人判为癌症病人的误判损失是10，把感冒病人判为癌症的误判损失是8，即L（正常|癌症） = 100， L（癌症|正常） = 10，L(癌症|感冒) = 8， 现在，我们经过计算知道有一个来体检的员工的后验概率分别为：p(正常|各项指标) = 0.2， p(感冒|各项指标) = 0.4, p（ 癌症|各项指标)=0.4。假如我们需要计算将这个员工判为癌症的条件风险，则：R（癌症|各项指标） = L（癌症|正常） p(正常|各项指标) + L(癌症|感冒) * p(感冒|各项指标) = 5.2。*
贝叶斯判别准则：
贝叶斯判别准则是找到一个使条件风险达到最小的判别方法。即，将样本判为哪一类，所得到的条件风险R(i|x)（或者说平均判别损失）最小，那就将样本归为那个造成平均判别损失最小的类。
此时：h*(x) = argminR(i|x) 就称为 贝叶斯最优分类器。
总结：贝叶斯决策论是基于先验概率求解后验概率的方法，其核心是寻找一个判别准则使得条件风险达到最小。而在最小化分类错误率的目标下，贝叶斯最优分类器又可以转化为求后验概率达到最大的类别标记，即 h*（x) = argmaxP(i|x)。（此时，L(i|j) = 0, if i = j;L(i|j) = 1, otherwise)...</p></div><footer class=entry-footer><span title='2022-06-07 19:40:44 +0800 +0800'>2022-06-07</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Reid</footer><a class=entry-link aria-label="post link to 朴素贝叶斯" href=https://reid00.github.io/posts/ml/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/></a></article><article class=post-entry><header class=entry-header><h2>生成模型vs判别模型</h2></header><div class=entry-content><p>什么是生成模型和判别模型？ 从本质上讲，生成模型和判别模型是解决分类问题的两类基本思路。首先，您得先了解，分类问题，就是给定一个数据x，要判断它对应的标签y（这么naive的东西都要解释下，求面试官此时内心的阴影面积，嘎嘎）。生成模型就是要学习x和y的联合概率分布P(x,y)，然后根据贝叶斯公式来求得条件概率P(y|x)，预测条件概率最大的y。贝叶斯公式这么简单的知识相信您也了解，我就不啰嗦了。判别模型就是直接学习条件概率分布P(y|x)。
举个栗子 例子1 假设你从来没有见过大象和猫，连听都没有听过，这时，给你看了一张大象的照片和一张猫的照片。如下所示：
然后牵来我家的大象（面试官：你家开动物园的吗？），让你判断这是大象还是猫。你咋办？
你开始回想刚刚看过的照片，大概记起来，大象和猫比起来，有个长鼻子，而眼前这个家伙也有个长鼻子，所以，你兴奋地说：“这是大象！”恭喜你答对了！
你也有可能这样做，你努力回想刚才的两张照片，然后用笔把它们画在了纸上，拿着纸和我家的大象做比较，你发现，眼前的动物更像是大象。于是，你惊喜地宣布：“这玩意是大象！”恭喜你又答对了！
在这个问题中，第一个解决问题的思路就是判别模型，因为你只记住了大象和猫之间的不同之处。第二个解决问题的思路就是生成模型，因为你实际上学习了什么是大象，什么是猫。
例子2 来来来，看一下这四个形式为(x,y)的样本。(1,0), (1,0), (2,0), (2, 1）。假设，我们想从这四个样本中，学习到如何通过x判断y的模型。用生成模型，我们要学习P(x,y)。如下所示：
我们学习到了四个概率值，它们的和是1，这就是P(x,y)。
我们也可以用判别模型，我们要学习P(y|x)，如下所示：
我们同样学习到了四个概率值，但是，这次，是每一行的两个概率值的和为1了。让我们具体来看一下，如何使用这两个模型做判断。
假设 x=1。
对于生成模型， 我们会比较：
P(x=1,y=0) = 1/2 P(x=1,y=1) = 0 我们发现P(x=1,y=0)的概率要比P(x=1,y=1)的概率大，所以，我们判断：x=1时，y=0。
对于判别模型，我们会比较：
P(y=0|x=1) = 1 P(y=1|x=1) = 0 同样，P(y=0|x=1)要比P(y=1|x=1)大，所以，我们判断：x=1时，y=0。
我们看到，虽然最后预测的结果一样，但是得出结果的逻辑却是完全不同的。两个栗子说完，你心里感到很痛快，面试官脸上也露出了赞赏的微笑，但是，他突然问了一个问题。
生成模型为啥叫生成模型 这个问题着实让你没想到，不过，聪明的你略加思考，应该就可以想到。生成模型之所以叫生成模型，是因为，它背后的思想是，x是特征，y是标签，什么样的标签就会生成什么样的特征。好比说，标签是大象，那么可能生成的特征就有大耳朵，长鼻子等等。
当我们来根据x来判断y时，我们实际上是在比较，什么样的y标签更可能生成特征x，我们预测的结果就是更可能生成x特征的y标签。
常见的生成模型和判别模型有哪些呢 生成模型
HMM
朴素贝叶斯
判别模型
逻辑回归
SVM
CRF
最近邻
一般的神经网络</p></div><footer class=entry-footer><span title='2022-06-07 19:39:13 +0800 +0800'>2022-06-07</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Reid</footer><a class=entry-link aria-label="post link to 生成模型vs判别模型" href=https://reid00.github.io/posts/ml/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8Bvs%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/></a></article><article class=post-entry><header class=entry-header><h2>特征工程之特征选择</h2></header><div class=entry-content><p>Summary 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。由此可见，特征工程在机器学习中占有相当重要的地位。在实际应用当中，可以说特征工程是机器学习成功的关键。
那特征工程是什么？
​ 特征工程是利用数据领域的相关知识来创建能够使机器学习算法达到最佳性能的特征的过程。
特征工程又包含了Feature Selection（特征选择）、Feature Extraction（特征提取）和Feature construction（特征构造）等子问题，本章内容主要讨论特征选择相关的方法及实现。
在实际项目中，我们可能会有大量的特征可使用，有的特征携带的信息丰富，有的特征携带的信息有重叠，有的特征则属于无关特征，如果所有特征不经筛选地全部作为训练特征，经常会出现维度灾难问题，甚至会降低模型的准确性。因此，我们需要进行特征筛选，排除无效/冗余的特征，把有用的特征挑选出来作为模型的训练数据。
特征选择介绍 特征按重要性分类 相关特征
对于学习任务（例如分类问题）有帮助，可以提升学习算法的效果
无关特征
对于我们的算法没有任何帮助，不会给算法的效果带来任何提升
冗余特征
不会对我们的算法带来新的信息，或者这种特征的信息可以由其他的特征推断出
特征选择的目的 对于一个特定的学习算法来说，哪一个特征是有效的是未知的。因此，需要从所有特征中选择出对于学习算法有益的相关特征。而且在实际应用中，经常会出现维度灾难问题。如果只选择所有特征中的部分特征构建模型，那么可以大大减少学习算法的运行时间，也可以增加模型的可解释性
特征选择的原则 获取尽可能小的特征子集，不显著降低分类精度、不影响分类分布以及特征子集应具有稳定、适应性强等特点
特征选择的方法 Filter 方法(过滤式) 先进行特征选择，然后去训练学习器，所以特征选择的过程与学习器无关。相当于先对特征进行过滤操作，然后用特征子集来训练分类器。
**主要思想：**对每一维特征“打分”，即给每一维的特征赋予权重，这样的权重就代表着该特征的重要性，然后依据权重排序。
主要方法：
卡方检验 信息增益 相关系数 优点: 运行速度快，是一种非常流行的特征选择方法。
**缺点：**无法提供反馈，特征选择的标准/规范的制定是在特征搜索算法中完成，学习算法无法向特征搜索算法传递对特征的需求。另外，可能处理某个特征时由于任意原因表示该特征不重要，但是该特征与其他特征结合起来则可能变得很重要。
Wrapper 方法 (封装式) 直接把最后要使用的分类器作为特征选择的评价函数，对于特定的分类器选择最优的特征子集。
主要思想： 将子集的选择看作是一个搜索寻优问题，生成不同的组合，对组合进行评价，再与其他的组合进行比较。这样就将子集的选择看作是一个优化问题，这里有很多的优化算法可以解决，尤其是一些启发式的优化算法，如GA、PSO（如：优化算法-粒子群算法）、DE、ABC（如：优化算法-人工蜂群算法）等。
主要方法:
递归特征消除算法 优点: 对特征进行搜索时围绕学习算法展开的，对特征选择的标准/规范是在学习算法的需求中展开的，能够考虑学习算法所属的任意学习偏差，从而确定最佳子特征，真正关注的是学习问题本身。由于每次尝试针对特定子集时必须运行学习算法，所以能够关注到学习算法的学习偏差/归纳偏差，因此封装能够发挥巨大的作用。
缺点: 运行速度远慢于过滤算法，实际应用用封装方法没有过滤方法流行。
Embedded 方法(嵌入式) 将特征选择嵌入到模型训练当中，其训练可能是相同的模型，但是特征选择完成后，还能给予特征选择完成的特征和模型训练出的超参数，再次训练优化。
主要思想: 在模型既定的情况下学习出对提高模型准确性最好的特征。也就是在确定模型的过程中，挑选出那些对模型的训练有重要意义的特征。
主要方法: 用带有L1正则化的项完成特征选择（也可以结合L2惩罚项来优化）、随机森林平均不纯度减少法/平均精确度减少法。
优点: 对特征进行搜索时围绕学习算法展开的，能够考虑学习算法所属的任意学习偏差。训练模型的次数小于Wrapper方法，比较节省时间。
缺点: 运行速度慢
特征选择的实现方法 从两个方面考虑来选择特征： 特征是否发散： 如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。
假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。如果100%都是1，那这个特征就没意义了。
**特征与目标的相关性：**这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。
Filter: 卡方检验 经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：
不难发现，这个统计量的含义简而言之就是自变量对因变量的相关性。用feature_selection库的SelectKBest类结合卡方检验来选择特征的代码如下：
1 2 3 4 5 from sklearn....</p></div><footer class=entry-footer><span title='2022-06-07 19:33:26 +0800 +0800'>2022-06-07</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Reid</footer><a class=entry-link aria-label="post link to 特征工程之特征选择" href=https://reid00.github.io/posts/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B9%8B%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/></a></article><article class=post-entry><header class=entry-header><h2>数据降维之主成分分析 PCA</h2></header><div class=entry-content><p>Summary PCA 是无监督学习中最常见的数据降维方法，但是实际上问题特征很多的情况，PCA通常会预处理来减少特征个数。
将维的意义： 通过降维提高算法的效率 通过降维更方便数据的可视化，通过可视化我们可以更好的理解数据
相关统计概念 均值： 述的是样本集合的中间点。 方差： 概率论和统计方差衡量随机变量或一组数据时离散程度的度量。 标准差：而标准差给我们描述的是样本集合的各个样本点到均值的距离之平均。方差开根号。 标准差和方差一般是用来描述一维数据的 协方差: （多维）度量两个随机变量关系的统计量,来度量各个维度偏离其均值的程度。 协方差矩阵: （多维）度量各个维度偏离其均值的程度 当 cov(X, Y)>0时，表明X与Y正相关(X越大，Y也越大；X越小Y，也越小。) 当 cov(X, Y)&lt;0时，表明X与Y负相关； 当 cov(X, Y)=0时，表明X与Y不相关。 cov协方差=[(x1-x均值)(y1-y均值)+(x2-x均值)(y2-y均值)+…+(xn-x均值)*(yn-y均值)]/(n-1) PCA 思想 对数据进行归一化处理（代码中并非这么做的，而是直接减去均值） 计算归一化后的数据集的协方差矩阵 计算协方差矩阵的特征值和特征向量 将特征值排序 保留前N个最大的特征值对应的特征向量 将数据转换到上面得到的N个特征向量构建的新空间中（实现了特征压缩） 简述主成分分析PCA工作原理，以及PCA的优缺点？ PCA旨在找到数据中的主成分，并利用这些主成分表征原始数据，从而达到降维的目的。
​ 工作原理可由两个角度解释，第一个是最大化投影方差（让数据在主轴上投影的方差尽可能大）；第二个是最小化平方误差（样本点到超平面的垂直距离足够近）。
​ 做法是数据中心化之后，对样本数据协方差矩阵进行特征分解，选取前d个最大的特征值对应的特征向量，即可将数据从原来的p维降到d维，也可根据奇异值分解来求解主成分。
优点： 1.计算简单，易于实现
2.各主成分之间正交，可消除原始数据成分间的相互影响的因素
3.仅仅需要以方差衡量信息量，不受数据集以外的因素影响
4.降维维数木有限制，可根据需要制定
缺点： 1.无法利用类别的先验信息
2.降维后，只与数据有关，主成分各个维度的含义模糊，不易于解释
3.方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响
4.线性模型，对于复杂数据集难以处理（可用核映射方式改进）
PCA中有第一主成分、第二主成分，它们分别是什么，又是如何确定的？ 主成分分析是设法将原来众多具有一定相关性（比如P个指标），重新组合成一组新的互相无关的综合指标来代替原来的指标。主成分分析，是考察多个变量间相关性一种多元统计方法，研究如何通过少数几个主成分来揭示多个变量间的内部结构，即从原始变量中导出少数几个主成分，使它们尽可能多地保留原始变量的信息，且彼此间互不相关，通常数学上的处理就是将原来P个指标作线性组合，作为新的综合指标。
​ 最经典的做法就是用F1（选取的第一个线性组合，即第一个综合指标）的方差来表达，即Var(F1)越大，表示F1包含的信息越多。因此在所有的线性组合中选取的F1应该是方差最大的，故称F1为第一主成分。如果第一主成分不足以代表原来P个指标的信息，再考虑选取F2即选第二个线性组合，为了有效地反映原来信息，F1已有的信息就不需要再出现在F2中，用数学语言表达就是要求Cov(F1, F2)=0，则称F2为第二主成分，依此类推可以构造出第三、第四，……，第P个主成分。
LDA与PCA都是常用的降维方法，二者的区别 它其实是对数据在高维空间下的一个投影转换，通过一定的投影规则将原来从一个角度看到的多个维度映射成较少的维度。到底什么是映射，下面的图就可以很好地解释这个问题——正常角度看是两个半椭圆形分布的数据集，但经过旋转（映射）之后是两条线性分布数据集。
LDA与PCA都是常用的降维方法，二者的区别在于：
**出发思想不同。**PCA主要是从特征的协方差角度，去找到比较好的投影方式，即选择样本点投影具有最大方差的方向（ 在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。）；而LDA则更多的是考虑了分类标签信息，寻求投影后不同类别之间数据点距离更大化以及同一类别数据点距离最小化，即选择分类性能最好的方向。
**学习模式不同。**PCA属于无监督式学习，因此大多场景下只作为数据处理过程的一部分，需要与其他算法结合使用，例如将PCA与聚类、判别分析、回归分析等组合使用；LDA是一种监督式学习方法，本身除了可以降维外，还可以进行预测应用，因此既可以组合其他模型一起使用，也可以独立使用。
**降维后可用维度数量不同。**LDA降维后最多可生成C-1维子空间（分类标签数-1），因此LDA与原始维度N数量无关，只有数据标签分类数量有关；而PCA最多有n维度可用，即最大可以选择全部可用维度。
线性判别分析LDA算法由于其简单有效性在多个领域都得到了广泛地应用，是目前机器学习、数据挖掘领域经典且热门的一个算法；但是算法本身仍然存在一些局限性：
当样本数量远小于样本的特征维数，样本与样本之间的距离变大使得距离度量失效，使LDA算法中的类内、类间离散度矩阵奇异，不能得到最优的投影方向，在人脸识别领域中表现得尤为突出
LDA不适合对非高斯分布的样本进行降维
LDA在样本分类信息依赖方差而不是均值时，效果不好
LDA可能过度拟合数据
主成分分析 PCA 详解 原理及对应操作 主成分分析顾名思义是对主成分进行分析，那么找出主成分应该是key点。PCA的基本思想就是将初始数据集中的n维特征映射至k维上，得到的k维特征就可以被称作主成分，k维不是在n维中挑选出来的，而是以n维特征为基础重构出来的。...</p></div><footer class=entry-footer><span title='2022-06-07 19:28:59 +0800 +0800'>2022-06-07</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Reid</footer><a class=entry-link aria-label="post link to 数据降维之主成分分析 PCA" href=https://reid00.github.io/posts/ml/%E6%95%B0%E6%8D%AE%E9%99%8D%E7%BB%B4%E4%B9%8B%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-pca/></a></article><article class=post-entry><header class=entry-header><h2>特征工程之数据预处理</h2></header><div class=entry-content><p>Summary 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。由此可见，特征工程在机器学习中占有相当重要的地位。在实际应用当中，可以说特征工程是机器学习成功的关键。
什么是特征工程 特征工程又包含了Data PreProcessing（数据预处理）、Feature Extraction（特征提取）、Feature Selection（特征选择）和Feature construction（特征构造）等子问题，本章内容主要讨论数据预处理的方法及实现。 特征工程是机器学习中最重要的起始步骤，数据预处理是特征工程的最重要的起始步骤，而数据清洗是数据预处理的重要组成部分，会直接影响机器学习的效果。
数据清洗整体介绍 1. 箱线图分析异常值 箱线图提供了识别异常值的标准，如果一个数下雨 QL-1.5IQR or 大于OU + 1.5 IQR, 则这个值被称为异常值。
QL 下四分位数，表示四分之一的数据值比它小 QU　上四分位数，表示四分之一的数据值比它大 IRQ　四分位距，是QU－QL　的差值，包含了全部关差值的一般 2. 数据的光滑处理 除了检测出异常值然后再处理异常值外，还可以使用以下方法对异常数据进行光滑处理。
2.1. 变量分箱（即变量离散化) 离散特征的增加和减少都很容易，易于模型的快速迭代； 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力； 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问； 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。 可以将缺失作为独立的一类带入模型。 将所有变量变换到相似的尺度上。 2.1.0 变量分箱的方法 2.1.1 无序变量分箱 举个例子，在实际模型建立当中，有个 job 职业的特征，取值为（“国家机关人员”，“专业技术人员”，“商业服务人员”），对于这一类变量，如果我们将其依次赋值为（国家机关人员=1；专业技术人员=2；商业服务人员=3），就很容易产生一个问题，不同种类的职业在数据层面上就有了大小顺序之分，国家机关人员和商业服务人员的差距是2，专业技术人员和商业服务人员的之间的差距是1，而我们原来的中文分类中是不存在这种先后顺序关系的。所以这么简单的赋值是会使变量失去原来的衡量效果。
怎么处理这个问题呢? “一位有效编码” （one-hot Encoding）可以解决这个问题，通常叫做虚变量或者哑变量（dummpy variable）：比如职业特征有3个不同变量，那么将其生成个2哑变量，分别是“是否国家党政职业人员”，“是否专业技术人员” ，每个虚变量取值（1，0）。 为什么2个哑变量而非3个？ 在模型中引入多个虚拟变量时，虚拟变量的个数应按下列原则确定： 回归模型有截距：一般的，若该特征下n个属性均互斥（如，男/女;儿童/青年/中年/老年），在生成虚拟变量时，应该生成 n-1个虚变量，这样可以避免产生多重共线性 回归模型无截距项：有n个特征，设置n个虚拟变量 python 实现方法pd.get_dummies() 2.1.2 有序变量分箱 有序多分类变量是很常见的变量形式，通常在变量中有多个可能会出现的取值，各取值之间还存在等级关系。比如高血压分级（0=正常，1=正常高值，2=1级高血压，3=2级高血压，4=3级高血压）这类变量处理起来简直不要太省心，使用 pandas 中的 map（）替换相应变量就行。
1 2 3 4 5 import pandas as pd df= pd....</p></div><footer class=entry-footer><span title='2022-06-07 19:09:30 +0800 +0800'>2022-06-07</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Reid</footer><a class=entry-link aria-label="post link to 特征工程之数据预处理" href=https://reid00.github.io/posts/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://reid00.github.io/posts/page/7/>« Prev</a></nav></footer></main><footer class=footer><span>&copy; 2022 <a href=https://reid00.github.io/>Reid's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>