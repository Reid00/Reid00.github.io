<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>最常考的树模型问题 | Reid's Blog</title><meta name=keywords content="树模型,面试"><meta name=description content="最常考的树模型问题"><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/posts/ml/%E6%9C%80%E5%B8%B8%E8%80%83%E7%9A%84%E6%A0%91%E6%A8%A1%E5%9E%8B%E9%97%AE%E9%A2%98/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><head><meta name=referrer content="no-referrer"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK",{anonymize_ip:!1})}</script><meta property="og:title" content="最常考的树模型问题"><meta property="og:description" content="最常考的树模型问题"><meta property="og:type" content="article"><meta property="og:url" content="https://reid00.github.io/posts/ml/%E6%9C%80%E5%B8%B8%E8%80%83%E7%9A%84%E6%A0%91%E6%A8%A1%E5%9E%8B%E9%97%AE%E9%A2%98/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-16T19:35:21+08:00"><meta property="article:modified_time" content="2023-03-16T19:35:21+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="最常考的树模型问题"><meta name=twitter:description content="最常考的树模型问题"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://reid00.github.io/posts/"},{"@type":"ListItem","position":2,"name":"最常考的树模型问题","item":"https://reid00.github.io/posts/ml/%E6%9C%80%E5%B8%B8%E8%80%83%E7%9A%84%E6%A0%91%E6%A8%A1%E5%9E%8B%E9%97%AE%E9%A2%98/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"最常考的树模型问题","name":"最常考的树模型问题","description":"最常考的树模型问题","keywords":["树模型","面试"],"articleBody":"问题目录： 1、决策树的实现、ID3、C4.5、CART（贝壳） 2、CART回归树是怎么实现的？（贝壳） 3、CART分类树和ID3以及C4.5有什么区别（贝壳） 4、剪枝有哪几种方式（贝壳） 5、树集成模型有哪几种实现方式？（贝壳）boosting和bagging的区别是什么？（知乎、阿里） 6、随机森林的随机体现在哪些方面（贝壳、阿里） 7、AdaBoost是如何改变样本权重，GBDT分类树的基模型是？（贝壳） 8、gbdt,xgboost,lgbm的区别(百度、滴滴、阿里，头条) 9、bagging为什么能减小方差？（知乎）\n其他问题： 10、关于AUC的另一种解释：是挑选一个正样本和一个负样本，正样本排在负样本前面的概率？如何理解？ 11、校招是集中时间刷题好，还是每天刷一点好呢？ 12、现在推荐在工业界基本都用match+ranking的架构，但是学术界论文中的大多算法算是没有区分吗？end-to-end的方式，还是算是召回？ 13、内推刷简历严重么？没有实习经历，也没有牛逼的竞赛和论文，提前批有面试机会么？提前批影响正式批么？ 14、除了自己项目中的模型了解清楚，还需要准备哪些？看了群主的面经大概知道了一些，能否大致描述下？\n1、决策树的实现、ID3、C4.5、CART（贝壳） 这道题主要是要求把公式写一下，所以决策树的公式大家要理解，并且能熟练地写出来。这里咱们简单回顾一下吧。主要参考统计学习方法就好了。\nID3使用信息增益来指导树的分裂： C4.5通过信息增益比来指导树的分裂： CART的话既可以是分类树，也可以是回归树。当是分类树时，使用基尼系数来指导树的分裂： 当是回归树时，则使用的是平方损失最小： 2、CART回归树是怎么实现的？（贝壳） CART回归树的实现包含两个步骤： 1）决策树生成：基于训练数据生成决策树、生成的决策树要尽量大 2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。\n这部分的知识，可以看一下《统计学习方法》一书。\n3、CART分类树和ID3以及C4.5有什么区别（贝壳） 1）首先是决策规则的区别，CART分类树使用基尼系数、ID3使用的是信息增益，而C4.5使用的是信息增益比。 2）ID3和C4.5可以是多叉树，但是CART分类树只能是二叉树（这是我当时主要回答的点）\n4、剪枝有哪几种方式（贝壳） 前剪枝和后剪枝，参考周志华《机器学习》。\n5、树集成模型有哪几种实现方式？（贝壳）boosting和bagging的区别是什么？（知乎、阿里） 树集成模型主要有两种实现方式，分别是Bagging和Boosting。二者的区别主要有以下四点： 1）样本选择上： Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的. Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化.而权值是根据上一轮的分类结果进行调整. 2）样例权重： Bagging：使用均匀取样，每个样例的权重相等 Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大. 3）预测函数： Bagging：所有预测函数的权重相等. Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重. 4）并行计算： Bagging：各个预测函数可以并行生成 Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果.\n6、随机森林的随机体现在哪些方面（贝壳、阿里） 随机森林的随机主要体现在两个方面：一个是建立每棵树时所选择的特征是随机选择的；二是生成每棵树的样本也是通过有放回抽样产生的。\n7、AdaBoost是如何改变样本权重，GBDT分类树的基模型是？（贝壳） AdaBoost改变样本权重：增加分类错误的样本的权重，减小分类正确的样本的权重。\n最后一个问题是我在面试之前没有了解到的，GBDT无论做分类还是回归问题，使用的都是CART回归树。\n8、gbdt,xgboost,lgbm的区别(百度、滴滴、阿里，头条) 首先来看GBDT和Xgboost，二者的区别如下：\n1）传统 GBDT 以 CART 作为基分类器，xgboost 还支持线性分类器，这个时候 xgboost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归(分类问题)或者线性回归(回归问题)。 2）传统 GBDT 在优化时只用到一阶导数信息，xgboost 则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便 一下，xgboost 工具支持自定义代价函数，只要函数可一阶和二阶求导。 3）xgboost 在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的 score 的 L2 模的平方和。从 Bias-variance tradeoff 角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是 xgboost 优于传统GBDT 的一个特性。 4）Shrinkage(缩减)，相当于学习速率(xgboost 中的eta)。xgboost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削 弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把 eta 设置得小一点，然后迭代次数设置得大一点。(补充:传统 GBDT 的实现 也有学习速率) 5）列抽样(column subsampling)。xgboost 借鉴了随机森林的做法，支 持列抽样，不仅能降低过拟合，还能减少计算，这也是 xgboost 异于传 统 gbdt 的一个特性。 6）对缺失值的处理。对于特征的值有缺失的样本，xgboost 可以自动学习 出它的分裂方向。 7）xgboost 工具支持并行。boosting 不是一种串行的结构吗?怎么并行的? 注意 xgboost 的并行不是 tree 粒度的并行，xgboost 也是一次迭代完才能进行下一次迭代的(第 t 次迭代的代价函数里包含了前面 t-1 次迭代 的预测值)。xgboost 的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序(因为要确定最佳分割点)，xgboost在训练之前，预先对数据进行了排序，然后保存为 block 结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每 个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 8）可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以 xgboost 还 出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。\n再来看Xgboost和LightGBM，二者的区别如下：\n1）由于在决策树在每一次选择节点特征的过程中，要遍历所有的属性的所有取 值并选择一个较好的。XGBoost 使用的是近似算法算法，先对特征值进行排序 Pre-sort，然后根据二阶梯度进行分桶，能够更精确的找到数据分隔点;但是 复杂度较高。LightGBM 使用的是 histogram 算法，这种只需要将数据分割成不同的段即可，不需要进行预先的排序。占用的内存更低，数据分隔的复杂度更低。 2）决策树生长策略，我们刚才介绍过了，XGBoost采用的是 Level-wise 的树 生长策略，LightGBM 采用的是 leaf-wise 的生长策略。 3）并行策略对比，XGBoost 的并行主要集中在特征并行上，而 LightGBM 的并 行策略分特征并行，数据并行以及投票并行。\n9、bagging为什么能减小方差？（知乎） 这个当时也没有答上来，可以参考一下博客：https://blog.csdn.net/shenxiaoming77/article/details/53894973\n树模型相关的题目以上就差不多了。接下来整理一些最近群友提出的问题，我觉得有一些可能作为面试题，有一些是准备校招过程中的经验：\n10、关于AUC的另一种解释：是挑选一个正样本和一个负样本，正样本排在负样本前面的概率？如何理解？ 我们都知道AUC是ROC曲线下方的面积，ROC曲线的横轴是真正例率，纵轴是假正例率。我们可以按照如下的方式理解一下：首先偷换一下概念，意思还是一样的，任意给定一个负样本，所有正样本的score中有多大比例是大于该负类样本的score？那么对每个负样本来说，有多少的正样本的score比它的score大呢？是不是就是当结果按照score排序，阈值恰好为该负样本score时的真正例率TPR？理解到这一层，二者等价的关系也就豁然开朗了。ROC曲线下的面积或者说AUC的值 与 测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score是等价的。\n11、校招是集中时间刷题好，还是每天刷一点好呢？ 我的建议是平时每天刷3～5道，然后临近校招的时候集中刷。另外就是根据每次面试中被问到的问题，如果有没答上来的，就针对这一类的题型多刷刷。\n12、现在推荐在工业界基本都用match+ranking的架构，但是学术界论文中的大多算法算是没有区分吗？end-to-end的方式，还是算是召回？ 学术界论文往往不针对整个推荐系统，而只针对match或者ranking阶段的某一种方法进行研究。比如DeepFM、Wide \u0026 Deep，只针对ranking阶段。而阿里有几篇介绍embedding的论文，只介绍match阶段的方法。end-to-end的方式，在match和ranking阶段都有吧。\n","wordCount":"160","inLanguage":"en","datePublished":"2023-03-16T19:35:21+08:00","dateModified":"2023-03-16T19:35:21+08:00","author":[{"@type":"Person","name":"Reid"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://reid00.github.io/posts/ml/%E6%9C%80%E5%B8%B8%E8%80%83%E7%9A%84%E6%A0%91%E6%A8%A1%E5%9E%8B%E9%97%AE%E9%A2%98/"},"publisher":{"@type":"Organization","name":"Reid's Blog","logo":{"@type":"ImageObject","url":"https://reid00.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://reid00.github.io/ accesskey=h title="Reid's Blog (Alt + H)">Reid's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://reid00.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://reid00.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://reid00.github.io/categories/ title=Categorys><span>Categorys</span></a></li><li><a href=https://reid00.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://reid00.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/posts/>Posts</a></div><h1 class=post-title>最常考的树模型问题</h1><div class=post-description>最常考的树模型问题</div><div class=post-meta><span title='2023-03-16 19:35:21 +0800 +0800'>2023-03-16</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Reid</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e9%97%ae%e9%a2%98%e7%9b%ae%e5%bd%95 aria-label=问题目录：><strong>问题目录：</strong></a></li><li><a href=#%e5%85%b6%e4%bb%96%e9%97%ae%e9%a2%98 aria-label=其他问题：><strong>其他问题：</strong></a></li><li><a href=#1%e5%86%b3%e7%ad%96%e6%a0%91%e7%9a%84%e5%ae%9e%e7%8e%b0id3c45cart%e8%b4%9d%e5%a3%b3 aria-label=1、决策树的实现、ID3、C4.5、CART（贝壳）>1、决策树的实现、ID3、C4.5、CART（贝壳）</a></li><li><a href=#2cart%e5%9b%9e%e5%bd%92%e6%a0%91%e6%98%af%e6%80%8e%e4%b9%88%e5%ae%9e%e7%8e%b0%e7%9a%84%e8%b4%9d%e5%a3%b3 aria-label=2、CART回归树是怎么实现的？（贝壳）>2、CART回归树是怎么实现的？（贝壳）</a></li><li><a href=#3cart%e5%88%86%e7%b1%bb%e6%a0%91%e5%92%8cid3%e4%bb%a5%e5%8f%8ac45%e6%9c%89%e4%bb%80%e4%b9%88%e5%8c%ba%e5%88%ab%e8%b4%9d%e5%a3%b3 aria-label=3、CART分类树和ID3以及C4.5有什么区别（贝壳）>3、CART分类树和ID3以及C4.5有什么区别（贝壳）</a></li><li><a href=#4%e5%89%aa%e6%9e%9d%e6%9c%89%e5%93%aa%e5%87%a0%e7%a7%8d%e6%96%b9%e5%bc%8f%e8%b4%9d%e5%a3%b3 aria-label=4、剪枝有哪几种方式（贝壳）>4、剪枝有哪几种方式（贝壳）</a></li><li><a href=#5%e6%a0%91%e9%9b%86%e6%88%90%e6%a8%a1%e5%9e%8b%e6%9c%89%e5%93%aa%e5%87%a0%e7%a7%8d%e5%ae%9e%e7%8e%b0%e6%96%b9%e5%bc%8f%e8%b4%9d%e5%a3%b3boosting%e5%92%8cbagging%e7%9a%84%e5%8c%ba%e5%88%ab%e6%98%af%e4%bb%80%e4%b9%88%e7%9f%a5%e4%b9%8e%e9%98%bf%e9%87%8c aria-label=5、树集成模型有哪几种实现方式？（贝壳）boosting和bagging的区别是什么？（知乎、阿里）>5、树集成模型有哪几种实现方式？（贝壳）boosting和bagging的区别是什么？（知乎、阿里）</a></li><li><a href=#6%e9%9a%8f%e6%9c%ba%e6%a3%ae%e6%9e%97%e7%9a%84%e9%9a%8f%e6%9c%ba%e4%bd%93%e7%8e%b0%e5%9c%a8%e5%93%aa%e4%ba%9b%e6%96%b9%e9%9d%a2%e8%b4%9d%e5%a3%b3%e9%98%bf%e9%87%8c aria-label=6、随机森林的随机体现在哪些方面（贝壳、阿里）>6、随机森林的随机体现在哪些方面（贝壳、阿里）</a></li><li><a href=#7adaboost%e6%98%af%e5%a6%82%e4%bd%95%e6%94%b9%e5%8f%98%e6%a0%b7%e6%9c%ac%e6%9d%83%e9%87%8dgbdt%e5%88%86%e7%b1%bb%e6%a0%91%e7%9a%84%e5%9f%ba%e6%a8%a1%e5%9e%8b%e6%98%af%e8%b4%9d%e5%a3%b3 aria-label=7、AdaBoost是如何改变样本权重，GBDT分类树的基模型是？（贝壳）>7、AdaBoost是如何改变样本权重，GBDT分类树的基模型是？（贝壳）</a></li><li><a href=#8gbdtxgboostlgbm%e7%9a%84%e5%8c%ba%e5%88%ab%e7%99%be%e5%ba%a6%e6%bb%b4%e6%bb%b4%e9%98%bf%e9%87%8c%e5%a4%b4%e6%9d%a1 aria-label=8、gbdt,xgboost,lgbm的区别(百度、滴滴、阿里，头条)>8、gbdt,xgboost,lgbm的区别(百度、滴滴、阿里，头条)</a></li><li><a href=#9bagging%e4%b8%ba%e4%bb%80%e4%b9%88%e8%83%bd%e5%87%8f%e5%b0%8f%e6%96%b9%e5%b7%ae%e7%9f%a5%e4%b9%8e aria-label=9、bagging为什么能减小方差？（知乎）>9、bagging为什么能减小方差？（知乎）</a></li><li><a href=#10%e5%85%b3%e4%ba%8eauc%e7%9a%84%e5%8f%a6%e4%b8%80%e7%a7%8d%e8%a7%a3%e9%87%8a%e6%98%af%e6%8c%91%e9%80%89%e4%b8%80%e4%b8%aa%e6%ad%a3%e6%a0%b7%e6%9c%ac%e5%92%8c%e4%b8%80%e4%b8%aa%e8%b4%9f%e6%a0%b7%e6%9c%ac%e6%ad%a3%e6%a0%b7%e6%9c%ac%e6%8e%92%e5%9c%a8%e8%b4%9f%e6%a0%b7%e6%9c%ac%e5%89%8d%e9%9d%a2%e7%9a%84%e6%a6%82%e7%8e%87%e5%a6%82%e4%bd%95%e7%90%86%e8%a7%a3 aria-label=10、关于AUC的另一种解释：是挑选一个正样本和一个负样本，正样本排在负样本前面的概率？如何理解？>10、关于AUC的另一种解释：是挑选一个正样本和一个负样本，正样本排在负样本前面的概率？如何理解？</a></li><li><a href=#11%e6%a0%a1%e6%8b%9b%e6%98%af%e9%9b%86%e4%b8%ad%e6%97%b6%e9%97%b4%e5%88%b7%e9%a2%98%e5%a5%bd%e8%bf%98%e6%98%af%e6%af%8f%e5%a4%a9%e5%88%b7%e4%b8%80%e7%82%b9%e5%a5%bd%e5%91%a2 aria-label=11、校招是集中时间刷题好，还是每天刷一点好呢？>11、校招是集中时间刷题好，还是每天刷一点好呢？</a></li><li><a href=#12%e7%8e%b0%e5%9c%a8%e6%8e%a8%e8%8d%90%e5%9c%a8%e5%b7%a5%e4%b8%9a%e7%95%8c%e5%9f%ba%e6%9c%ac%e9%83%bd%e7%94%a8matchranking%e7%9a%84%e6%9e%b6%e6%9e%84%e4%bd%86%e6%98%af%e5%ad%a6%e6%9c%af%e7%95%8c%e8%ae%ba%e6%96%87%e4%b8%ad%e7%9a%84%e5%a4%a7%e5%a4%9a%e7%ae%97%e6%b3%95%e7%ae%97%e6%98%af%e6%b2%a1%e6%9c%89%e5%8c%ba%e5%88%86%e5%90%97end-to-end%e7%9a%84%e6%96%b9%e5%bc%8f%e8%bf%98%e6%98%af%e7%ae%97%e6%98%af%e5%8f%ac%e5%9b%9e aria-label=12、现在推荐在工业界基本都用match+ranking的架构，但是学术界论文中的大多算法算是没有区分吗？end-to-end的方式，还是算是召回？>12、现在推荐在工业界基本都用match+ranking的架构，但是学术界论文中的大多算法算是没有区分吗？end-to-end的方式，还是算是召回？</a></li></ul></div></details></div><div class=post-content><h4 id=问题目录><strong>问题目录：</strong><a hidden class=anchor aria-hidden=true href=#问题目录>#</a></h4><p>1、决策树的实现、ID3、C4.5、CART（贝壳）
2、CART回归树是怎么实现的？（贝壳）
3、CART分类树和ID3以及C4.5有什么区别（贝壳）
4、剪枝有哪几种方式（贝壳）
5、树集成模型有哪几种实现方式？（贝壳）boosting和bagging的区别是什么？（知乎、阿里）
6、随机森林的随机体现在哪些方面（贝壳、阿里）
7、AdaBoost是如何改变样本权重，GBDT分类树的基模型是？（贝壳）
8、gbdt,xgboost,lgbm的区别(百度、滴滴、阿里，头条)
9、bagging为什么能减小方差？（知乎）</p><h4 id=其他问题><strong>其他问题：</strong><a hidden class=anchor aria-hidden=true href=#其他问题>#</a></h4><p>10、关于AUC的另一种解释：是挑选一个正样本和一个负样本，正样本排在负样本前面的概率？如何理解？
11、校招是集中时间刷题好，还是每天刷一点好呢？
12、现在推荐在工业界基本都用match+ranking的架构，但是学术界论文中的大多算法算是没有区分吗？end-to-end的方式，还是算是召回？
13、内推刷简历严重么？没有实习经历，也没有牛逼的竞赛和论文，提前批有面试机会么？提前批影响正式批么？
14、除了自己项目中的模型了解清楚，还需要准备哪些？看了群主的面经大概知道了一些，能否大致描述下？</p><h4 id=1决策树的实现id3c45cart贝壳>1、决策树的实现、ID3、C4.5、CART（贝壳）<a hidden class=anchor aria-hidden=true href=#1决策树的实现id3c45cart贝壳>#</a></h4><p>这道题主要是要求把公式写一下，所以决策树的公式大家要理解，并且能熟练地写出来。这里咱们简单回顾一下吧。主要参考统计学习方法就好了。</p><p>ID3使用信息增益来指导树的分裂：
<img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.52hmsiov6sc0.webp alt=id3></p><p>C4.5通过信息增益比来指导树的分裂：
<img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.3o3xcavy9si0.webp alt=c4.5></p><p>CART的话既可以是分类树，也可以是回归树。当是分类树时，使用基尼系数来指导树的分裂：
<img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.2mb92paed020.webp alt=cart></p><p>当是回归树时，则使用的是平方损失最小：
<img loading=lazy src=https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.3kv8v81g8vo0.webp alt=regression-tree></p><h4 id=2cart回归树是怎么实现的贝壳>2、CART回归树是怎么实现的？（贝壳）<a hidden class=anchor aria-hidden=true href=#2cart回归树是怎么实现的贝壳>#</a></h4><p>CART回归树的实现包含两个步骤：
1）决策树生成：基于训练数据生成决策树、生成的决策树要尽量大
2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。</p><p>这部分的知识，可以看一下《统计学习方法》一书。</p><h4 id=3cart分类树和id3以及c45有什么区别贝壳>3、CART分类树和ID3以及C4.5有什么区别（贝壳）<a hidden class=anchor aria-hidden=true href=#3cart分类树和id3以及c45有什么区别贝壳>#</a></h4><p>1）首先是决策规则的区别，CART分类树使用基尼系数、ID3使用的是信息增益，而C4.5使用的是信息增益比。
2）ID3和C4.5可以是多叉树，但是CART分类树只能是二叉树（这是我当时主要回答的点）</p><h4 id=4剪枝有哪几种方式贝壳>4、剪枝有哪几种方式（贝壳）<a hidden class=anchor aria-hidden=true href=#4剪枝有哪几种方式贝壳>#</a></h4><p>前剪枝和后剪枝，参考周志华《机器学习》。</p><h4 id=5树集成模型有哪几种实现方式贝壳boosting和bagging的区别是什么知乎阿里>5、树集成模型有哪几种实现方式？（贝壳）boosting和bagging的区别是什么？（知乎、阿里）<a hidden class=anchor aria-hidden=true href=#5树集成模型有哪几种实现方式贝壳boosting和bagging的区别是什么知乎阿里>#</a></h4><p>树集成模型主要有两种实现方式，分别是Bagging和Boosting。二者的区别主要有以下四点：
1）样本选择上：
Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的.
Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化.而权值是根据上一轮的分类结果进行调整.
2）样例权重：
Bagging：使用均匀取样，每个样例的权重相等
Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大.
3）预测函数：
Bagging：所有预测函数的权重相等.
Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重.
4）并行计算：
Bagging：各个预测函数可以并行生成
Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果.</p><h4 id=6随机森林的随机体现在哪些方面贝壳阿里>6、随机森林的随机体现在哪些方面（贝壳、阿里）<a hidden class=anchor aria-hidden=true href=#6随机森林的随机体现在哪些方面贝壳阿里>#</a></h4><p>随机森林的随机主要体现在两个方面：一个是建立每棵树时所选择的特征是随机选择的；二是生成每棵树的样本也是通过有放回抽样产生的。</p><h4 id=7adaboost是如何改变样本权重gbdt分类树的基模型是贝壳>7、AdaBoost是如何改变样本权重，GBDT分类树的基模型是？（贝壳）<a hidden class=anchor aria-hidden=true href=#7adaboost是如何改变样本权重gbdt分类树的基模型是贝壳>#</a></h4><p>AdaBoost改变样本权重：增加分类错误的样本的权重，减小分类正确的样本的权重。</p><p>最后一个问题是我在面试之前没有了解到的，GBDT无论做分类还是回归问题，使用的都是CART回归树。</p><h4 id=8gbdtxgboostlgbm的区别百度滴滴阿里头条>8、gbdt,xgboost,lgbm的区别(百度、滴滴、阿里，头条)<a hidden class=anchor aria-hidden=true href=#8gbdtxgboostlgbm的区别百度滴滴阿里头条>#</a></h4><p>首先来看GBDT和Xgboost，二者的区别如下：</p><p>1）传统 GBDT 以 CART 作为基分类器，xgboost 还支持线性分类器，这个时候 xgboost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归(分类问题)或者线性回归(回归问题)。
2）传统 GBDT 在优化时只用到一阶导数信息，xgboost 则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便 一下，xgboost 工具支持自定义代价函数，只要函数可一阶和二阶求导。
3）xgboost 在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的 score 的 L2 模的平方和。从 Bias-variance tradeoff 角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是 xgboost 优于传统GBDT 的一个特性。
4）Shrinkage(缩减)，相当于学习速率(xgboost 中的eta)。xgboost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削 弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把 eta 设置得小一点，然后迭代次数设置得大一点。(补充:传统 GBDT 的实现 也有学习速率)
5）列抽样(column subsampling)。xgboost 借鉴了随机森林的做法，支 持列抽样，不仅能降低过拟合，还能减少计算，这也是 xgboost 异于传 统 gbdt 的一个特性。
6）对缺失值的处理。对于特征的值有缺失的样本，xgboost 可以自动学习 出它的分裂方向。
7）xgboost 工具支持并行。boosting 不是一种串行的结构吗?怎么并行的? 注意 xgboost 的并行不是 tree 粒度的并行，xgboost 也是一次迭代完才能进行下一次迭代的(第 t 次迭代的代价函数里包含了前面 t-1 次迭代 的预测值)。xgboost 的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序(因为要确定最佳分割点)，xgboost在训练之前，预先对数据进行了排序，然后保存为 block 结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每 个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
8）可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以 xgboost 还 出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。</p><p>再来看Xgboost和LightGBM，二者的区别如下：</p><p>1）由于在决策树在每一次选择节点特征的过程中，要遍历所有的属性的所有取 值并选择一个较好的。XGBoost 使用的是近似算法算法，先对特征值进行排序 Pre-sort，然后根据二阶梯度进行分桶，能够更精确的找到数据分隔点;但是 复杂度较高。LightGBM 使用的是 histogram 算法，这种只需要将数据分割成不同的段即可，不需要进行预先的排序。占用的内存更低，数据分隔的复杂度更低。
2）决策树生长策略，我们刚才介绍过了，XGBoost采用的是 Level-wise 的树 生长策略，LightGBM 采用的是 leaf-wise 的生长策略。
3）并行策略对比，XGBoost 的并行主要集中在特征并行上，而 LightGBM 的并 行策略分特征并行，数据并行以及投票并行。</p><h4 id=9bagging为什么能减小方差知乎>9、bagging为什么能减小方差？（知乎）<a hidden class=anchor aria-hidden=true href=#9bagging为什么能减小方差知乎>#</a></h4><p>这个当时也没有答上来，可以参考一下博客：https://blog.csdn.net/shenxiaoming77/article/details/53894973</p><p>树模型相关的题目以上就差不多了。接下来整理一些最近群友提出的问题，我觉得有一些可能作为面试题，有一些是准备校招过程中的经验：</p><h4 id=10关于auc的另一种解释是挑选一个正样本和一个负样本正样本排在负样本前面的概率如何理解>10、关于AUC的另一种解释：是挑选一个正样本和一个负样本，正样本排在负样本前面的概率？如何理解？<a hidden class=anchor aria-hidden=true href=#10关于auc的另一种解释是挑选一个正样本和一个负样本正样本排在负样本前面的概率如何理解>#</a></h4><p>我们都知道AUC是ROC曲线下方的面积，ROC曲线的横轴是真正例率，纵轴是假正例率。我们可以按照如下的方式理解一下：首先偷换一下概念，意思还是一样的，任意给定一个负样本，所有正样本的score中有多大比例是大于该负类样本的score？那么对每个负样本来说，有多少的正样本的score比它的score大呢？是不是就是当结果按照score排序，阈值恰好为该负样本score时的真正例率TPR？理解到这一层，二者等价的关系也就豁然开朗了。ROC曲线下的面积或者说AUC的值 与 测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score是等价的。</p><h4 id=11校招是集中时间刷题好还是每天刷一点好呢>11、校招是集中时间刷题好，还是每天刷一点好呢？<a hidden class=anchor aria-hidden=true href=#11校招是集中时间刷题好还是每天刷一点好呢>#</a></h4><p>我的建议是平时每天刷3～5道，然后临近校招的时候集中刷。另外就是根据每次面试中被问到的问题，如果有没答上来的，就针对这一类的题型多刷刷。</p><h4 id=12现在推荐在工业界基本都用matchranking的架构但是学术界论文中的大多算法算是没有区分吗end-to-end的方式还是算是召回>12、现在推荐在工业界基本都用match+ranking的架构，但是学术界论文中的大多算法算是没有区分吗？end-to-end的方式，还是算是召回？<a hidden class=anchor aria-hidden=true href=#12现在推荐在工业界基本都用matchranking的架构但是学术界论文中的大多算法算是没有区分吗end-to-end的方式还是算是召回>#</a></h4><p>学术界论文往往不针对整个推荐系统，而只针对match或者ranking阶段的某一种方法进行研究。比如DeepFM、Wide & Deep，只针对ranking阶段。而阿里有几篇介绍embedding的论文，只介绍match阶段的方法。end-to-end的方式，在match和ranking阶段都有吧。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://reid00.github.io/tags/%E6%A0%91%E6%A8%A1%E5%9E%8B/>树模型</a></li><li><a href=https://reid00.github.io/tags/%E9%9D%A2%E8%AF%95/>面试</a></li></ul><nav class=paginav><a class=prev href=https://reid00.github.io/posts/ml/%E6%95%B0%E6%8D%AE%E9%99%8D%E7%BB%B4%E4%B9%8B%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-pca/><span class=title>« Prev</span><br><span>数据降维之主成分分析 PCA</span></a>
<a class=next href=https://reid00.github.io/posts/ml/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%B0%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/><span class=title>Next »</span><br><span>决策树到随机森林</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2023 <a href=https://reid00.github.io/>Reid's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>