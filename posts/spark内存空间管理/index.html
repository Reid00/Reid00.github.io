<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Spark内存空间管理 | Reid's Blog</title><meta name=keywords content="内存空间管理,Spark"><meta name=description content="1. 概述 Spark应用在yarn运行模式下，其以Executor Container的形式存在，container能申请到的最大内存受yarn.scheduler.maximum-allocation-mb限制。下面说的大部分内容其实与yarn等没有多少直接关系，知识均为通用的。
Spark应用运行过程中的内存可以分为堆内内存与堆外内存，其中堆内内存onheap由spark.executor.memory指定，堆外内存offheap由spark.yarn.executor.memoryOverhead参数指定，默认为executorMemory*0.1,最小384M。堆内内存executorMemory是spark使用的主要部分，其大小通过-Xmx参数传给jvm，内部有300M的保留资源不被executor使用。这里的堆外内存部分主要用于JVM自身，如字符串、NIO Buffer等开销，此部分用户代码及spark都无法直接操作。
executor执行的时候，用的内存可能会超过executor-memory，所以会为executor额外预留一部分内存，spark.yarn.executor.memoryOverhead即代表这部分内存。
另外还有部分堆外内存由spark.memory.offHeap.enabled及spark.memory.offHeap.size控制的堆外内存，这部分也归offheap，但主要是供统一内存管理使用的。 2. 堆内内存 1 2 3 4 5 6 7 object UnifiedMemoryManager { // Set aside a fixed amount of memory for non-storage, non-execution purposes. // This serves a function similar to `spark.memory.fraction`, but guarantees that we reserve // sufficient memory for the system even for small heaps. E.g. if we have a 1GB JVM, then // the memory used for execution and storage will be (1024 - 300) * 0."><meta name=author content="Reid"><link rel=canonical href=https://reid00.github.io/posts/spark%E5%86%85%E5%AD%98%E7%A9%BA%E9%97%B4%E7%AE%A1%E7%90%86/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://reid00.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://reid00.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://reid00.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://reid00.github.io/apple-touch-icon.png><link rel=mask-icon href=https://reid00.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRR6GRNQGK"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QRR6GRNQGK",{anonymize_ip:!1})}</script><meta property="og:title" content="Spark内存空间管理"><meta property="og:description" content="1. 概述 Spark应用在yarn运行模式下，其以Executor Container的形式存在，container能申请到的最大内存受yarn.scheduler.maximum-allocation-mb限制。下面说的大部分内容其实与yarn等没有多少直接关系，知识均为通用的。
Spark应用运行过程中的内存可以分为堆内内存与堆外内存，其中堆内内存onheap由spark.executor.memory指定，堆外内存offheap由spark.yarn.executor.memoryOverhead参数指定，默认为executorMemory*0.1,最小384M。堆内内存executorMemory是spark使用的主要部分，其大小通过-Xmx参数传给jvm，内部有300M的保留资源不被executor使用。这里的堆外内存部分主要用于JVM自身，如字符串、NIO Buffer等开销，此部分用户代码及spark都无法直接操作。
executor执行的时候，用的内存可能会超过executor-memory，所以会为executor额外预留一部分内存，spark.yarn.executor.memoryOverhead即代表这部分内存。
另外还有部分堆外内存由spark.memory.offHeap.enabled及spark.memory.offHeap.size控制的堆外内存，这部分也归offheap，但主要是供统一内存管理使用的。 2. 堆内内存 1 2 3 4 5 6 7 object UnifiedMemoryManager { // Set aside a fixed amount of memory for non-storage, non-execution purposes. // This serves a function similar to `spark.memory.fraction`, but guarantees that we reserve // sufficient memory for the system even for small heaps. E.g. if we have a 1GB JVM, then // the memory used for execution and storage will be (1024 - 300) * 0."><meta property="og:type" content="article"><meta property="og:url" content="https://reid00.github.io/posts/spark%E5%86%85%E5%AD%98%E7%A9%BA%E9%97%B4%E7%AE%A1%E7%90%86/"><meta property="og:image" content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-03T16:14:39+08:00"><meta property="article:modified_time" content="2022-11-03T16:14:39+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://i.loli.net/2021/09/26/3OMGXylm8HUYJ6p.png"><meta name=twitter:title content="Spark内存空间管理"><meta name=twitter:description content="1. 概述 Spark应用在yarn运行模式下，其以Executor Container的形式存在，container能申请到的最大内存受yarn.scheduler.maximum-allocation-mb限制。下面说的大部分内容其实与yarn等没有多少直接关系，知识均为通用的。
Spark应用运行过程中的内存可以分为堆内内存与堆外内存，其中堆内内存onheap由spark.executor.memory指定，堆外内存offheap由spark.yarn.executor.memoryOverhead参数指定，默认为executorMemory*0.1,最小384M。堆内内存executorMemory是spark使用的主要部分，其大小通过-Xmx参数传给jvm，内部有300M的保留资源不被executor使用。这里的堆外内存部分主要用于JVM自身，如字符串、NIO Buffer等开销，此部分用户代码及spark都无法直接操作。
executor执行的时候，用的内存可能会超过executor-memory，所以会为executor额外预留一部分内存，spark.yarn.executor.memoryOverhead即代表这部分内存。
另外还有部分堆外内存由spark.memory.offHeap.enabled及spark.memory.offHeap.size控制的堆外内存，这部分也归offheap，但主要是供统一内存管理使用的。 2. 堆内内存 1 2 3 4 5 6 7 object UnifiedMemoryManager { // Set aside a fixed amount of memory for non-storage, non-execution purposes. // This serves a function similar to `spark.memory.fraction`, but guarantees that we reserve // sufficient memory for the system even for small heaps. E.g. if we have a 1GB JVM, then // the memory used for execution and storage will be (1024 - 300) * 0."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://reid00.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Spark内存空间管理","item":"https://reid00.github.io/posts/spark%E5%86%85%E5%AD%98%E7%A9%BA%E9%97%B4%E7%AE%A1%E7%90%86/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Spark内存空间管理","name":"Spark内存空间管理","description":"1. 概述 Spark应用在yarn运行模式下，其以Executor Container的形式存在，container能申请到的最大内存受yarn.scheduler.maximum-allocation-mb限制。下面说的大部分内容其实与yarn等没有多少直接关系，知识均为通用的。\nSpark应用运行过程中的内存可以分为堆内内存与堆外内存，其中堆内内存onheap由spark.executor.memory指定，堆外内存offheap由spark.yarn.executor.memoryOverhead参数指定，默认为executorMemory*0.1,最小384M。堆内内存executorMemory是spark使用的主要部分，其大小通过-Xmx参数传给jvm，内部有300M的保留资源不被executor使用。这里的堆外内存部分主要用于JVM自身，如字符串、NIO Buffer等开销，此部分用户代码及spark都无法直接操作。\nexecutor执行的时候，用的内存可能会超过executor-memory，所以会为executor额外预留一部分内存，spark.yarn.executor.memoryOverhead即代表这部分内存。\n另外还有部分堆外内存由spark.memory.offHeap.enabled及spark.memory.offHeap.size控制的堆外内存，这部分也归offheap，但主要是供统一内存管理使用的。 2. 堆内内存 1 2 3 4 5 6 7 object UnifiedMemoryManager { // Set aside a fixed amount of memory for non-storage, non-execution purposes. // This serves a function similar to `spark.memory.fraction`, but guarantees that we reserve // sufficient memory for the system even for small heaps. E.g. if we have a 1GB JVM, then // the memory used for execution and storage will be (1024 - 300) * 0.","keywords":["内存空间管理","Spark"],"articleBody":"1. 概述 Spark应用在yarn运行模式下，其以Executor Container的形式存在，container能申请到的最大内存受yarn.scheduler.maximum-allocation-mb限制。下面说的大部分内容其实与yarn等没有多少直接关系，知识均为通用的。\nSpark应用运行过程中的内存可以分为堆内内存与堆外内存，其中堆内内存onheap由spark.executor.memory指定，堆外内存offheap由spark.yarn.executor.memoryOverhead参数指定，默认为executorMemory*0.1,最小384M。堆内内存executorMemory是spark使用的主要部分，其大小通过-Xmx参数传给jvm，内部有300M的保留资源不被executor使用。这里的堆外内存部分主要用于JVM自身，如字符串、NIO Buffer等开销，此部分用户代码及spark都无法直接操作。\nexecutor执行的时候，用的内存可能会超过executor-memory，所以会为executor额外预留一部分内存，spark.yarn.executor.memoryOverhead即代表这部分内存。\n另外还有部分堆外内存由spark.memory.offHeap.enabled及spark.memory.offHeap.size控制的堆外内存，这部分也归offheap，但主要是供统一内存管理使用的。 2. 堆内内存 1 2 3 4 5 6 7 object UnifiedMemoryManager { // Set aside a fixed amount of memory for non-storage, non-execution purposes. // This serves a function similar to `spark.memory.fraction`, but guarantees that we reserve // sufficient memory for the system even for small heaps. E.g. if we have a 1GB JVM, then // the memory used for execution and storage will be (1024 - 300) * 0.6 = 434MB by default. private val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024 堆内内存有300M的保留资源，此外的可用内存usableMemory被分为spark管理的内存和用户管理的内存两部分，spark管理的内存通过spark.memory.fraction进行控制，默认0.6。\nSpark管理的统一内存： 在设置了executor memory为3G时，debug代码 其各部分值如下：\nsystemMemory=3087007744 //container的JVM最多可用的内存 reservedMemory=314572800 //保留的300M minSystemMemory=471859200 //300M*1.5 executorMemory=3221225472 // 通过spark.executor.memory指定的值3g usableMemory=2772434944 //为systemMemory-reservedMemory 由上，spark可管理的内存大小为 1 2 3 注意： usableMemory 不是User Memory(有些也叫做other Memory) 实际为spark-submit 提交时申请的exector-memory 大小 - reservedMemory usableMemory * memoryFraction=2772434944 *0.6=1,663,460,966 这块内存在spark中被称为unified region(代号M)或统一内存或可用内存，其进一步被分为执行内存ExecutionMemory和StorageMemory，见上图。其中storage memory(代号R)是M的一个subregion，其的大小占比受spark.memory.storageFraction控制，默认为0.5，即默认占usableMemory的 0.6*0.5=0.3。我们用onHeapStorageRegionSize来表示storage这部分的大小。\nExecutionMemory执行内存：主要存储Shuffle、Join、Sort、Aggregation等计算过程中的临时数据； StorageMemory存储内存：主要存储spark的cache数据，如RDD.cache RDD.persist在调用时的数据存储，用户自定义变量及系统的广播变量等 这两块内存在当前默认的UnifiedMemoryManager(Spark1.6引入)下是可以互相动态侵占的，即Execution内存不足时可以占用Storage的内存，反之亦然，其详细规则如下：\nExecution内存不足且onHeapStorageRegionSize有空闲时，可以向Storage Memory借用内存，- 但借用后storage不能将execution占用的部分驱逐evict出去，只能等着Execution自己释放。 Storage内存不足时可以借用Execution的内存，且当Execution又有内存资源需求时可以驱逐Storage占用的部分，但只能驱逐StorageMemory-onHeapStorageRegionSize的大小，原来划定的onHeapStorageRegionSize且在使用的不可被抢占。 在spark的WebUI下，我们会看到Executors的信息如下图所示 我指定的executor-memory=5g,此处显示的StorageMemory其实是Spark的可用内存，包括Execution和Storage部分。(5G - 300M) * 0.6 = 2.7 用户管理的内存(Other)： 上面说了占可用内存spark.memory.fraction(0.6)的spark 统一内存，另外0.4的用户内存用于存储用户代码生成的对象及RDD依赖等,用户在处理partition中的记录时，其遍历到的记录可以看做存储在Other区，当需要将RDD缓存时，将会序列化或不序列化的方式以Block的形式存储到Storage内存中。 3. 堆外内存 前面说了，堆外内存有的是参数spark.yarn.executor.memoryOverhead控制，有的是参数spark.memory.offHeap.size控制，这个都算offheap内存，不过前者主要用于JVM运行自身，字符串, NIO Buffer等开销，而后者主要是供统一内存管理用作Execution Memory及Storage Memory的用途。\nspark.yarn.executor.memoryOverhead设置的内存默认为executor.memory的0.1倍，最低384M，这个始终存在的，在采用yarn时，这块内存是包含在申请的容器内的，即申请容器大小大于spark.executor.memory+spark.yarn.executor.memoryOverhead。\n而通过spark.memory.offHeap.enable/size申请的内存不在JVM内，Spark利用TungSten技术直接操作管理JVM外的原生内存。主要是为了解决Java对象开销大和GC的问题。 1 2 3 4 5 6 protected[this] val maxOffHeapMemory = conf.get(MEMORY_OFFHEAP_SIZE) protected[this] val offHeapStorageMemory = (maxOffHeapMemory * conf.getDouble(\"spark.memory.storageFraction\", 0.5)).toLong offHeapExecutionMemoryPool.incrementPoolSize(maxOffHeapMemory - offHeapStorageMemory) offHeapStorageMemoryPool.incrementPoolSize(offHeapStorageMemory) 其中MEMORY_OFFHEAP_SIZE为spark.memory.offHeap.size，这部分offHeap内存被spark.memory.storageFraction分为storage与execution用途供统一内存管理使用。\n统一内存管理UnifiedMemoryManager会管理堆内堆外的execution和storage内存，定义了四个内存池分别为：onHeapStorageMemoryPool, offHeapStorageMemoryPool, onHeapExecutionMemoryPool, offHeapExecutionMemoryPool，在spark内部申请内存时会指定MemoryMode为ON_HEAP或OFF_HEAP决定从哪部分申请内存。\n我们在WebUI看到的executors信息中Storage是包括了统一内存管理控制的堆内堆外区域的。\n下面的5.9G中包括了2.7G的堆内和3.2G(3g按1000算为3.221G,非1024算) 对大的几个RDD进行cache并action后，立马看会看到存储占用了堆内2.7G的大部分，即把execution的抢占了，仍然不够时已经有些序列化到磁盘中了。稍等一会execution会将storage抢占的这部分驱逐并序列化到disk中，如上将会变成下面的状况 按前面所说，这种均是在堆内内存存储的，我们查看被缓存的RDD的信息也可看到。 序列化存储级别怎么存到堆外？尤其是那些不希望被GC的长期存在的RDD，例如常驻内存的名单库等。我们可以使用persist时设置level为StorageLevel.OFF_HEAP，此种情况下只能用内存，不能同时存储到其他地方。 注意: 默认情况下Off-heap模式的内存并不启用，可以通过“spark.memory.offHeap.enabled”参数开启，并由spark.memory.offHeap.size指定堆外内存的大小（占用的空间划归JVM OffHeap内存）。\n4. 任务内存管理（Task Memory Manager） Executor中任务以线程的方式执行，各线程共享JVM的资源，任务之间的内存资源没有强隔离（任务没有专用的Heap区域）。因此，可能会出现这样的情况：先到达的任务可能占用较大的内存，而后到的任务因得不到足够的内存而挂起。\n在Spark任务内存管理中，使用HashMap存储任务与其消耗内存的映射关系。每个任务可占用的内存大小为潜在可使用计算内存的[1/2n, 1/n], 当剩余内存为小于1/2n时，任务将被挂起，直至有其他任务释放执行内存，而满足内存下限1/2n，任务被唤醒，其中n为当前Executor中活跃的任务数。\n任务执行过程中，如果需要更多的内存，则会进行申请，如果，存在空闲内存，则自动扩容成功，否则，将抛出OutOffMemroyError。\n5. 相关调优 什么时候需要调节Executor的堆外内存大小？ 当出现一下异常时：shuffle file cannot find，executor lost、task lost，out of memory\n出现这种问题的现象大致有这么两种情况：\nExecutor挂掉了，对应的Executor上面的block manager也挂掉了，找不到对应的shuffle map output文件，Reducer端不能够拉取数据 Executor并没有挂掉，而是在拉取数据的过程出现了问题。 上述情况下，就可以去考虑调节一下executor的堆外内存。也许就可以避免报错；此外，有时，堆外内存调节的比较大的时候，对于性能来说，也会带来一定的提升。这个executor跑着跑着，突然内存不足了，堆外内存不足了，可能会OOM，挂掉。block manager也没有了，数据也丢失掉了。\n如果此时，stage0的executor挂了，BlockManager也没有了；此时，stage1的executor的task，虽然通过 Driver的MapOutputTrakcer获取到了自己数据的地址；但是实际上去找对方的BlockManager获取数据的 时候，是获取不到的。\n此时，就会在spark-submit运行作业（jar），client（standalone client、yarn client），在本机就会打印出log:shuffle output file not found。。。DAGScheduler，resubmitting task，一直会挂掉。反复挂掉几次，反复报错几次,整个spark作业就崩溃了\n1 2 3 4 5 --conf spark.yarn.executor.memoryOverhead=2048 spark-submit脚本里面，去用--conf的方式，去添加配置；一定要注意！！！切记， 不是在你的spark作业代码中，用new SparkConf().set()这种方式去设置，不要这样去设置，是没有用的！ 一定要在spark-submit脚本中去设置。 调节等待时长 executor，优先从自己本地关联的BlockManager中获取某份数据\n如果本地BlockManager没有的话，那么会通过TransferService，去远程连接其他节点上executor 的BlockManager去获取,尝试建立远程的网络连接，并且去拉取数据，task创建的对象特别大，特别多频繁的让JVM堆内存满溢，进行垃圾回收。正好碰到那个exeuctor的JVM在垃圾回收。\n处于垃圾回收过程中，所有的工作线程全部停止；相当于只要一旦进行垃圾回收，spark / executor停止工作，无法提供响应，此时呢，就会没有响应，无法建立网络连接，会卡住；ok，spark默认的网络连接的超时时长，是60s，如果卡住60s都无法建立连接的话，那么就宣告失败了。碰到一种情况，偶尔，偶尔，偶尔！！！没有规律！！！某某file。一串file id。uuid（dsfsfd-2342vs–sdf–sdfsd）。not found。file lost。这种情况下，很有可能是有那份数据的executor在jvm gc。所以拉取数据的时候，建立不了连接。然后超过默认60s以后，直接宣告失败。报错几次，几次都拉取不到数据的话，可能会导致spark作业的崩溃。也可能会导致DAGScheduler，反复提交几次stage。TaskScheduler，反复提交几次task。大大延长我们的spark作业的运行时间。\n可以考虑调节连接的超时时长。\n1 2 --conf spark.core.connection.ack.wait.timeout=300 spark-submit脚本，切记，不是在new SparkConf().set()这种方式来设置的。spark.core.connection.ack.wait.timeout（spark core，connection，连接，ack，wait timeout，建立不上连接的时候，超时等待时长）调节这个值比较大以后，通常来说，可以避免部分的偶尔出现的某某文件拉取失败，某某文件lost掉了。。。 executor-memory 设置建议 如果设置小了，会发生什么：\n频繁GC，GC超限，CPU大部分时间用来做GC而回首的内存又很少，也就是executor堆内存不足。(通常gc 时间建议不超过task 时间的5%) 如果发生OOM或者GC耗时过长，考虑提高executor-memory或降低executor-core\n2. java.lang.OutOfMemoryError内存溢出，这和程序实现强相关，例如内存排序等，通常是要放入内存的数据量太大，内存空间不够引起的。 3. 数据频繁spill到磁盘，如果是I/O密集型的应用，响应时间就会显著延长。\n具体怎么样算调整到位呢？ TimeLine显示状态合理（通通绿条），GC时长合理（占比很小），系统能够稳定运行。 当然内存给太大了也是浪费资源，合理的临界值是在内存给到一定程度，对运行效率已经没有帮助了的时候，就可以了。\n增加executor内存量以后，性能的提升： 如果需要对RDD进行cache，那么更多的内存，就可以缓存更多的数据，将更少的数据写入磁盘，甚至不写入磁盘。减少了磁盘IO。 对于shuffle操作，reduce端，会需要内存来存放拉取的数据并进行聚合。如果内存不够，也会写入磁盘。如果给executor分配更多内存以后，就有更少的数据，需要写入磁盘，甚至不需要写入磁盘。减少了磁盘IO，提升了性能。 对于task的执行，可能会创建很多对象。如果内存比较小，可能会频繁导致JVM堆内存满了，然后频繁GC，垃圾回收，minor GC和full GC。（速度很慢）。内存加大以后，带来更少的GC，垃圾回收，避免了速度变慢，性能提升。 在给定执行内存 M、线程池大小 N 和数据总量 D 的时候，想要有效地提升 CPU 利用率，我们就要计算出最佳并行度 P，计算方法是让数据分片的平均大小 D/P 坐落在（M/N*2, M/N）区间，让每个Task能够拿到并处理适量的数据。怎么理解适量呢？D/P是原始数据的尺寸，真正到内存里去，是会翻倍的，至于翻多少倍，这个和文件格式有关系。不过，不管他翻多少倍，只要原始的D/P和M/N在一个当量，那么我们大概率就能避开OOM的问题，不至于某些Tasks需要处理的数据分片过大而OOM。Shuffle过后每个Reduce Task也会产生数据分片，spark.sql.shuffle.partitions 控制Joins之中的Shuffle Reduce阶段并行度，spark.sql.shuffle.partitions = 估算结果文件大小 / [128M,256M]，确保shuffle 后的数据分片大小在[128M,256M]区间。PS： 核心思路是，根据“定下来的”，去调整“未定下来的”，就可以去设置每一个参数了。\n假定Spark读取分布式文件，总大小512M，HDFS的分片是128M，那么并行度 = 512M / 128M = 4 Executor 并发度=1，那么Executor 内存 M 应在 128M 到 256M 之间。 Executor 并发度=2，那么Executor 内存 M 应在 256M 到 512M 之间。\n","wordCount":"329","inLanguage":"en","datePublished":"2022-11-03T16:14:39+08:00","dateModified":"2022-11-03T16:14:39+08:00","author":[{"@type":"Person","name":"Reid"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://reid00.github.io/posts/spark%E5%86%85%E5%AD%98%E7%A9%BA%E9%97%B4%E7%AE%A1%E7%90%86/"},"publisher":{"@type":"Organization","name":"Reid's Blog","logo":{"@type":"ImageObject","url":"https://reid00.github.io/favicon.ico"}}}</script></head><body id=top><meta name=referrer content="no-referrer"><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://reid00.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://reid00.github.io/posts/>Posts</a></div><h1 class=post-title>Spark内存空间管理</h1><div class=post-meta><span title='2022-11-03 16:14:39 +0800 +0800'>2022-11-03</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Reid</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-%e6%a6%82%e8%bf%b0 aria-label="1. 概述">1. 概述</a></li><li><a href=#2-%e5%a0%86%e5%86%85%e5%86%85%e5%ad%98 aria-label="2. 堆内内存">2. 堆内内存</a><ul><li><a href=#spark%e7%ae%a1%e7%90%86%e7%9a%84%e7%bb%9f%e4%b8%80%e5%86%85%e5%ad%98 aria-label=Spark管理的统一内存：>Spark管理的统一内存：</a></li></ul></li><li><a href=#3-%e5%a0%86%e5%a4%96%e5%86%85%e5%ad%98 aria-label="3. 堆外内存">3. 堆外内存</a></li><li><a href=#4-%e4%bb%bb%e5%8a%a1%e5%86%85%e5%ad%98%e7%ae%a1%e7%90%86task-memory-manager aria-label="4. 任务内存管理（Task Memory Manager）">4. 任务内存管理（Task Memory Manager）</a></li><li><a href=#5-%e7%9b%b8%e5%85%b3%e8%b0%83%e4%bc%98 aria-label="5. 相关调优">5. 相关调优</a><ul><li><a href=#%e4%bb%80%e4%b9%88%e6%97%b6%e5%80%99%e9%9c%80%e8%a6%81%e8%b0%83%e8%8a%82executor%e7%9a%84%e5%a0%86%e5%a4%96%e5%86%85%e5%ad%98%e5%a4%a7%e5%b0%8f aria-label=什么时候需要调节Executor的堆外内存大小？>什么时候需要调节Executor的堆外内存大小？</a></li><li><a href=#%e8%b0%83%e8%8a%82%e7%ad%89%e5%be%85%e6%97%b6%e9%95%bf aria-label=调节等待时长>调节等待时长</a></li><li><a href=#executor-memory-%e8%ae%be%e7%bd%ae%e5%bb%ba%e8%ae%ae aria-label="executor-memory 设置建议">executor-memory 设置建议</a></li><li><a href=#%e5%a2%9e%e5%8a%a0executor%e5%86%85%e5%ad%98%e9%87%8f%e4%bb%a5%e5%90%8e%e6%80%a7%e8%83%bd%e7%9a%84%e6%8f%90%e5%8d%87 aria-label=增加executor内存量以后，性能的提升：>增加executor内存量以后，性能的提升：</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=1-概述>1. 概述<a hidden class=anchor aria-hidden=true href=#1-概述>#</a></h1><p>Spark应用在yarn运行模式下，其以Executor Container的形式存在，container能申请到的最大内存受yarn.scheduler.maximum-allocation-mb限制。下面说的大部分内容其实与yarn等没有多少直接关系，知识均为通用的。</p><p>Spark应用运行过程中的内存可以分为<code>堆内内存</code>与<code>堆外内存</code>，其中堆内内存onheap由<code>spark.executor.memory</code>指定，堆外内存offheap由<code>spark.yarn.executor.memoryOverhead</code>参数指定，默认为<code>executorMemory*0.1,最小384M</code>。堆内内存executorMemory是spark使用的主要部分，其大小通过-Xmx参数传给jvm，内部有300M的保留资源不被executor使用。这里的堆外内存部分主要用于JVM自身，如字符串、NIO Buffer等开销，此部分用户代码及spark都无法直接操作。</p><p>executor执行的时候，用的内存可能会超过executor-memory，所以会为executor额外预留一部分内存，spark.yarn.executor.memoryOverhead即代表这部分内存。</p><p>另外还有部分堆外内存由<code>spark.memory.offHeap.enabled及spark.memory.offHeap.size</code>控制的堆外内存，这部分也归offheap，但主要是供统一内存管理使用的。
<img loading=lazy src=https://cdn.staticaly.com/gh/Reid00/image-host@main/20221103/image.5x1yv5utkjg0.webp alt=img1></p><h1 id=2-堆内内存>2. 堆内内存<a hidden class=anchor aria-hidden=true href=#2-堆内内存>#</a></h1><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>object</span> <span class=nc>UnifiedMemoryManager</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// Set aside a fixed amount of memory for non-storage, non-execution purposes.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>// This serves a function similar to `spark.memory.fraction`, but guarantees that we reserve
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>// sufficient memory for the system even for small heaps. E.g. if we have a 1GB JVM, then
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>// the memory used for execution and storage will be (1024 - 300) * 0.6 = 434MB by default.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>private</span> <span class=k>val</span> <span class=nc>RESERVED_SYSTEM_MEMORY_BYTES</span> <span class=k>=</span> <span class=mi>300</span> <span class=o>*</span> <span class=mi>1024</span> <span class=o>*</span> <span class=mi>1024</span>
</span></span></code></pre></td></tr></table></div></div><p>堆内内存有300M的保留资源，此外的可用内存usableMemory被分为<code>spark管理的内存和用户管理的内存</code>两部分，spark管理的内存通过spark.memory.fraction进行控制，默认0.6。</p><h2 id=spark管理的统一内存>Spark管理的统一内存：<a hidden class=anchor aria-hidden=true href=#spark管理的统一内存>#</a></h2><p>在设置了executor memory为3G时，debug代码 其各部分值如下：</p><ol><li>systemMemory=3087007744 //container的JVM最多可用的内存</li><li>reservedMemory=314572800 //保留的300M</li><li>minSystemMemory=471859200 //300M*1.5</li><li>executorMemory=3221225472 // 通过spark.executor.memory指定的值3g</li><li>usableMemory=2772434944 //为systemMemory-reservedMemory
由上，spark可管理的内存大小为</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=n>注意</span><span class=err>：</span> <span class=n>usableMemory</span> <span class=n>不是User</span> <span class=nc>Memory</span><span class=o>(</span><span class=n>有些也叫做other</span> <span class=nc>Memory</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=n>实际为spark</span><span class=o>-</span><span class=n>submit</span> <span class=n>提交时申请的exector</span><span class=o>-</span><span class=n>memory</span> <span class=n>大小</span> <span class=o>-</span> <span class=n>reservedMemory</span>
</span></span><span class=line><span class=cl><span class=n>usableMemory</span> <span class=o>*</span> <span class=n>memoryFraction</span><span class=k>=</span><span class=mi>2772434944</span> <span class=o>*</span><span class=mf>0.6</span><span class=k>=</span><span class=mi>1</span><span class=o>,</span><span class=mi>663</span><span class=o>,</span><span class=mi>460</span><span class=o>,</span><span class=mi>966</span>
</span></span></code></pre></td></tr></table></div></div><p>这块内存在spark中被称为unified region(代号M)或统一内存或可用内存，其进一步被分为执行内存ExecutionMemory和StorageMemory，见上图。其中storage memory(代号R)是M的一个subregion，其的大小占比受spark.memory.storageFraction控制，默认为0.5，即默认占usableMemory的 0.6*0.5=0.3。我们用onHeapStorageRegionSize来表示storage这部分的大小。</p><hr><ul><li><strong>ExecutionMemory执行内存</strong>：主要存储Shuffle、Join、Sort、Aggregation等计算过程中的临时数据；</li><li><strong>StorageMemory存储内存</strong>：主要存储spark的cache数据，如RDD.cache RDD.persist在调用时的数据存储，用户自定义变量及系统的广播变量等</li></ul><p>这两块内存在当前默认的UnifiedMemoryManager(Spark1.6引入)下是可以互相动态侵占的，即Execution内存不足时可以占用Storage的内存，反之亦然，其详细规则如下：</p><ul><li>Execution内存不足且onHeapStorageRegionSize有空闲时，可以向Storage Memory借用内存，- 但借用后storage不能将execution占用的部分驱逐evict出去，只能等着Execution自己释放。</li><li>Storage内存不足时可以借用Execution的内存，且当Execution又有内存资源需求时可以驱逐Storage占用的部分，但只能驱逐StorageMemory-onHeapStorageRegionSize的大小，原来划定的onHeapStorageRegionSize且在使用的不可被抢占。</li></ul><p>在spark的WebUI下，我们会看到Executors的信息如下图所示
我指定的executor-memory=5g,此处显示的StorageMemory其实是Spark的可用内存，包括Execution和Storage部分。(5G - 300M) * 0.6 = 2.7
<img loading=lazy src=https://cdn.staticaly.com/gh/Reid00/image-host@main/20221103/image.6y3kc6w8pdc0.webp alt=img2></p><ul><li>用户管理的内存(Other)：
上面说了占可用内存spark.memory.fraction(0.6)的spark 统一内存，另外0.4的用户内存用于存储用户代码生成的对象及RDD依赖等,用户在处理partition中的记录时，其遍历到的记录可以看做存储在Other区，当需要将RDD缓存时，将会序列化或不序列化的方式以Block的形式存储到Storage内存中。</li></ul><h1 id=3-堆外内存>3. 堆外内存<a hidden class=anchor aria-hidden=true href=#3-堆外内存>#</a></h1><p>前面说了，堆外内存有的是参数<code>spark.yarn.executor.memoryOverhead</code>控制，有的是参数<code>spark.memory.offHeap.size</code>控制，这个都算offheap内存，不过前者<code>主要用于JVM运行自身，字符串, NIO Buffer等开销</code>，而后者主要是供统一内存管理用<code>作Execution Memory及Storage Memory的用途</code>。</p><p>spark.yarn.executor.memoryOverhead设置的内存默认为executor.memory的0.1倍，最低384M，这个始终存在的，在采用yarn时，这块内存是包含在申请的容器内的，即<code>申请容器大小大于</code>spark.executor.memory+spark.yarn.executor.memoryOverhead。</p><p>而通过spark.memory.offHeap.enable/size申请的内存不在JVM内，Spark利用TungSten技术直接操作管理JVM外的原生内存。主要是为了解决Java对象开销大和GC的问题。
<img loading=lazy src=https://cdn.staticaly.com/gh/Reid00/image-host@main/20221103/image.14yijab2t2rg.webp alt=img3></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>protected</span><span class=o>[</span><span class=kt>this</span><span class=o>]</span> <span class=k>val</span> <span class=n>maxOffHeapMemory</span> <span class=k>=</span> <span class=n>conf</span><span class=o>.</span><span class=n>get</span><span class=o>(</span><span class=nc>MEMORY_OFFHEAP_SIZE</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=k>protected</span><span class=o>[</span><span class=kt>this</span><span class=o>]</span> <span class=k>val</span> <span class=n>offHeapStorageMemory</span> <span class=k>=</span>
</span></span><span class=line><span class=cl>  <span class=o>(</span><span class=n>maxOffHeapMemory</span> <span class=o>*</span> <span class=n>conf</span><span class=o>.</span><span class=n>getDouble</span><span class=o>(</span><span class=s>&#34;spark.memory.storageFraction&#34;</span><span class=o>,</span> <span class=mf>0.5</span><span class=o>)).</span><span class=n>toLong</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>offHeapExecutionMemoryPool</span><span class=o>.</span><span class=n>incrementPoolSize</span><span class=o>(</span><span class=n>maxOffHeapMemory</span> <span class=o>-</span> <span class=n>offHeapStorageMemory</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=n>offHeapStorageMemoryPool</span><span class=o>.</span><span class=n>incrementPoolSize</span><span class=o>(</span><span class=n>offHeapStorageMemory</span><span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><p>其中MEMORY_OFFHEAP_SIZE为spark.memory.offHeap.size，这部分offHeap内存被spark.memory.storageFraction分为storage与execution用途供统一内存管理使用。</p><p>统一内存管理UnifiedMemoryManager会管理堆内堆外的execution和storage内存，定义了四个内存池分别为：onHeapStorageMemoryPool, offHeapStorageMemoryPool, onHeapExecutionMemoryPool, offHeapExecutionMemoryPool，在spark内部申请内存时会指定MemoryMode为ON_HEAP或OFF_HEAP决定从哪部分申请内存。</p><p>我们在WebUI看到的executors信息中Storage是包括了统一内存管理控制的堆内堆外区域的。</p><p>下面的5.9G中包括了2.7G的堆内和3.2G(3g按1000算为3.221G,非1024算)
<img loading=lazy src=https://cdn.staticaly.com/gh/Reid00/image-host@main/20221103/image.1ukc20ki33a8.webp alt=img4></p><p>对大的几个RDD进行cache并action后，立马看会看到存储占用了堆内2.7G的大部分，即把execution的抢占了，仍然不够时已经有些序列化到磁盘中了。稍等一会execution会将storage抢占的这部分驱逐并序列化到disk中，如上将会变成下面的状况
<img loading=lazy src=https://cdn.staticaly.com/gh/Reid00/image-host@main/20221103/image.4yae4pzl4g40.webp alt=img5></p><p>按前面所说，这种均是在堆内内存存储的，我们查看被缓存的RDD的信息也可看到。
<img loading=lazy src=https://cdn.staticaly.com/gh/Reid00/image-host@main/20221103/image.5ijbkqhn78g0.webp alt=img6></p><p>序列化存储级别怎么存到堆外？尤其是那些不希望被GC的长期存在的RDD，例如常驻内存的名单库等。我们可以使用persist时设置level为StorageLevel.OFF_HEAP，此种情况下只能用内存，不能同时存储到其他地方。
<img loading=lazy src=https://cdn.staticaly.com/gh/Reid00/image-host@main/20221103/image.2qdkicxj19a0.webp alt=img7>
注意: 默认情况下Off-heap模式的内存并不启用，可以通过“spark.memory.offHeap.enabled”参数开启，并由spark.memory.offHeap.size指定堆外内存的大小（占用的空间划归JVM OffHeap内存）。</p><h1 id=4-任务内存管理task-memory-manager>4. 任务内存管理（Task Memory Manager）<a hidden class=anchor aria-hidden=true href=#4-任务内存管理task-memory-manager>#</a></h1><p>Executor中任务以线程的方式执行，各线程共享JVM的资源，任务之间的内存资源没有强隔离（任务没有专用的Heap区域）。因此，可能会出现这样的情况：先到达的任务可能占用较大的内存，而后到的任务因得不到足够的内存而挂起。</p><p>在Spark任务内存管理中，使用HashMap存储任务与其消耗内存的映射关系。每个任务可占用的内存大小为潜在可使用计算内存的[1/2n, 1/n], 当剩余内存为小于1/2n时，任务将被挂起，直至有其他任务释放执行内存，而满足内存下限1/2n，任务被唤醒，其中n为当前Executor中活跃的任务数。</p><p>任务执行过程中，如果需要更多的内存，则会进行申请，如果，存在空闲内存，则自动扩容成功，否则，将抛出OutOffMemroyError。</p><h1 id=5-相关调优>5. 相关调优<a hidden class=anchor aria-hidden=true href=#5-相关调优>#</a></h1><h2 id=什么时候需要调节executor的堆外内存大小>什么时候需要调节Executor的堆外内存大小？<a hidden class=anchor aria-hidden=true href=#什么时候需要调节executor的堆外内存大小>#</a></h2><p>当出现一下异常时：shuffle file cannot find，executor lost、task lost，out of memory</p><p>出现这种问题的现象大致有这么两种情况：</p><ul><li>Executor挂掉了，对应的Executor上面的block manager也挂掉了，找不到对应的shuffle map output文件，Reducer端不能够拉取数据</li><li>Executor并没有挂掉，而是在拉取数据的过程出现了问题。</li></ul><p>上述情况下，就可以去考虑调节一下executor的堆外内存。也许就可以避免报错；此外，有时，堆外内存调节的比较大的时候，对于性能来说，也会带来一定的提升。这个executor跑着跑着，突然内存不足了，堆外内存不足了，可能会OOM，挂掉。block manager也没有了，数据也丢失掉了。</p><p>如果此时，stage0的executor挂了，BlockManager也没有了；此时，stage1的executor的task，虽然通过
Driver的MapOutputTrakcer获取到了自己数据的地址；但是实际上去找对方的BlockManager获取数据的
时候，是获取不到的。</p><p>此时，就会在spark-submit运行作业（jar），client（standalone client、yarn client），在本机就会打印出log:shuffle output file not found。。。DAGScheduler，resubmitting task，一直会挂掉。反复挂掉几次，反复报错几次,整个spark作业就崩溃了</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=o>--</span><span class=n>conf</span> <span class=n>spark</span><span class=o>.</span><span class=n>yarn</span><span class=o>.</span><span class=n>executor</span><span class=o>.</span><span class=n>memoryOverhead</span><span class=o>=</span><span class=mi>2048</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>spark</span><span class=o>-</span><span class=n>submit脚本里面</span><span class=err>，</span><span class=n>去用</span><span class=o>--</span><span class=n>conf的方式</span><span class=err>，</span><span class=n>去添加配置</span><span class=err>；</span><span class=n>一定要注意</span><span class=err>！！！</span><span class=n>切记</span><span class=err>，</span>
</span></span><span class=line><span class=cl><span class=n>不是在你的spark作业代码中</span><span class=err>，</span><span class=n>用new</span> <span class=n>SparkConf</span><span class=p>()</span><span class=o>.</span><span class=n>set</span><span class=p>()</span><span class=n>这种方式去设置</span><span class=err>，</span><span class=n>不要这样去设置</span><span class=err>，</span><span class=n>是没有用的</span><span class=err>！</span>
</span></span><span class=line><span class=cl><span class=n>一定要在spark</span><span class=o>-</span><span class=n>submit脚本中去设置</span><span class=err>。</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=调节等待时长>调节等待时长<a hidden class=anchor aria-hidden=true href=#调节等待时长>#</a></h2><p><img loading=lazy src=https://cdn.staticaly.com/gh/Reid00/image-host@main/20221103/image.3x3dp6frpay0.webp alt=img8>
executor，优先从自己本地关联的BlockManager中获取某份数据</p><p>如果本地BlockManager没有的话，那么会通过TransferService，去远程连接其他节点上executor
的BlockManager去获取,尝试建立远程的网络连接，并且去拉取数据，task创建的对象特别大，特别多频繁的让JVM堆内存满溢，进行垃圾回收。正好碰到那个exeuctor的JVM在垃圾回收。</p><p>处于垃圾回收过程中，所有的工作线程全部停止；相当于只要一旦进行垃圾回收，spark / executor停止工作，无法提供响应，此时呢，就会没有响应，无法建立网络连接，会卡住；ok，spark默认的网络连接的超时时长，是60s，如果卡住60s都无法建立连接的话，那么就宣告失败了。碰到一种情况，偶尔，偶尔，偶尔！！！没有规律！！！某某file。一串file id。uuid（dsfsfd-2342vs&ndash;sdf&ndash;sdfsd）。not found。file lost。这种情况下，很有可能是有那份数据的executor在jvm gc。所以拉取数据的时候，建立不了连接。然后超过默认60s以后，直接宣告失败。报错几次，几次都拉取不到数据的话，可能会导致spark作业的崩溃。也可能会导致DAGScheduler，反复提交几次stage。TaskScheduler，反复提交几次task。大大延长我们的spark作业的运行时间。</p><p>可以考虑调节连接的超时时长。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=o>--</span><span class=n>conf</span> <span class=n>spark</span><span class=o>.</span><span class=n>core</span><span class=o>.</span><span class=n>connection</span><span class=o>.</span><span class=n>ack</span><span class=o>.</span><span class=n>wait</span><span class=o>.</span><span class=n>timeout</span><span class=o>=</span><span class=mi>300</span>
</span></span><span class=line><span class=cl><span class=n>spark</span><span class=o>-</span><span class=n>submit脚本</span><span class=err>，</span><span class=n>切记</span><span class=err>，</span><span class=n>不是在new</span> <span class=n>SparkConf</span><span class=p>()</span><span class=o>.</span><span class=n>set</span><span class=p>()</span><span class=n>这种方式来设置的</span><span class=err>。</span><span class=n>spark</span><span class=o>.</span><span class=n>core</span><span class=o>.</span><span class=n>connection</span><span class=o>.</span><span class=n>ack</span><span class=o>.</span><span class=n>wait</span><span class=o>.</span><span class=n>timeout</span><span class=err>（</span><span class=n>spark</span> <span class=n>core</span><span class=err>，</span><span class=n>connection</span><span class=err>，</span><span class=n>连接</span><span class=err>，</span><span class=n>ack</span><span class=err>，</span><span class=n>wait</span> <span class=n>timeout</span><span class=err>，</span><span class=n>建立不上连接的时候</span><span class=err>，</span><span class=n>超时等待时长</span><span class=err>）</span><span class=n>调节这个值比较大以后</span><span class=err>，</span><span class=n>通常来说</span><span class=err>，</span><span class=n>可以避免部分的偶尔出现的某某文件拉取失败</span><span class=err>，</span><span class=n>某某文件lost掉了</span><span class=err>。。。</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=executor-memory-设置建议>executor-memory 设置建议<a hidden class=anchor aria-hidden=true href=#executor-memory-设置建议>#</a></h2><p>如果设置小了，会发生什么：</p><ol><li>频繁GC，GC超限，CPU大部分时间用来做GC而回首的内存又很少，也就是executor堆内存不足。(通常gc 时间建议不超过task 时间的5%)</li></ol><blockquote><p>如果发生OOM或者GC耗时过长，考虑提高executor-memory或降低executor-core</p></blockquote><p><img loading=lazy src=https://cdn.staticaly.com/gh/Reid00/image-host@main/20221103/image.7bwwn0k9cls0.webp alt=img9>
2. java.lang.OutOfMemoryError内存溢出，这和程序实现强相关，例如内存排序等，通常是要放入内存的数据量太大，内存空间不够引起的。
3. 数据频繁spill到磁盘，如果是I/O密集型的应用，响应时间就会显著延长。</p><p>具体怎么样算调整到位呢？
TimeLine显示状态合理（通通绿条），GC时长合理（占比很小），系统能够稳定运行。
当然内存给太大了也是浪费资源，合理的临界值是在内存给到一定程度，对运行效率已经没有帮助了的时候，就可以了。</p><h2 id=增加executor内存量以后性能的提升>增加executor内存量以后，性能的提升：<a hidden class=anchor aria-hidden=true href=#增加executor内存量以后性能的提升>#</a></h2><ul><li>如果需要对RDD进行cache，那么更多的内存，就可以缓存更多的数据，将更少的数据写入磁盘，甚至不写入磁盘。减少了磁盘IO。</li><li>对于shuffle操作，reduce端，会需要内存来存放拉取的数据并进行聚合。如果内存不够，也会写入磁盘。如果给executor分配更多内存以后，就有更少的数据，需要写入磁盘，甚至不需要写入磁盘。减少了磁盘IO，提升了性能。</li><li>对于task的执行，可能会创建很多对象。如果内存比较小，可能会频繁导致JVM堆内存满了，然后频繁GC，垃圾回收，minor GC和full GC。（速度很慢）。内存加大以后，带来更少的GC，垃圾回收，避免了速度变慢，性能提升。</li></ul><p>在给定执行内存 M、线程池大小 N 和数据总量 D 的时候，想要有效地提升 CPU 利用率，我们就要计算出最佳并行度 P，计算方法是让数据分片的平均大小 D/P 坐落在（M/N*2, M/N）区间，让每个Task能够拿到并处理适量的数据。怎么理解适量呢？D/P是原始数据的尺寸，真正到内存里去，是会翻倍的，至于翻多少倍，这个和文件格式有关系。不过，不管他翻多少倍，只要原始的D/P和M/N在一个当量，那么我们大概率就能避开OOM的问题，不至于某些Tasks需要处理的数据分片过大而OOM。Shuffle过后每个Reduce Task也会产生数据分片，<code>spark.sql.shuffle.partitions</code> 控制Joins之中的Shuffle Reduce阶段并行度，<code>spark.sql.shuffle.partitions</code> = 估算结果文件大小 / [128M,256M]，确保shuffle 后的数据分片大小在[128M,256M]区间。PS： 核心思路是，根据“定下来的”，去调整“未定下来的”，就可以去设置每一个参数了。</p><p>假定Spark读取分布式文件，总大小512M，HDFS的分片是128M，那么并行度 = 512M / 128M = 4
Executor 并发度=1，那么Executor 内存 M 应在 128M 到 256M 之间。
Executor 并发度=2，那么Executor 内存 M 应在 256M 到 512M 之间。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://reid00.github.io/tags/%E5%86%85%E5%AD%98%E7%A9%BA%E9%97%B4%E7%AE%A1%E7%90%86/>内存空间管理</a></li><li><a href=https://reid00.github.io/tags/spark/>spark</a></li></ul><nav class=paginav><a class=prev href=https://reid00.github.io/posts/http-502-%E9%97%AE%E9%A2%98-%E6%8E%92%E6%9F%A5/><span class=title>« Prev</span><br><span>Http 502 问题 排查</span></a>
<a class=next href=https://reid00.github.io/posts/raft-%E4%BB%8B%E7%BB%8D/><span class=title>Next »</span><br><span>Raft 介绍</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2023 <a href=https://reid00.github.io/>Reid's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>